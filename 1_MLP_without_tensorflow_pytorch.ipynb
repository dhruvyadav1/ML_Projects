{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP_without_tensorflow_pytorch.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8w0CtP2JpMc"
      },
      "source": [
        "# MLP from Scratch with Exploding and Vanishing Gradients\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lIm6R7VLyYV"
      },
      "source": [
        "**SGD and CE loss are covered**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XI7MRPGS4JHr"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import random\n",
        "import seaborn as sns"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErYaAEzEJpMi"
      },
      "source": [
        "\n",
        "**For vanishing we don't need to change the network architecture(solved later)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFvwbgO-4JHw"
      },
      "source": [
        "class Neural_Network():\n",
        "    def __init__(self, neurons, Activations):\n",
        "        # arguments: an array \"neurons\" consist of number of neurons for each layer, \n",
        "        # an array \"activations\" consisting of activation functions used for the hidden layers and output layer\n",
        "        self.inputSize = neurons[0] # Number of neurons in input layer\n",
        "        self.outputSize = neurons[-1] # Number of neurons in output layer\n",
        "        self.layers = len(neurons)\n",
        "        self.weights = [] #weights for each layer\n",
        "        self.biases = [] #biases in each layer \n",
        "        self.layer_activations = [] #activations in each layer\n",
        "        self.exploding = exploding  #this is when are asked to design a network that explodes(gradient)\n",
        "        #this change can be seen in the loop below\n",
        "        for i in range(len(neurons)-1):      #the case when we don't want gradients to explode we go with this layer\n",
        "            self.weights.append(np.random.rand(neurons[i+1],neurons[i])) #weight matrix between layer i and layer i+1\n",
        "            self.biases.append(np.random.rand(neurons[i+1],1))\n",
        "            self.layer_activations.append(Activations[i]) #activations for each layer\n",
        "            \n",
        "    def sigmoid(self, z): # sigmoid activation function                  \n",
        "        return 1.0/(1.0+np.exp(-z))\n",
        "    \n",
        "    def sigmoidPrime(self,z): # derivative of sigmoid activation function\n",
        "        return self.sigmoid(z)*(1-self.sigmoid(z))\n",
        "\n",
        "                          \n",
        "    def tanh(self, z): # hyperbolic tan activation function\n",
        "        '''\n",
        "           tanh activation fuction  \n",
        "        '''\n",
        "        #Fill in the details to compute and return the tanh activation function                  \n",
        "        return (np.exp(z)-np.exp(-z))/(np.exp(z)+np.exp(-z))          \n",
        "    \n",
        "    def tanhPrime(self,x): # derivative of hyperbolic tan activation function\n",
        "        '''\n",
        "            derivative of tanh function \n",
        "        '''\n",
        "        return (1.0-(self.tanh(x))**2)                               \n",
        "                          \n",
        "    def linear(self, z): \n",
        "        '''\n",
        "            linear activation function\n",
        "        '''\n",
        "        # Linear activation function                                    \n",
        "        return z                                                      \n",
        "    \n",
        "    def linearPrime(self,x): # derivative of linear activation function\n",
        "        '''\n",
        "            derivative of linear function \n",
        "        '''                                                   \n",
        "        return np.ones(x.shape)                                      \n",
        "\n",
        "    def ReLU(self,z): # ReLU activation function\n",
        "        '''\n",
        "            ReLU activation function \n",
        "        '''                \n",
        "        return np.maximum(0,z)                                        \n",
        "    \n",
        "    def ReLUPrime(self,z): # derivative of ReLU activation function\n",
        "        '''\n",
        "            derivative of ReLU function \n",
        "        '''\n",
        "        return (z>0).astype(z.dtype)                                  \n",
        "    \n",
        "    def forward(self, a): # function of forward pass which will receive input and give the output of final layer\n",
        "        '''\n",
        "            This is the forward pass step for the Neural Network\n",
        "            There are two steps : \n",
        "            1. computing z from activations of last layer \n",
        "            2. applying activation function on z to compute a\n",
        "        '''\n",
        "        layer_activations_a = [a] #store the input as the input layer activations\n",
        "        layer_dot_prod_z = []\n",
        "        for i, param in enumerate(zip(self.biases, self.weights)):\n",
        "            b, w = param[0], param[1]\n",
        "            if self.layer_activations[i].lower()=='sigmoid':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.sigmoid(z)\n",
        "            elif self.layer_activations[i].lower()=='relu':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.ReLU(z)\n",
        "            elif self.layer_activations[i].lower()=='tanh':   \n",
        "                z = np.dot(w, a)+b\n",
        "                a =self.tanh(z)\n",
        "            elif self.layer_activations[i].lower()=='linear':\n",
        "                z = np.dot(w, a)+b\n",
        "                a = self.linear(z)\n",
        "            layer_dot_prod_z.append(z)    \n",
        "            layer_activations_a.append(a)\n",
        "        return a, layer_dot_prod_z, layer_activations_a\n",
        "                          \n",
        "            \n",
        "    \n",
        "    def backward(self, x, y, zs, activations): # find the loss and return derivative of loss w.r.t every parameter\n",
        "        '''\n",
        "            This is the step where we update our weights to optimize loss\n",
        "        '''\n",
        "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        \n",
        "        # backward pass\n",
        "        \n",
        "        ##\n",
        "        #this is for the last layer\n",
        "       ####################\n",
        "        if self.layer_activations[-1].lower()=='sigmoid':\n",
        "            delta = (activations[-1] - y)*self.sigmoidPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='relu':\n",
        "            delta = (activations[-1] - y)*ReLUPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='tanh':   \n",
        "            delta = (activations[-1] - y)*tanhPrime(zs[-1])\n",
        "        elif self.layer_activations[-1].lower()=='linear':\n",
        "            delta = (activations[-1] - y)*linearPrime(zs[-1])\n",
        "       ####################\n",
        "        # fill in the appropriate details for gradients of w and b\n",
        "        grad_b[-1] = np.sum(delta,axis=1,keepdims=True)/(delta.shape[1]) \n",
        "        grad_w[-1] = np.dot(delta,activations[-2].T)/(delta.shape[1])\n",
        "       ####################\n",
        "        for l in range(2, self.layers): # Here l is in backward sense i.e. last l th layer\n",
        "            z = zs[-l]\n",
        "            if self.layer_activations[-l].lower()=='sigmoid':\n",
        "                prime = self.sigmoidPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='relu':\n",
        "                prime = self.ReLUPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='tanh':   \n",
        "                prime = self.tanhPrime(z)\n",
        "            elif self.layer_activations[-l].lower()=='linear':\n",
        "                prime = self.linearPrime(z)\n",
        "\n",
        "            #Compute delta, gradients of b and w \n",
        "            delta = np.dot((self.weights[-l+1]).T,delta)*prime\n",
        "            grad_w[-l] = np.dot(delta,activations[-l-1].T)/(delta.shape[1])\n",
        "            grad_b[-l] = np.sum(delta,axis=1,keepdims=True)/(delta.shape[1]) \n",
        "                          \n",
        "        return (grad_b, grad_w)                 \n",
        "\n",
        "    def update_parameters(self, grads, learning_rate): # update the parameters using the gradients\n",
        "        # update weights and biases using the gradients and the learning rate\n",
        "        grad_b, grad_w = grads[0], grads[1]       \n",
        "        \n",
        "        self.weights = np.array(self.weights)\n",
        "        self.biases = np.array(self.biases)\n",
        "        grad_w = np.array(grad_w)\n",
        "        grad_b = np.array(grad_b)\n",
        "\n",
        "        #Implement the update rule for weights  and biases\n",
        "        self.weights = self.weights - learning_rate*grad_w\n",
        "        self.biases = self.biases - learning_rate*grad_b\n",
        "        \n",
        "    def loss(self, predicted, actual):\n",
        "        #Implement the loss function\n",
        "        '''\n",
        "           This is the squared error loss funciton \n",
        "        '''\n",
        "        return 0.5*(predicted-actual)**2\n",
        "                     \n",
        "    def train(self, X, Y, minibatch=False,minibatchsize=20,lr=1e-3,epochs=1000): # receive the full training data set \n",
        "        loss_list = []\n",
        "        if minibatch==False:\n",
        "            for e in range(epochs): \n",
        "                losses = []\n",
        "                for q in range(len(X)):\n",
        "                    train_x = np.resize(X[q],(X[q].shape[0],1))\n",
        "                    if not onehotencoded: \n",
        "                        train_y = np.resize(Y[q],(1,1))\n",
        "                    else:\n",
        "                        train_y = np.resize(np.argmax(Y[q]),(1,1)) \n",
        "                    out, dot_prod_z, activations_a = self.forward(train_x)\n",
        "                    loss = self.loss(out, train_y)\n",
        "                    grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
        "                    self.update_parameters(grads, lr)\n",
        "                    losses.append(loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))}')\n",
        "        else:\n",
        "            X_mini,Y_mini = create_minibatches(X,Y,minibatchsize)\n",
        "            for e in range(epochs):\n",
        "                losses = []\n",
        "                for X_b,Y_b in zip(X_mini,Y_mini):\n",
        "                    train_x = X_b.T\n",
        "                    train_y = Y_b.reshape(1,minibatchsize)\n",
        "                    out, dot_prod_z, activations_a = self.forward(train_x)\n",
        "                    loss = np.mean(np.array((self.loss(out, train_y))))\n",
        "                    grads = self.backward(train_x, train_y, dot_prod_z, activations_a) # find the gradients using backward pass\n",
        "                    self.update_parameters(grads, lr)\n",
        "                    losses.append(loss)\n",
        "                loss_list.append(np.mean(np.array(losses)))\n",
        "                print(f'Epoch: {e} Loss: {np.mean(np.array(losses))}')\n",
        "        return loss_list\n",
        "        \n",
        "    def predict(self, x):\n",
        "        print (\"Input : \\n\" + str(x))\n",
        "        prediction,_,_ = self.forward(x)\n",
        "        print (\"Output: \\n\" + str(prediction))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knWNSapu4JH0"
      },
      "source": [
        "# a method for creating one hot encoded labels \n",
        "def onehotencoding(Y):\n",
        "    pass      #done on multiclass file        githublink: https://github.com/dhruvyadav1/ML_Projects \n",
        "\n",
        "#a method to create minibatches \n",
        "def create_minibatches(X,Y,minibatchsize):\n",
        "    numbatches = int(np.ceil(len(X)/minibatchsize))\n",
        "    idx = np.arange(len(X))\n",
        "    np.random.shuffle(idx)\n",
        "    X_minibatches = []\n",
        "    Y_minibatches = [] \n",
        "    for i in range(numbatches):\n",
        "        idx_minibatch = idx[i*minibatchsize:min(len(idx),(i+1)*minibatchsize)]\n",
        "        xn = np.take(X,idx_minibatch,axis=0) \n",
        "        yn = np.take(Y,idx_minibatch,axis=0)\n",
        "        X_minibatches.append(xn)\n",
        "        Y_minibatches.append(yn)\n",
        "    return X_minibatches, Y_minibatches\n",
        "\n",
        "def test_create_minibatches():\n",
        "    X = []\n",
        "    Y = []\n",
        "    inputsize = 3\n",
        "    minibatch = False\n",
        "    onehotencoded = False\n",
        "    n_batch = 20\n",
        "    batch_size = 5\n",
        "    for i in range(50):\n",
        "        if(i % 2 == 0):\n",
        "            X.append([np.random.randint(1,10) for i1 in range(inputsize)])\n",
        "            Y.append(1)\n",
        "        else:\n",
        "            X.append([np.random.randint(-10,1) for i1 in range(inputsize)])\n",
        "            Y.append(0)\n",
        "    X = np.array(X)\n",
        "    Y = np.array(Y)\n",
        "\n",
        "    X_mb, Y_mb = create_minibatches(X,Y,6)\n",
        "    print(X_mb, Y_mb)\n",
        "\n",
        "# test_create_minibatches()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BphFhrM4JH2"
      },
      "source": [
        "# Generating some training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ATmwVlX4JH2"
      },
      "source": [
        "X = []\n",
        "Y = []\n",
        "inputsize = 3\n",
        "minibatch = False\n",
        "onehotencoded = False\n",
        "exploding = False\n",
        "n_batch = 20\n",
        "batch_size = 5\n",
        "# minibatchsize = 20\n",
        "for i in range(500):\n",
        "    if(i % 2 == 0):\n",
        "        X.append([random.randint(1,10) for i1 in range(inputsize)])\n",
        "        Y.append(1)\n",
        "    else:\n",
        "        X.append([random.randint(-10,1) for i1 in range(inputsize)])\n",
        "        Y.append(0)\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "if onehotencoded:\n",
        "    Y = onehotencoding(Y)\n",
        "\n",
        "train_X = X\n",
        "train_Y = Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BwdBtuHk4JH5",
        "scrolled": true,
        "outputId": "7b44fd2b-231e-40ad-e080-d1012bc02829",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(train_X.shape, train_Y.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 3) (500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUCdWmPM4JH7"
      },
      "source": [
        "# Defining the network\n",
        "**This network is defined as asked in question 1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iJPn_Gx4JH8"
      },
      "source": [
        "#D_in is input dimension\n",
        "#H1 is dimension of first hidden layer \n",
        "#H2 is dimension of second hidden layer\n",
        "#D_out is output dimension.\n",
        "D_in, H1, H2,H3,H4, D_out = inputsize, 5, 5 ,10, 5, 1                              #You can add more layers if you wish to \n",
        "\n",
        "neurons = [D_in, H1, H2,H3,H4, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['linear','linear', 'tanh','ReLU', 'sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
        "my_neuralnet1 = Neural_Network(neurons, activation_functions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqRqZYPp4JH-"
      },
      "source": [
        "# Training the network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "moQohCt-JpMx"
      },
      "source": [
        "**SGD with l_r=0.001, epochs=1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GgKuz8f4JH_",
        "scrolled": true,
        "outputId": "dd204cc6-ed83-4367-e749-87b2dede3ca9"
      },
      "source": [
        "loss1 = my_neuralnet1.train(train_X,train_Y,minibatch=False,minibatchsize=20,lr=0.001,epochs=1000)\n",
        "#the loss function above is for the SGD algorithm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 0.07078061435240064\n",
            "Epoch: 1 Loss: 0.06869356535339266\n",
            "Epoch: 2 Loss: 0.06684787441817917\n",
            "Epoch: 3 Loss: 0.06505817997791918\n",
            "Epoch: 4 Loss: 0.06332426447798883\n",
            "Epoch: 5 Loss: 0.06164570832485941\n",
            "Epoch: 6 Loss: 0.06002191072455541\n",
            "Epoch: 7 Loss: 0.058452110383520405\n",
            "Epoch: 8 Loss: 0.05693540574380283\n",
            "Epoch: 9 Loss: 0.055470774483036465\n",
            "Epoch: 10 Loss: 0.054057092066353056\n",
            "Epoch: 11 Loss: 0.05269314919043258\n",
            "Epoch: 12 Loss: 0.051377668008179424\n",
            "Epoch: 13 Loss: 0.0501093170652738\n",
            "Epoch: 14 Loss: 0.048886724916734965\n",
            "Epoch: 15 Loss: 0.047708492422613005\n",
            "Epoch: 16 Loss: 0.04657320374720194\n",
            "Epoch: 17 Loss: 0.045479436106123504\n",
            "Epoch: 18 Loss: 0.044425768320761394\n",
            "Epoch: 19 Loss: 0.043410788250392136\n",
            "Epoch: 20 Loss: 0.04243309917954645\n",
            "Epoch: 21 Loss: 0.04149132524222146\n",
            "Epoch: 22 Loss: 0.04058411596610108\n",
            "Epoch: 23 Loss: 0.03971015001943308\n",
            "Epoch: 24 Loss: 0.038868138241112675\n",
            "Epoch: 25 Loss: 0.03805682603122964\n",
            "Epoch: 26 Loss: 0.03727499517518779\n",
            "Epoch: 27 Loss: 0.03652146516979126\n",
            "Epoch: 28 Loss: 0.03579509411464014\n",
            "Epoch: 29 Loss: 0.03509477922698218\n",
            "Epoch: 30 Loss: 0.03441945703297265\n",
            "Epoch: 31 Loss: 0.03376810328321241\n",
            "Epoch: 32 Loss: 0.03313973263555286\n",
            "Epoch: 33 Loss: 0.03253339814352807\n",
            "Epoch: 34 Loss: 0.0319481905844421\n",
            "Epoch: 35 Loss: 0.031383237657122566\n",
            "Epoch: 36 Loss: 0.030837703075661017\n",
            "Epoch: 37 Loss: 0.03031078558209441\n",
            "Epoch: 38 Loss: 0.029801717897934323\n",
            "Epoch: 39 Loss: 0.029309765631707285\n",
            "Epoch: 40 Loss: 0.028834226157213923\n",
            "Epoch: 41 Loss: 0.028374427475029593\n",
            "Epoch: 42 Loss: 0.027929727067832144\n",
            "Epoch: 43 Loss: 0.02749951075843494\n",
            "Epoch: 44 Loss: 0.027083191577904654\n",
            "Epoch: 45 Loss: 0.02668020864983252\n",
            "Epoch: 46 Loss: 0.026290026095688018\n",
            "Epoch: 47 Loss: 0.025912131965194957\n",
            "Epoch: 48 Loss: 0.02554603719481849\n",
            "Epoch: 49 Loss: 0.025191274596718393\n",
            "Epoch: 50 Loss: 0.02484739787989891\n",
            "Epoch: 51 Loss: 0.02451398070475295\n",
            "Epoch: 52 Loss: 0.02419061577174887\n",
            "Epoch: 53 Loss: 0.023876913944629825\n",
            "Epoch: 54 Loss: 0.023572503408180595\n",
            "Epoch: 55 Loss: 0.023277028860355323\n",
            "Epoch: 56 Loss: 0.022990150738346708\n",
            "Epoch: 57 Loss: 0.022711544478002495\n",
            "Epoch: 58 Loss: 0.02244089980585851\n",
            "Epoch: 59 Loss: 0.02217792006294721\n",
            "Epoch: 60 Loss: 0.021922321559459543\n",
            "Epoch: 61 Loss: 0.021673832959275403\n",
            "Epoch: 62 Loss: 0.02143219469333558\n",
            "Epoch: 63 Loss: 0.021197158400800588\n",
            "Epoch: 64 Loss: 0.02096848639692711\n",
            "Epoch: 65 Loss: 0.020745951166588783\n",
            "Epoch: 66 Loss: 0.02052933488237307\n",
            "Epoch: 67 Loss: 0.020318428946198126\n",
            "Epoch: 68 Loss: 0.020113033553411555\n",
            "Epoch: 69 Loss: 0.019912957278355223\n",
            "Epoch: 70 Loss: 0.019718016680407322\n",
            "Epoch: 71 Loss: 0.01952803592954077\n",
            "Epoch: 72 Loss: 0.019342846450469034\n",
            "Epoch: 73 Loss: 0.01916228658448236\n",
            "Epoch: 74 Loss: 0.018986201268111024\n",
            "Epoch: 75 Loss: 0.018814441727786093\n",
            "Epoch: 76 Loss: 0.01864686518970196\n",
            "Epoch: 77 Loss: 0.01848333460411906\n",
            "Epoch: 78 Loss: 0.01832371838337849\n",
            "Epoch: 79 Loss: 0.01816789015293317\n",
            "Epoch: 80 Loss: 0.018015728514732352\n",
            "Epoch: 81 Loss: 0.017867116822327587\n",
            "Epoch: 82 Loss: 0.0177219429670985\n",
            "Epoch: 83 Loss: 0.017580099175026467\n",
            "Epoch: 84 Loss: 0.017441481813471903\n",
            "Epoch: 85 Loss: 0.017305991207439177\n",
            "Epoch: 86 Loss: 0.017173531464838\n",
            "Epoch: 87 Loss: 0.017044010310276348\n",
            "Epoch: 88 Loss: 0.01691733892694325\n",
            "Epoch: 89 Loss: 0.016793431806162985\n",
            "Epoch: 90 Loss: 0.016672206604223895\n",
            "Epoch: 91 Loss: 0.01655358400610564\n",
            "Epoch: 92 Loss: 0.016437487595749162\n",
            "Epoch: 93 Loss: 0.01632384373253185\n",
            "Epoch: 94 Loss: 0.01621258143362813\n",
            "Epoch: 95 Loss: 0.016103632261953486\n",
            "Epoch: 96 Loss: 0.01599693021940542\n",
            "Epoch: 97 Loss: 0.015892411645129768\n",
            "Epoch: 98 Loss: 0.015790015118556513\n",
            "Epoch: 99 Loss: 0.01568968136696154\n",
            "Epoch: 100 Loss: 0.015591353177324805\n",
            "Epoch: 101 Loss: 0.01549497531226693\n",
            "Epoch: 102 Loss: 0.015400494429858465\n",
            "Epoch: 103 Loss: 0.015307859007106682\n",
            "Epoch: 104 Loss: 0.015217019266935257\n",
            "Epoch: 105 Loss: 0.015127927108482311\n",
            "Epoch: 106 Loss: 0.015040536040550776\n",
            "Epoch: 107 Loss: 0.014954801118055106\n",
            "Epoch: 108 Loss: 0.014870678881315467\n",
            "Epoch: 109 Loss: 0.014788127298059128\n",
            "Epoch: 110 Loss: 0.014707105707995854\n",
            "Epoch: 111 Loss: 0.014627574769841459\n",
            "Epoch: 112 Loss: 0.014549496410669717\n",
            "Epoch: 113 Loss: 0.014472833777479863\n",
            "Epoch: 114 Loss: 0.014397551190872186\n",
            "Epoch: 115 Loss: 0.014323614100730218\n",
            "Epoch: 116 Loss: 0.014250989043813176\n",
            "Epoch: 117 Loss: 0.01417964360316718\n",
            "Epoch: 118 Loss: 0.014109546369268795\n",
            "Epoch: 119 Loss: 0.014040666902818624\n",
            "Epoch: 120 Loss: 0.013972975699107163\n",
            "Epoch: 121 Loss: 0.013906444153878953\n",
            "Epoch: 122 Loss: 0.013841044530624735\n",
            "Epoch: 123 Loss: 0.013776749929235225\n",
            "Epoch: 124 Loss: 0.013713534255953242\n",
            "Epoch: 125 Loss: 0.013651372194564308\n",
            "Epoch: 126 Loss: 0.013590239178768372\n",
            "Epoch: 127 Loss: 0.013530111365679038\n",
            "Epoch: 128 Loss: 0.013470965610398516\n",
            "Epoch: 129 Loss: 0.013412779441619642\n",
            "Epoch: 130 Loss: 0.013355531038208433\n",
            "Epoch: 131 Loss: 0.01329919920672324\n",
            "Epoch: 132 Loss: 0.013243763359828235\n",
            "Epoch: 133 Loss: 0.013189203495561642\n",
            "Epoch: 134 Loss: 0.013135500177420672\n",
            "Epoch: 135 Loss: 0.013082634515226867\n",
            "Epoch: 136 Loss: 0.013030588146737947\n",
            "Epoch: 137 Loss: 0.012979343219973011\n",
            "Epoch: 138 Loss: 0.012928882376220259\n",
            "Epoch: 139 Loss: 0.012879188733697606\n",
            "Epoch: 140 Loss: 0.012830245871837814\n",
            "Epoch: 141 Loss: 0.012782037816171382\n",
            "Epoch: 142 Loss: 0.012734549023781721\n",
            "Epoch: 143 Loss: 0.01268776436930798\n",
            "Epoch: 144 Loss: 0.012641669131472433\n",
            "Epoch: 145 Loss: 0.012596248980110228\n",
            "Epoch: 146 Loss: 0.012551489963680414\n",
            "Epoch: 147 Loss: 0.012507378497238028\n",
            "Epoch: 148 Loss: 0.01246390135084812\n",
            "Epoch: 149 Loss: 0.01242104563842323\n",
            "Epoch: 150 Loss: 0.01237879880696704\n",
            "Epoch: 151 Loss: 0.012337148626207245\n",
            "Epoch: 152 Loss: 0.01229608317860195\n",
            "Epoch: 153 Loss: 0.012255590849704207\n",
            "Epoch: 154 Loss: 0.012215660318870204\n",
            "Epoch: 155 Loss: 0.012176280550297285\n",
            "Epoch: 156 Loss: 0.01213744078437838\n",
            "Epoch: 157 Loss: 0.01209913052936033\n",
            "Epoch: 158 Loss: 0.012061339553293856\n",
            "Epoch: 159 Loss: 0.012024057876263775\n",
            "Epoch: 160 Loss: 0.011987275762888135\n",
            "Epoch: 161 Loss: 0.011950983715075804\n",
            "Epoch: 162 Loss: 0.011915172465032452\n",
            "Epoch: 163 Loss: 0.01187983296850502\n",
            "Epoch: 164 Loss: 0.011844956398255618\n",
            "Epoch: 165 Loss: 0.011810534137755839\n",
            "Epoch: 166 Loss: 0.011776557775092922\n",
            "Epoch: 167 Loss: 0.011743019097079756\n",
            "Epoch: 168 Loss: 0.011709910083560846\n",
            "Epoch: 169 Loss: 0.011677222901906722\n",
            "Epoch: 170 Loss: 0.011644949901689699\n",
            "Epoch: 171 Loss: 0.011613083609534027\n",
            "Epoch: 172 Loss: 0.011581616724133895\n",
            "Epoch: 173 Loss: 0.011550542111432974\n",
            "Epoch: 174 Loss: 0.011519852799959306\n",
            "Epoch: 175 Loss: 0.011489541976309924\n",
            "Epoch: 176 Loss: 0.011459602980779403\n",
            "Epoch: 177 Loss: 0.011430029303127044\n",
            "Epoch: 178 Loss: 0.011400814578477641\n",
            "Epoch: 179 Loss: 0.011371952583350583\n",
            "Epoch: 180 Loss: 0.011343437231812798\n",
            "Epoch: 181 Loss: 0.011315262571750848\n",
            "Epoch: 182 Loss: 0.01128742278125761\n",
            "Epoch: 183 Loss: 0.011259912165129535\n",
            "Epoch: 184 Loss: 0.011232725151470085\n",
            "Epoch: 185 Loss: 0.0112058562883957\n",
            "Epoch: 186 Loss: 0.011179300240840177\n",
            "Epoch: 187 Loss: 0.011153051787453891\n",
            "Epoch: 188 Loss: 0.011127105817594317\n",
            "Epoch: 189 Loss: 0.011101457328404256\n",
            "Epoch: 190 Loss: 0.011076101421974533\n",
            "Epoch: 191 Loss: 0.01105103330258784\n",
            "Epoch: 192 Loss: 0.011026248274040484\n",
            "Epoch: 193 Loss: 0.011001741737039054\n",
            "Epoch: 194 Loss: 0.010977509186668972\n",
            "Epoch: 195 Loss: 0.010953546209931822\n",
            "Epoch: 196 Loss: 0.010929848483348693\n",
            "Epoch: 197 Loss: 0.010906411770626697\n",
            "Epoch: 198 Loss: 0.010883231920385629\n",
            "Epoch: 199 Loss: 0.010860304863942292\n",
            "Epoch: 200 Loss: 0.010837626613149336\n",
            "Epoch: 201 Loss: 0.01081519325828609\n",
            "Epoch: 202 Loss: 0.010793000965998507\n",
            "Epoch: 203 Loss: 0.010771045977285322\n",
            "Epoch: 204 Loss: 0.010749324605527655\n",
            "Epoch: 205 Loss: 0.010727833234558982\n",
            "Epoch: 206 Loss: 0.010706568316772696\n",
            "Epoch: 207 Loss: 0.010685526371263769\n",
            "Epoch: 208 Loss: 0.010664703982001497\n",
            "Epoch: 209 Loss: 0.010644097796029857\n",
            "Epoch: 210 Loss: 0.010623704521691693\n",
            "Epoch: 211 Loss: 0.010603520926872967\n",
            "Epoch: 212 Loss: 0.010583543837262722\n",
            "Epoch: 213 Loss: 0.01056377013462433\n",
            "Epoch: 214 Loss: 0.010544196755072956\n",
            "Epoch: 215 Loss: 0.010524820687353746\n",
            "Epoch: 216 Loss: 0.01050563897111457\n",
            "Epoch: 217 Loss: 0.010486648695166622\n",
            "Epoch: 218 Loss: 0.010467846995725016\n",
            "Epoch: 219 Loss: 0.010449231054620832\n",
            "Epoch: 220 Loss: 0.010430798097474511\n",
            "Epoch: 221 Loss: 0.010412545391819385\n",
            "Epoch: 222 Loss: 0.010394470245162114\n",
            "Epoch: 223 Loss: 0.010376570002964726\n",
            "Epoch: 224 Loss: 0.010358842046530762\n",
            "Epoch: 225 Loss: 0.010341283790774536\n",
            "Epoch: 226 Loss: 0.010323892681849038\n",
            "Epoch: 227 Loss: 0.010306666194603686\n",
            "Epoch: 228 Loss: 0.01028960182983723\n",
            "Epoch: 229 Loss: 0.01027269711130473\n",
            "Epoch: 230 Loss: 0.010255949582429157\n",
            "Epoch: 231 Loss: 0.010239356802657832\n",
            "Epoch: 232 Loss: 0.010222916343391026\n",
            "Epoch: 233 Loss: 0.010206625783394357\n",
            "Epoch: 234 Loss: 0.010190482703586147\n",
            "Epoch: 235 Loss: 0.010174484681065847\n",
            "Epoch: 236 Loss: 0.010158629282216704\n",
            "Epoch: 237 Loss: 0.010142914054674929\n",
            "Epoch: 238 Loss: 0.010127336517903493\n",
            "Epoch: 239 Loss: 0.010111894152039455\n",
            "Epoch: 240 Loss: 0.010096584384592074\n",
            "Epoch: 241 Loss: 0.010081404574448853\n",
            "Epoch: 242 Loss: 0.0100663519924858\n",
            "Epoch: 243 Loss: 0.01005142379786204\n",
            "Epoch: 244 Loss: 0.01003661700878602\n",
            "Epoch: 245 Loss: 0.01002192846613711\n",
            "Epoch: 246 Loss: 0.010007354787767124\n",
            "Epoch: 247 Loss: 0.00999289231051964\n",
            "Epoch: 248 Loss: 0.009978537015883943\n",
            "Epoch: 249 Loss: 0.009964284433581038\n",
            "Epoch: 250 Loss: 0.009950129515001122\n",
            "Epoch: 251 Loss: 0.009936066464863514\n",
            "Epoch: 252 Loss: 0.009922088514073515\n",
            "Epoch: 253 Loss: 0.00990818760837963\n",
            "Epoch: 254 Loss: 0.009894353974155869\n",
            "Epoch: 255 Loss: 0.00988057550104787\n",
            "Epoch: 256 Loss: 0.009866836845160184\n",
            "Epoch: 257 Loss: 0.00985311809434986\n",
            "Epoch: 258 Loss: 0.009839392726465406\n",
            "Epoch: 259 Loss: 0.009825624386116076\n",
            "Epoch: 260 Loss: 0.00981176160767608\n",
            "Epoch: 261 Loss: 0.00979772879996817\n",
            "Epoch: 262 Loss: 0.00978341004629329\n",
            "Epoch: 263 Loss: 0.009768618167123386\n",
            "Epoch: 264 Loss: 0.009753031052886822\n",
            "Epoch: 265 Loss: 0.009736047731334192\n",
            "Epoch: 266 Loss: 0.009716420907547365\n",
            "Epoch: 267 Loss: 0.00969115260034849\n",
            "Epoch: 268 Loss: 0.009651323293679922\n",
            "Epoch: 269 Loss: 0.009560242183843692\n",
            "Epoch: 270 Loss: 0.009202708177147738\n",
            "Epoch: 271 Loss: 0.008713466709790115\n",
            "Epoch: 272 Loss: 0.008696500582398094\n",
            "Epoch: 273 Loss: 0.008682734596865999\n",
            "Epoch: 274 Loss: 0.008669496631950345\n",
            "Epoch: 275 Loss: 0.008656675057470928\n",
            "Epoch: 276 Loss: 0.008644192416227686\n",
            "Epoch: 277 Loss: 0.008632030069608294\n",
            "Epoch: 278 Loss: 0.00862066406751856\n",
            "Epoch: 279 Loss: 0.008609402164857794\n",
            "Epoch: 280 Loss: 0.00859824223557398\n",
            "Epoch: 281 Loss: 0.008587182263727821\n",
            "Epoch: 282 Loss: 0.008576220333505667\n",
            "Epoch: 283 Loss: 0.00856535462039256\n",
            "Epoch: 284 Loss: 0.008554583383349848\n",
            "Epoch: 285 Loss: 0.008543950365367778\n",
            "Epoch: 286 Loss: 0.008533555416375911\n",
            "Epoch: 287 Loss: 0.008523240309747864\n",
            "Epoch: 288 Loss: 0.008513004157962948\n",
            "Epoch: 289 Loss: 0.008502846086321587\n",
            "Epoch: 290 Loss: 0.00849276523271848\n",
            "Epoch: 291 Loss: 0.00848276074742051\n",
            "Epoch: 292 Loss: 0.008472831792849196\n",
            "Epoch: 293 Loss: 0.008462977543367788\n",
            "Epoch: 294 Loss: 0.008453197185072658\n",
            "Epoch: 295 Loss: 0.008443489915589027\n",
            "Epoch: 296 Loss: 0.008433854943870875\n",
            "Epoch: 297 Loss: 0.008424291490004955\n",
            "Epoch: 298 Loss: 0.008414798785018825\n",
            "Epoch: 299 Loss: 0.00840537607069274\n",
            "Epoch: 300 Loss: 0.008396022599375396\n",
            "Epoch: 301 Loss: 0.008386737633803435\n",
            "Epoch: 302 Loss: 0.00837752044692449\n",
            "Epoch: 303 Loss: 0.008368370321723877\n",
            "Epoch: 304 Loss: 0.008359286551054734\n",
            "Epoch: 305 Loss: 0.008350268437471496\n",
            "Epoch: 306 Loss: 0.008341315293066774\n",
            "Epoch: 307 Loss: 0.008332426439311433\n",
            "Epoch: 308 Loss: 0.008323601206897823\n",
            "Epoch: 309 Loss: 0.008314838935586131\n",
            "Epoch: 310 Loss: 0.00830613897405374\n",
            "Epoch: 311 Loss: 0.00829750067974749\n",
            "Epoch: 312 Loss: 0.008288923418738974\n",
            "Epoch: 313 Loss: 0.008280406565582512\n",
            "Epoch: 314 Loss: 0.008271949503175927\n",
            "Epoch: 315 Loss: 0.008263551622624054\n",
            "Epoch: 316 Loss: 0.008255212323104829\n",
            "Epoch: 317 Loss: 0.008246931011738038\n",
            "Epoch: 318 Loss: 0.008238707103456512\n",
            "Epoch: 319 Loss: 0.00823054002087982\n",
            "Epoch: 320 Loss: 0.008222429194190365\n",
            "Epoch: 321 Loss: 0.008214374061011824\n",
            "Epoch: 322 Loss: 0.008206374066289983\n",
            "Epoch: 323 Loss: 0.008198428662175686\n",
            "Epoch: 324 Loss: 0.00819053730791013\n",
            "Epoch: 325 Loss: 0.008182699469712219\n",
            "Epoch: 326 Loss: 0.008174914620668073\n",
            "Epoch: 327 Loss: 0.0081671822406226\n",
            "Epoch: 328 Loss: 0.0081595018160731\n",
            "Epoch: 329 Loss: 0.008151872840064866\n",
            "Epoch: 330 Loss: 0.008144294812088643\n",
            "Epoch: 331 Loss: 0.00813676723798006\n",
            "Epoch: 332 Loss: 0.008129289629820907\n",
            "Epoch: 333 Loss: 0.008121861505842197\n",
            "Epoch: 334 Loss: 0.008114482390329032\n",
            "Epoch: 335 Loss: 0.008107151813527188\n",
            "Epoch: 336 Loss: 0.008099869311551423\n",
            "Epoch: 337 Loss: 0.008092634426295446\n",
            "Epoch: 338 Loss: 0.008085446705343518\n",
            "Epoch: 339 Loss: 0.008078305701883635\n",
            "Epoch: 340 Loss: 0.008071210974622316\n",
            "Epoch: 341 Loss: 0.008064162087700886\n",
            "Epoch: 342 Loss: 0.008057158610613275\n",
            "Epoch: 343 Loss: 0.008050200118125305\n",
            "Epoch: 344 Loss: 0.008043286190195353\n",
            "Epoch: 345 Loss: 0.008036416411896516\n",
            "Epoch: 346 Loss: 0.008029590373340065\n",
            "Epoch: 347 Loss: 0.008022807669600347\n",
            "Epoch: 348 Loss: 0.008016067900640914\n",
            "Epoch: 349 Loss: 0.008009370671242048\n",
            "Epoch: 350 Loss: 0.008002715590929478\n",
            "Epoch: 351 Loss: 0.007996102273904382\n",
            "Epoch: 352 Loss: 0.007989530338974581\n",
            "Epoch: 353 Loss: 0.007982999409487006\n",
            "Epoch: 354 Loss: 0.007976509113261232\n",
            "Epoch: 355 Loss: 0.007970059082524236\n",
            "Epoch: 356 Loss: 0.007963648953846264\n",
            "Epoch: 357 Loss: 0.007957278368077818\n",
            "Epoch: 358 Loss: 0.007950946970287628\n",
            "Epoch: 359 Loss: 0.00794465440970185\n",
            "Epoch: 360 Loss: 0.007938400339644169\n",
            "Epoch: 361 Loss: 0.007932184417476968\n",
            "Epoch: 362 Loss: 0.007926006304543513\n",
            "Epoch: 363 Loss: 0.00791986566611109\n",
            "Epoch: 364 Loss: 0.007913762171315118\n",
            "Epoch: 365 Loss: 0.007907695493104189\n",
            "Epoch: 366 Loss: 0.007901665308186074\n",
            "Epoch: 367 Loss: 0.007895671296974565\n",
            "Epoch: 368 Loss: 0.007889713143537265\n",
            "Epoch: 369 Loss: 0.007883790535544234\n",
            "Epoch: 370 Loss: 0.00787790316421743\n",
            "Epoch: 371 Loss: 0.007872050724281137\n",
            "Epoch: 372 Loss: 0.007866232913913038\n",
            "Epoch: 373 Loss: 0.00786044943469621\n",
            "Epoch: 374 Loss: 0.007854699991571864\n",
            "Epoch: 375 Loss: 0.007848984292792884\n",
            "Epoch: 376 Loss: 0.007843302049878098\n",
            "Epoch: 377 Loss: 0.007837652977567363\n",
            "Epoch: 378 Loss: 0.007832036793777253\n",
            "Epoch: 379 Loss: 0.007826453219557605\n",
            "Epoch: 380 Loss: 0.0078209019790487\n",
            "Epoch: 381 Loss: 0.007815382799439146\n",
            "Epoch: 382 Loss: 0.007809895410924443\n",
            "Epoch: 383 Loss: 0.007804439546666223\n",
            "Epoch: 384 Loss: 0.007799014942752116\n",
            "Epoch: 385 Loss: 0.007793621338156307\n",
            "Epoch: 386 Loss: 0.007788258474700655\n",
            "Epoch: 387 Loss: 0.007782926097016527\n",
            "Epoch: 388 Loss: 0.0077776239525071365\n",
            "Epoch: 389 Loss: 0.0077723517913105495\n",
            "Epoch: 390 Loss: 0.007767109366263227\n",
            "Epoch: 391 Loss: 0.0077618964328642175\n",
            "Epoch: 392 Loss: 0.007756712749239817\n",
            "Epoch: 393 Loss: 0.007751558076108841\n",
            "Epoch: 394 Loss: 0.007746432176748426\n",
            "Epoch: 395 Loss: 0.0077413348169603855\n",
            "Epoch: 396 Loss: 0.0077362657650380555\n",
            "Epoch: 397 Loss: 0.007731224791733691\n",
            "Epoch: 398 Loss: 0.007726211670226342\n",
            "Epoch: 399 Loss: 0.0077212261760902165\n",
            "Epoch: 400 Loss: 0.007716268087263592\n",
            "Epoch: 401 Loss: 0.0077113371840181345\n",
            "Epoch: 402 Loss: 0.007706433248928713\n",
            "Epoch: 403 Loss: 0.0077015560668436895\n",
            "Epoch: 404 Loss: 0.0076967054248556705\n",
            "Epoch: 405 Loss: 0.00769188111227267\n",
            "Epoch: 406 Loss: 0.007687082920589736\n",
            "Epoch: 407 Loss: 0.0076823106434610175\n",
            "Epoch: 408 Loss: 0.007677564076672216\n",
            "Epoch: 409 Loss: 0.0076728430181134885\n",
            "Epoch: 410 Loss: 0.007668147267752753\n",
            "Epoch: 411 Loss: 0.007663476627609359\n",
            "Epoch: 412 Loss: 0.007658830901728224\n",
            "Epoch: 413 Loss: 0.007654209896154297\n",
            "Epoch: 414 Loss: 0.00764961341890742\n",
            "Epoch: 415 Loss: 0.007645041279957569\n",
            "Epoch: 416 Loss: 0.007640493291200452\n",
            "Epoch: 417 Loss: 0.007635969266433479\n",
            "Epoch: 418 Loss: 0.0076314690213321245\n",
            "Epoch: 419 Loss: 0.007626992373426535\n",
            "Epoch: 420 Loss: 0.0076225391420785904\n",
            "Epoch: 421 Loss: 0.007618109148459235\n",
            "Epoch: 422 Loss: 0.007613702215526187\n",
            "Epoch: 423 Loss: 0.007609318168001908\n",
            "Epoch: 424 Loss: 0.007604956832351982\n",
            "Epoch: 425 Loss: 0.0076006180367637465\n",
            "Epoch: 426 Loss: 0.00759630161112525\n",
            "Epoch: 427 Loss: 0.007592007387004502\n",
            "Epoch: 428 Loss: 0.0075877351976290424\n",
            "Epoch: 429 Loss: 0.007583484877865807\n",
            "Epoch: 430 Loss: 0.007579256264201273\n",
            "Epoch: 431 Loss: 0.007575049194721897\n",
            "Epoch: 432 Loss: 0.007570863509094797\n",
            "Epoch: 433 Loss: 0.0075666990485487706\n",
            "Epoch: 434 Loss: 0.00756255565585556\n",
            "Epoch: 435 Loss: 0.007558433175311352\n",
            "Epoch: 436 Loss: 0.007554331452718568\n",
            "Epoch: 437 Loss: 0.007550250335367931\n",
            "Epoch: 438 Loss: 0.007546189672020748\n",
            "Epoch: 439 Loss: 0.007542149312891501\n",
            "Epoch: 440 Loss: 0.007538129109630534\n",
            "Epoch: 441 Loss: 0.007534128915307217\n",
            "Epoch: 442 Loss: 0.007530148584393174\n",
            "Epoch: 443 Loss: 0.007526187972745742\n",
            "Epoch: 444 Loss: 0.00752224693759178\n",
            "Epoch: 445 Loss: 0.007518325337511596\n",
            "Epoch: 446 Loss: 0.00751442303242315\n",
            "Epoch: 447 Loss: 0.007510539883566426\n",
            "Epoch: 448 Loss: 0.0075066757534881\n",
            "Epoch: 449 Loss: 0.007502830506026362\n",
            "Epoch: 450 Loss: 0.007499004006295939\n",
            "Epoch: 451 Loss: 0.007495196120673345\n",
            "Epoch: 452 Loss: 0.007491406716782354\n",
            "Epoch: 453 Loss: 0.007487635663479637\n",
            "Epoch: 454 Loss: 0.0074838828308406045\n",
            "Epoch: 455 Loss: 0.007480148090145448\n",
            "Epoch: 456 Loss: 0.007476431313865357\n",
            "Epoch: 457 Loss: 0.007472732375648962\n",
            "Epoch: 458 Loss: 0.0074690511503088895\n",
            "Epoch: 459 Loss: 0.007465387513808593\n",
            "Epoch: 460 Loss: 0.007461741343249272\n",
            "Epoch: 461 Loss: 0.007458112516857014\n",
            "Epoch: 462 Loss: 0.007454500913970103\n",
            "Epoch: 463 Loss: 0.007450906415026502\n",
            "Epoch: 464 Loss: 0.007447328901551499\n",
            "Epoch: 465 Loss: 0.007443768256145467\n",
            "Epoch: 466 Loss: 0.007440224362471891\n",
            "Epoch: 467 Loss: 0.007436697105245492\n",
            "Epoch: 468 Loss: 0.007433186370220488\n",
            "Epoch: 469 Loss: 0.007429692044179043\n",
            "Epoch: 470 Loss: 0.007426214014919854\n",
            "Epoch: 471 Loss: 0.00742275217124693\n",
            "Epoch: 472 Loss: 0.007419306402958456\n",
            "Epoch: 473 Loss: 0.007415876600835819\n",
            "Epoch: 474 Loss: 0.007412462656632833\n",
            "Epoch: 475 Loss: 0.0074090644630650055\n",
            "Epoch: 476 Loss: 0.007405681913799037\n",
            "Epoch: 477 Loss: 0.0074023149034424165\n",
            "Epoch: 478 Loss: 0.007398963327533141\n",
            "Epoch: 479 Loss: 0.007395627082529611\n",
            "Epoch: 480 Loss: 0.007392306065800579\n",
            "Epoch: 481 Loss: 0.007389000175615323\n",
            "Epoch: 482 Loss: 0.007385709311133885\n",
            "Epoch: 483 Loss: 0.007382433372397435\n",
            "Epoch: 484 Loss: 0.007379172260318779\n",
            "Epoch: 485 Loss: 0.007375925876673021\n",
            "Epoch: 486 Loss: 0.007372694124088244\n",
            "Epoch: 487 Loss: 0.0073694769060364184\n",
            "Epoch: 488 Loss: 0.007366274126824362\n",
            "Epoch: 489 Loss: 0.0073630856915848164\n",
            "Epoch: 490 Loss: 0.007359911506267688\n",
            "Epoch: 491 Loss: 0.007356751477631341\n",
            "Epoch: 492 Loss: 0.007353605513234034\n",
            "Epoch: 493 Loss: 0.007350473521425429\n",
            "Epoch: 494 Loss: 0.007347355411338276\n",
            "Epoch: 495 Loss: 0.007344251092880112\n",
            "Epoch: 496 Loss: 0.007341160476725124\n",
            "Epoch: 497 Loss: 0.007338083474306116\n",
            "Epoch: 498 Loss: 0.007335019997806549\n",
            "Epoch: 499 Loss: 0.007331969960152694\n",
            "Epoch: 500 Loss: 0.007328933275005858\n",
            "Epoch: 501 Loss: 0.007325909856754741\n",
            "Epoch: 502 Loss: 0.007322899620507888\n",
            "Epoch: 503 Loss: 0.007319902482086215\n",
            "Epoch: 504 Loss: 0.007316918358015597\n",
            "Epoch: 505 Loss: 0.0073139471655196305\n",
            "Epoch: 506 Loss: 0.0073109888225124\n",
            "Epoch: 507 Loss: 0.0073080432475914016\n",
            "Epoch: 508 Loss: 0.0073051103600304795\n",
            "Epoch: 509 Loss: 0.007302190079772936\n",
            "Epoch: 510 Loss: 0.007299282327424645\n",
            "Epoch: 511 Loss: 0.007296387024247303\n",
            "Epoch: 512 Loss: 0.007293504092151732\n",
            "Epoch: 513 Loss: 0.007290633453691274\n",
            "Epoch: 514 Loss: 0.007287775032055279\n",
            "Epoch: 515 Loss: 0.007284928751062642\n",
            "Epoch: 516 Loss: 0.007282094535155423\n",
            "Epoch: 517 Loss: 0.0072792723093926025\n",
            "Epoch: 518 Loss: 0.00727646199944383\n",
            "Epoch: 519 Loss: 0.007273663531583264\n",
            "Epoch: 520 Loss: 0.007270876832683539\n",
            "Epoch: 521 Loss: 0.007268101830209746\n",
            "Epoch: 522 Loss: 0.007265338452213509\n",
            "Epoch: 523 Loss: 0.007262586627327142\n",
            "Epoch: 524 Loss: 0.007259846284757854\n",
            "Epoch: 525 Loss: 0.0072571173542820295\n",
            "Epoch: 526 Loss: 0.007254399766239585\n",
            "Epoch: 527 Loss: 0.007251693451528383\n",
            "Epoch: 528 Loss: 0.007248998341598709\n",
            "Epoch: 529 Loss: 0.007246314368447821\n",
            "Epoch: 530 Loss: 0.007243641464614584\n",
            "Epoch: 531 Loss: 0.007240979563174095\n",
            "Epoch: 532 Loss: 0.007238328597732468\n",
            "Epoch: 533 Loss: 0.007235688502421607\n",
            "Epoch: 534 Loss: 0.007233059211894059\n",
            "Epoch: 535 Loss: 0.007230440661317964\n",
            "Epoch: 536 Loss: 0.007227832786372002\n",
            "Epoch: 537 Loss: 0.0072252355232404215\n",
            "Epoch: 538 Loss: 0.007222648808608168\n",
            "Epoch: 539 Loss: 0.0072200725796559995\n",
            "Epoch: 540 Loss: 0.0072175067740557165\n",
            "Epoch: 541 Loss: 0.007214951329965396\n",
            "Epoch: 542 Loss: 0.007212406186024729\n",
            "Epoch: 543 Loss: 0.007209871281350386\n",
            "Epoch: 544 Loss: 0.007207346555531423\n",
            "Epoch: 545 Loss: 0.007204831948624764\n",
            "Epoch: 546 Loss: 0.007202327401150722\n",
            "Epoch: 547 Loss: 0.007199832854088583\n",
            "Epoch: 548 Loss: 0.007197348248872226\n",
            "Epoch: 549 Loss: 0.007194873527385791\n",
            "Epoch: 550 Loss: 0.0071924086319594275\n",
            "Epoch: 551 Loss: 0.007189953505365039\n",
            "Epoch: 552 Loss: 0.007187508090812114\n",
            "Epoch: 553 Loss: 0.007185072331943605\n",
            "Epoch: 554 Loss: 0.007182646172831822\n",
            "Epoch: 555 Loss: 0.007180229557974395\n",
            "Epoch: 556 Loss: 0.0071778224322903055\n",
            "Epoch: 557 Loss: 0.007175424741115888\n",
            "Epoch: 558 Loss: 0.007173036430200969\n",
            "Epoch: 559 Loss: 0.0071706574457049786\n",
            "Epoch: 560 Loss: 0.007168287734193124\n",
            "Epoch: 561 Loss: 0.007165927242632628\n",
            "Epoch: 562 Loss: 0.007163575918389009\n",
            "Epoch: 563 Loss: 0.007161233709222334\n",
            "Epoch: 564 Loss: 0.007158900563283607\n",
            "Epoch: 565 Loss: 0.00715657642911115\n",
            "Epoch: 566 Loss: 0.007154261255627011\n",
            "Epoch: 567 Loss: 0.007151954992133423\n",
            "Epoch: 568 Loss: 0.007149657588309358\n",
            "Epoch: 569 Loss: 0.007147368994207012\n",
            "Epoch: 570 Loss: 0.007145089160248407\n",
            "Epoch: 571 Loss: 0.007142818037222014\n",
            "Epoch: 572 Loss: 0.007140555576279391\n",
            "Epoch: 573 Loss: 0.00713830172893189\n",
            "Epoch: 574 Loss: 0.00713605644704737\n",
            "Epoch: 575 Loss: 0.0071338196828469565\n",
            "Epoch: 576 Loss: 0.007131591388901848\n",
            "Epoch: 577 Loss: 0.007129371518130137\n",
            "Epoch: 578 Loss: 0.0071271600237936905\n",
            "Epoch: 579 Loss: 0.007124956859495034\n",
            "Epoch: 580 Loss: 0.00712276197917428\n",
            "Epoch: 581 Loss: 0.007120575337106097\n",
            "Epoch: 582 Loss: 0.007118396887896712\n",
            "Epoch: 583 Loss: 0.007116226586480936\n",
            "Epoch: 584 Loss: 0.007114064388119238\n",
            "Epoch: 585 Loss: 0.0071119102483947995\n",
            "Epoch: 586 Loss: 0.007109764123210674\n",
            "Epoch: 587 Loss: 0.007107625968786932\n",
            "Epoch: 588 Loss: 0.00710549574165783\n",
            "Epoch: 589 Loss: 0.0071033733986690505\n",
            "Epoch: 590 Loss: 0.0071012588969749085\n",
            "Epoch: 591 Loss: 0.00709915219403565\n",
            "Epoch: 592 Loss: 0.007097053247614745\n",
            "Epoch: 593 Loss: 0.007094962015776215\n",
            "Epoch: 594 Loss: 0.007092878456881993\n",
            "Epoch: 595 Loss: 0.007090802529589295\n",
            "Epoch: 596 Loss: 0.007088734192848056\n",
            "Epoch: 597 Loss: 0.007086673405898347\n",
            "Epoch: 598 Loss: 0.00708462012826785\n",
            "Epoch: 599 Loss: 0.007082574319769332\n",
            "Epoch: 600 Loss: 0.007080535940498205\n",
            "Epoch: 601 Loss: 0.007078504950830022\n",
            "Epoch: 602 Loss: 0.007076481311418067\n",
            "Epoch: 603 Loss: 0.007074464983190951\n",
            "Epoch: 604 Loss: 0.007072455927350219\n",
            "Epoch: 605 Loss: 0.00707045410536801\n",
            "Epoch: 606 Loss: 0.007068459478984702\n",
            "Epoch: 607 Loss: 0.007066472010206623\n",
            "Epoch: 608 Loss: 0.007064491661303763\n",
            "Epoch: 609 Loss: 0.007062518394807504\n",
            "Epoch: 610 Loss: 0.007060552173508374\n",
            "Epoch: 611 Loss: 0.007058592960453846\n",
            "Epoch: 612 Loss: 0.007056640718946151\n",
            "Epoch: 613 Loss: 0.007054695412540088\n",
            "Epoch: 614 Loss: 0.007052757005040885\n",
            "Epoch: 615 Loss: 0.007050825460502053\n",
            "Epoch: 616 Loss: 0.007048900743223323\n",
            "Epoch: 617 Loss: 0.007046982817748507\n",
            "Epoch: 618 Loss: 0.007045071648863481\n",
            "Epoch: 619 Loss: 0.00704316720159412\n",
            "Epoch: 620 Loss: 0.007041269441204271\n",
            "Epoch: 621 Loss: 0.007039378333193752\n",
            "Epoch: 622 Loss: 0.0070374938432963855\n",
            "Epoch: 623 Loss: 0.007035615937478033\n",
            "Epoch: 624 Loss: 0.007033744581934632\n",
            "Epoch: 625 Loss: 0.0070318797430902885\n",
            "Epoch: 626 Loss: 0.007030021387595372\n",
            "Epoch: 627 Loss: 0.007028169482324626\n",
            "Epoch: 628 Loss: 0.007026323994375316\n",
            "Epoch: 629 Loss: 0.007024484891065347\n",
            "Epoch: 630 Loss: 0.0070226521399314725\n",
            "Epoch: 631 Loss: 0.0070208257087274566\n",
            "Epoch: 632 Loss: 0.007019005565422286\n",
            "Epoch: 633 Loss: 0.007017191678198405\n",
            "Epoch: 634 Loss: 0.007015384015449941\n",
            "Epoch: 635 Loss: 0.007013582545780971\n",
            "Epoch: 636 Loss: 0.007011787238003802\n",
            "Epoch: 637 Loss: 0.0070099980611372445\n",
            "Epoch: 638 Loss: 0.0070082149844049365\n",
            "Epoch: 639 Loss: 0.00700643797723367\n",
            "Epoch: 640 Loss: 0.007004667009251719\n",
            "Epoch: 641 Loss: 0.007002902050287217\n",
            "Epoch: 642 Loss: 0.007001143070366509\n",
            "Epoch: 643 Loss: 0.006999390039712554\n",
            "Epoch: 644 Loss: 0.006997642928743334\n",
            "Epoch: 645 Loss: 0.00699590170807026\n",
            "Epoch: 646 Loss: 0.006994166348496624\n",
            "Epoch: 647 Loss: 0.00699243682101603\n",
            "Epoch: 648 Loss: 0.0069907130968108735\n",
            "Epoch: 649 Loss: 0.006988995147250824\n",
            "Epoch: 650 Loss: 0.006987282943891307\n",
            "Epoch: 651 Loss: 0.006985576458472015\n",
            "Epoch: 652 Loss: 0.00698387566291545\n",
            "Epoch: 653 Loss: 0.006982180529325438\n",
            "Epoch: 654 Loss: 0.0069804910299856955\n",
            "Epoch: 655 Loss: 0.006978807137358373\n",
            "Epoch: 656 Loss: 0.006977128824082675\n",
            "Epoch: 657 Loss: 0.006975456062973411\n",
            "Epoch: 658 Loss: 0.006973788827019612\n",
            "Epoch: 659 Loss: 0.0069721270893831604\n",
            "Epoch: 660 Loss: 0.006970470823397413\n",
            "Epoch: 661 Loss: 0.006968820002565846\n",
            "Epoch: 662 Loss: 0.006967174600560723\n",
            "Epoch: 663 Loss: 0.006965534591221744\n",
            "Epoch: 664 Loss: 0.006963899948554737\n",
            "Epoch: 665 Loss: 0.0069622706467303815\n",
            "Epoch: 666 Loss: 0.006960646660082867\n",
            "Epoch: 667 Loss: 0.006959027963108631\n",
            "Epoch: 668 Loss: 0.006957414530465104\n",
            "Epoch: 669 Loss: 0.006955806336969447\n",
            "Epoch: 670 Loss: 0.0069542033575972885\n",
            "Epoch: 671 Loss: 0.006952605567481511\n",
            "Epoch: 672 Loss: 0.006951012941911016\n",
            "Epoch: 673 Loss: 0.006949425456329525\n",
            "Epoch: 674 Loss: 0.006947843086334359\n",
            "Epoch: 675 Loss: 0.0069462658076752766\n",
            "Epoch: 676 Loss: 0.006944693596253275\n",
            "Epoch: 677 Loss: 0.006943126428119422\n",
            "Epoch: 678 Loss: 0.006941564279473722\n",
            "Epoch: 679 Loss: 0.006940007126663943\n",
            "Epoch: 680 Loss: 0.006938454946184504\n",
            "Epoch: 681 Loss: 0.006936907714675333\n",
            "Epoch: 682 Loss: 0.0069353654089207625\n",
            "Epoch: 683 Loss: 0.006933828005848415\n",
            "Epoch: 684 Loss: 0.006932295482528126\n",
            "Epoch: 685 Loss: 0.006930767816170837\n",
            "Epoch: 686 Loss: 0.006929244984127535\n",
            "Epoch: 687 Loss: 0.00692772696388819\n",
            "Epoch: 688 Loss: 0.006926213733080696\n",
            "Epoch: 689 Loss: 0.0069247052694698\n",
            "Epoch: 690 Loss: 0.006923201550956133\n",
            "Epoch: 691 Loss: 0.006921702555575108\n",
            "Epoch: 692 Loss: 0.0069202082614959306\n",
            "Epoch: 693 Loss: 0.006918718647020613\n",
            "Epoch: 694 Loss: 0.006917233690582943\n",
            "Epoch: 695 Loss: 0.006915753370747515\n",
            "Epoch: 696 Loss: 0.006914277666208724\n",
            "Epoch: 697 Loss: 0.006912806555789805\n",
            "Epoch: 698 Loss: 0.006911340018441896\n",
            "Epoch: 699 Loss: 0.006909878033243035\n",
            "Epoch: 700 Loss: 0.0069084205793972465\n",
            "Epoch: 701 Loss: 0.006906967636233578\n",
            "Epoch: 702 Loss: 0.006905519183205195\n",
            "Epoch: 703 Loss: 0.006904075199888446\n",
            "Epoch: 704 Loss: 0.006902635665981948\n",
            "Epoch: 705 Loss: 0.006901200561305673\n",
            "Epoch: 706 Loss: 0.006899769865800074\n",
            "Epoch: 707 Loss: 0.006898343559525166\n",
            "Epoch: 708 Loss: 0.006896921622659659\n",
            "Epoch: 709 Loss: 0.0068955040355000875\n",
            "Epoch: 710 Loss: 0.006894090778459937\n",
            "Epoch: 711 Loss: 0.006892681832068782\n",
            "Epoch: 712 Loss: 0.006891277176971444\n",
            "Epoch: 713 Loss: 0.006889876793927131\n",
            "Epoch: 714 Loss: 0.006888480663808618\n",
            "Epoch: 715 Loss: 0.006887088767601423\n",
            "Epoch: 716 Loss: 0.0068857010864029405\n",
            "Epoch: 717 Loss: 0.006884317601421677\n",
            "Epoch: 718 Loss: 0.00688293829397642\n",
            "Epoch: 719 Loss: 0.006881563145495452\n",
            "Epoch: 720 Loss: 0.006880192137515726\n",
            "Epoch: 721 Loss: 0.006878825251682106\n",
            "Epoch: 722 Loss: 0.006877462469746568\n",
            "Epoch: 723 Loss: 0.006876103773567451\n",
            "Epoch: 724 Loss: 0.0068747491451086554\n",
            "Epoch: 725 Loss: 0.006873398566438901\n",
            "Epoch: 726 Loss: 0.006872052019730998\n",
            "Epoch: 727 Loss: 0.00687070948726105\n",
            "Epoch: 728 Loss: 0.006869370951407733\n",
            "Epoch: 729 Loss: 0.00686803639465158\n",
            "Epoch: 730 Loss: 0.0068667057995742305\n",
            "Epoch: 731 Loss: 0.006865379148857712\n",
            "Epoch: 732 Loss: 0.0068640564252837296\n",
            "Epoch: 733 Loss: 0.006862737611732947\n",
            "Epoch: 734 Loss: 0.006861422691184283\n",
            "Epoch: 735 Loss: 0.006860111646714222\n",
            "Epoch: 736 Loss: 0.006858804461496123\n",
            "Epoch: 737 Loss: 0.006857501118799516\n",
            "Epoch: 738 Loss: 0.006856201601989444\n",
            "Epoch: 739 Loss: 0.00685490589452576\n",
            "Epoch: 740 Loss: 0.006853613979962491\n",
            "Epoch: 741 Loss: 0.006852325841947143\n",
            "Epoch: 742 Loss: 0.006851041464220074\n",
            "Epoch: 743 Loss: 0.006849760830613812\n",
            "Epoch: 744 Loss: 0.006848483925052417\n",
            "Epoch: 745 Loss: 0.0068472107315508575\n",
            "Epoch: 746 Loss: 0.006845941234214358\n",
            "Epoch: 747 Loss: 0.0068446754172377545\n",
            "Epoch: 748 Loss: 0.006843413264904891\n",
            "Epoch: 749 Loss: 0.006842154761587997\n",
            "Epoch: 750 Loss: 0.006840899891747065\n",
            "Epoch: 751 Loss: 0.006839648639929241\n",
            "Epoch: 752 Loss: 0.006838400990768213\n",
            "Epoch: 753 Loss: 0.006837156928983622\n",
            "Epoch: 754 Loss: 0.006835916439380478\n",
            "Epoch: 755 Loss: 0.0068346795068485325\n",
            "Epoch: 756 Loss: 0.006833446116361733\n",
            "Epoch: 757 Loss: 0.006832216252977606\n",
            "Epoch: 758 Loss: 0.006830989901836711\n",
            "Epoch: 759 Loss: 0.006829767048162056\n",
            "Epoch: 760 Loss: 0.00682854767725853\n",
            "Epoch: 761 Loss: 0.006827331774512346\n",
            "Epoch: 762 Loss: 0.006826119325390478\n",
            "Epoch: 763 Loss: 0.006824910315440109\n",
            "Epoch: 764 Loss: 0.0068237047302881\n",
            "Epoch: 765 Loss: 0.006822502555640407\n",
            "Epoch: 766 Loss: 0.006821303777281591\n",
            "Epoch: 767 Loss: 0.006820108381074234\n",
            "Epoch: 768 Loss: 0.00681891635295845\n",
            "Epoch: 769 Loss: 0.006817727678951338\n",
            "Epoch: 770 Loss: 0.006816542345146452\n",
            "Epoch: 771 Loss: 0.00681536033771331\n",
            "Epoch: 772 Loss: 0.006814181642896857\n",
            "Epoch: 773 Loss: 0.006813006247016966\n",
            "Epoch: 774 Loss: 0.00681183413646792\n",
            "Epoch: 775 Loss: 0.006810665297717929\n",
            "Epoch: 776 Loss: 0.006809499717308611\n",
            "Epoch: 777 Loss: 0.006808337381854523\n",
            "Epoch: 778 Loss: 0.006807178278042667\n",
            "Epoch: 779 Loss: 0.006806022392631981\n",
            "Epoch: 780 Loss: 0.006804869712452879\n",
            "Epoch: 781 Loss: 0.0068037202244067676\n",
            "Epoch: 782 Loss: 0.006802573915465571\n",
            "Epoch: 783 Loss: 0.0068014307726712644\n",
            "Epoch: 784 Loss: 0.006800290783135404\n",
            "Epoch: 785 Loss: 0.006799153934038663\n",
            "Epoch: 786 Loss: 0.006798020212630382\n",
            "Epoch: 787 Loss: 0.006796889606228095\n",
            "Epoch: 788 Loss: 0.006795762102217102\n",
            "Epoch: 789 Loss: 0.006794637688049989\n",
            "Epoch: 790 Loss: 0.00679351635124622\n",
            "Epoch: 791 Loss: 0.00679239807939166\n",
            "Epoch: 792 Loss: 0.006791282860138169\n",
            "Epoch: 793 Loss: 0.0067901706812031485\n",
            "Epoch: 794 Loss: 0.006789061530369107\n",
            "Epoch: 795 Loss: 0.00678795539548324\n",
            "Epoch: 796 Loss: 0.006786852264457027\n",
            "Epoch: 797 Loss: 0.006785752125265766\n",
            "Epoch: 798 Loss: 0.0067846549659481906\n",
            "Epoch: 799 Loss: 0.006783560774606039\n",
            "Epoch: 800 Loss: 0.0067824695394036565\n",
            "Epoch: 801 Loss: 0.006781381248567575\n",
            "Epoch: 802 Loss: 0.006780295890386103\n",
            "Epoch: 803 Loss: 0.006779213453208948\n",
            "Epoch: 804 Loss: 0.0067781339254467925\n",
            "Epoch: 805 Loss: 0.0067770572955709145\n",
            "Epoch: 806 Loss: 0.006775983552112789\n",
            "Epoch: 807 Loss: 0.006774912683663699\n",
            "Epoch: 808 Loss: 0.006773844678874344\n",
            "Epoch: 809 Loss: 0.006772779526454474\n",
            "Epoch: 810 Loss: 0.00677171721517249\n",
            "Epoch: 811 Loss: 0.006770657733855067\n",
            "Epoch: 812 Loss: 0.006769601071386797\n",
            "Epoch: 813 Loss: 0.006768547216709798\n",
            "Epoch: 814 Loss: 0.006767496158823357\n",
            "Epoch: 815 Loss: 0.006766447886783555\n",
            "Epoch: 816 Loss: 0.006765402389702922\n",
            "Epoch: 817 Loss: 0.006764359656750053\n",
            "Epoch: 818 Loss: 0.006763319677149257\n",
            "Epoch: 819 Loss: 0.006762282440180217\n",
            "Epoch: 820 Loss: 0.0067612479351776035\n",
            "Epoch: 821 Loss: 0.00676021615153077\n",
            "Epoch: 822 Loss: 0.006759187078683376\n",
            "Epoch: 823 Loss: 0.006758160706133045\n",
            "Epoch: 824 Loss: 0.006757137023431027\n",
            "Epoch: 825 Loss: 0.006756116020181859\n",
            "Epoch: 826 Loss: 0.0067550976860430365\n",
            "Epoch: 827 Loss: 0.006754082010724662\n",
            "Epoch: 828 Loss: 0.006753068983989127\n",
            "Epoch: 829 Loss: 0.00675205859565078\n",
            "Epoch: 830 Loss: 0.006751050835575592\n",
            "Epoch: 831 Loss: 0.006750045693680848\n",
            "Epoch: 832 Loss: 0.0067490431599348056\n",
            "Epoch: 833 Loss: 0.006748043224356395\n",
            "Epoch: 834 Loss: 0.00674704587701488\n",
            "Epoch: 835 Loss: 0.006746051108029571\n",
            "Epoch: 836 Loss: 0.006745058907569483\n",
            "Epoch: 837 Loss: 0.006744069265853046\n",
            "Epoch: 838 Loss: 0.0067430821731478\n",
            "Epoch: 839 Loss: 0.006742097619770068\n",
            "Epoch: 840 Loss: 0.006741115596084671\n",
            "Epoch: 841 Loss: 0.00674013609250462\n",
            "Epoch: 842 Loss: 0.006739159099490827\n",
            "Epoch: 843 Loss: 0.006738184607551799\n",
            "Epoch: 844 Loss: 0.006737212607243328\n",
            "Epoch: 845 Loss: 0.006736243089168236\n",
            "Epoch: 846 Loss: 0.006735276043976061\n",
            "Epoch: 847 Loss: 0.006734311462362767\n",
            "Epoch: 848 Loss: 0.006733349335070466\n",
            "Epoch: 849 Loss: 0.006732389652887146\n",
            "Epoch: 850 Loss: 0.0067314324066463605\n",
            "Epoch: 851 Loss: 0.006730477587226968\n",
            "Epoch: 852 Loss: 0.006729525185552856\n",
            "Epoch: 853 Loss: 0.006728575192592654\n",
            "Epoch: 854 Loss: 0.006727627599359483\n",
            "Epoch: 855 Loss: 0.0067266823969106505\n",
            "Epoch: 856 Loss: 0.006725739576347404\n",
            "Epoch: 857 Loss: 0.006724799128814663\n",
            "Epoch: 858 Loss: 0.006723861045500741\n",
            "Epoch: 859 Loss: 0.006722925317637099\n",
            "Epoch: 860 Loss: 0.006721991936498065\n",
            "Epoch: 861 Loss: 0.006721060893400588\n",
            "Epoch: 862 Loss: 0.006720132179703973\n",
            "Epoch: 863 Loss: 0.006719205786809629\n",
            "Epoch: 864 Loss: 0.006718281706160818\n",
            "Epoch: 865 Loss: 0.00671735992924239\n",
            "Epoch: 866 Loss: 0.0067164404475805435\n",
            "Epoch: 867 Loss: 0.006715523252742568\n",
            "Epoch: 868 Loss: 0.006714608336336612\n",
            "Epoch: 869 Loss: 0.0067136956900114165\n",
            "Epoch: 870 Loss: 0.00671278530545609\n",
            "Epoch: 871 Loss: 0.006711877174399867\n",
            "Epoch: 872 Loss: 0.006710971288611844\n",
            "Epoch: 873 Loss: 0.006710067639900771\n",
            "Epoch: 874 Loss: 0.0067091662201147935\n",
            "Epoch: 875 Loss: 0.006708267021141231\n",
            "Epoch: 876 Loss: 0.006707370034906328\n",
            "Epoch: 877 Loss: 0.006706475253375042\n",
            "Epoch: 878 Loss: 0.006705582668550811\n",
            "Epoch: 879 Loss: 0.006704692272475304\n",
            "Epoch: 880 Loss: 0.0067038040572282255\n",
            "Epoch: 881 Loss: 0.006702918014927049\n",
            "Epoch: 882 Loss: 0.006702034137726845\n",
            "Epoch: 883 Loss: 0.006701152417820012\n",
            "Epoch: 884 Loss: 0.006700272847436081\n",
            "Epoch: 885 Loss: 0.006699395418841499\n",
            "Epoch: 886 Loss: 0.006698520124339402\n",
            "Epoch: 887 Loss: 0.0066976469562693985\n",
            "Epoch: 888 Loss: 0.006696775907007357\n",
            "Epoch: 889 Loss: 0.006695906968965199\n",
            "Epoch: 890 Loss: 0.006695040134590681\n",
            "Epoch: 891 Loss: 0.0066941753963671875\n",
            "Epoch: 892 Loss: 0.006693312746813518\n",
            "Epoch: 893 Loss: 0.0066924521784836955\n",
            "Epoch: 894 Loss: 0.0066915936839667435\n",
            "Epoch: 895 Loss: 0.006690737255886484\n",
            "Epoch: 896 Loss: 0.006689882886901346\n",
            "Epoch: 897 Loss: 0.006689030569704151\n",
            "Epoch: 898 Loss: 0.006688180297021923\n",
            "Epoch: 899 Loss: 0.00668733206161569\n",
            "Epoch: 900 Loss: 0.00668648585628028\n",
            "Epoch: 901 Loss: 0.006685641673844129\n",
            "Epoch: 902 Loss: 0.006684799507169089\n",
            "Epoch: 903 Loss: 0.006683959349150223\n",
            "Epoch: 904 Loss: 0.006683121192715638\n",
            "Epoch: 905 Loss: 0.0066822850308262734\n",
            "Epoch: 906 Loss: 0.006681450856475709\n",
            "Epoch: 907 Loss: 0.0066806186626900054\n",
            "Epoch: 908 Loss: 0.00667978844252748\n",
            "Epoch: 909 Loss: 0.0066789601890785534\n",
            "Epoch: 910 Loss: 0.006678133895465542\n",
            "Epoch: 911 Loss: 0.006677309554842497\n",
            "Epoch: 912 Loss: 0.006676487160395004\n",
            "Epoch: 913 Loss: 0.0066756667053400115\n",
            "Epoch: 914 Loss: 0.006674848182925652\n",
            "Epoch: 915 Loss: 0.006674031586431067\n",
            "Epoch: 916 Loss: 0.006673216909166222\n",
            "Epoch: 917 Loss: 0.006672404144471735\n",
            "Epoch: 918 Loss: 0.0066715932857186995\n",
            "Epoch: 919 Loss: 0.006670784326308529\n",
            "Epoch: 920 Loss: 0.006669977259672765\n",
            "Epoch: 921 Loss: 0.006669172079272906\n",
            "Epoch: 922 Loss: 0.006668368778600259\n",
            "Epoch: 923 Loss: 0.006667567351175741\n",
            "Epoch: 924 Loss: 0.006666767790549733\n",
            "Epoch: 925 Loss: 0.00666597009030192\n",
            "Epoch: 926 Loss: 0.006665174244041103\n",
            "Epoch: 927 Loss: 0.006664380245405051\n",
            "Epoch: 928 Loss: 0.006663588088060331\n",
            "Epoch: 929 Loss: 0.00666279776570215\n",
            "Epoch: 930 Loss: 0.006662009272054204\n",
            "Epoch: 931 Loss: 0.006661222600868492\n",
            "Epoch: 932 Loss: 0.006660437745925187\n",
            "Epoch: 933 Loss: 0.006659654701032464\n",
            "Epoch: 934 Loss: 0.006658873460026344\n",
            "Epoch: 935 Loss: 0.006658094016770534\n",
            "Epoch: 936 Loss: 0.00665731636515628\n",
            "Epoch: 937 Loss: 0.006656540499102224\n",
            "Epoch: 938 Loss: 0.006655766412554231\n",
            "Epoch: 939 Loss: 0.006654994099485246\n",
            "Epoch: 940 Loss: 0.0066542235538951415\n",
            "Epoch: 941 Loss: 0.006653454769810588\n",
            "Epoch: 942 Loss: 0.006652687741284874\n",
            "Epoch: 943 Loss: 0.006651922462397776\n",
            "Epoch: 944 Loss: 0.006651158927255418\n",
            "Epoch: 945 Loss: 0.006650397129990114\n",
            "Epoch: 946 Loss: 0.006649637064760223\n",
            "Epoch: 947 Loss: 0.006648878725750021\n",
            "Epoch: 948 Loss: 0.0066481221071695434\n",
            "Epoch: 949 Loss: 0.00664736720325444\n",
            "Epoch: 950 Loss: 0.006646614008265859\n",
            "Epoch: 951 Loss: 0.00664586251649028\n",
            "Epoch: 952 Loss: 0.006645112722239385\n",
            "Epoch: 953 Loss: 0.006644364619849932\n",
            "Epoch: 954 Loss: 0.0066436182036836\n",
            "Epoch: 955 Loss: 0.006642873468126863\n",
            "Epoch: 956 Loss: 0.006642130407590844\n",
            "Epoch: 957 Loss: 0.006641389016511208\n",
            "Epoch: 958 Loss: 0.006640649289347993\n",
            "Epoch: 959 Loss: 0.0066399112205855\n",
            "Epoch: 960 Loss: 0.006639174804732155\n",
            "Epoch: 961 Loss: 0.006638440036320379\n",
            "Epoch: 962 Loss: 0.0066377069099064585\n",
            "Epoch: 963 Loss: 0.00663697542007041\n",
            "Epoch: 964 Loss: 0.006636245561415864\n",
            "Epoch: 965 Loss: 0.006635517328569929\n",
            "Epoch: 966 Loss: 0.006634790716183062\n",
            "Epoch: 967 Loss: 0.006634065718928951\n",
            "Epoch: 968 Loss: 0.006633342331504388\n",
            "Epoch: 969 Loss: 0.006632620548629133\n",
            "Epoch: 970 Loss: 0.006631900365045813\n",
            "Epoch: 971 Loss: 0.0066311817755197785\n",
            "Epoch: 972 Loss: 0.006630464774838988\n",
            "Epoch: 973 Loss: 0.006629749357813892\n",
            "Epoch: 974 Loss: 0.006629035519277307\n",
            "Epoch: 975 Loss: 0.006628323254084298\n",
            "Epoch: 976 Loss: 0.00662761255711206\n",
            "Epoch: 977 Loss: 0.006626903423259804\n",
            "Epoch: 978 Loss: 0.0066261958474486335\n",
            "Epoch: 979 Loss: 0.006625489824621425\n",
            "Epoch: 980 Loss: 0.006624785349742716\n",
            "Epoch: 981 Loss: 0.006624082417798602\n",
            "Epoch: 982 Loss: 0.0066233810237966\n",
            "Epoch: 983 Loss: 0.006622681162765552\n",
            "Epoch: 984 Loss: 0.0066219828297555015\n",
            "Epoch: 985 Loss: 0.006621286019837579\n",
            "Epoch: 986 Loss: 0.006620590728103915\n",
            "Epoch: 987 Loss: 0.006619896949667503\n",
            "Epoch: 988 Loss: 0.006619204679662088\n",
            "Epoch: 989 Loss: 0.006618513913242083\n",
            "Epoch: 990 Loss: 0.006617824645582436\n",
            "Epoch: 991 Loss: 0.00661713687187853\n",
            "Epoch: 992 Loss: 0.00661645058734608\n",
            "Epoch: 993 Loss: 0.0066157657872210176\n",
            "Epoch: 994 Loss: 0.006615082466759393\n",
            "Epoch: 995 Loss: 0.006614400621237261\n",
            "Epoch: 996 Loss: 0.006613720245950581\n",
            "Epoch: 997 Loss: 0.0066130413362151235\n",
            "Epoch: 998 Loss: 0.0066123638873663475\n",
            "Epoch: 999 Loss: 0.0066116878947593116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6SfZXvt4JIC"
      },
      "source": [
        "# Prediction for a data point after the training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aiPKA9BV4JIC",
        "outputId": "78a37427-2a1f-4070-cec8-88f0925afcfd"
      },
      "source": [
        "my_neuralnet1.predict(np.array([8,4,9]).reshape((3,1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : \n",
            "[[8]\n",
            " [4]\n",
            " [9]]\n",
            "Output: \n",
            "[[1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dh2QW6kS4JIE",
        "outputId": "a28cb4c9-a8c0-4765-f63f-296216f76b9a"
      },
      "source": [
        "sns.set()\n",
        "plt.plot(loss1)\n",
        "plt.title(\"SGD Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"180100039.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QevUHCwgYFg"
      },
      "source": [
        "# MiniBatch \n",
        "**minibatch with lr = 0.001 and epochs = 1000**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGVym8B74JIG",
        "outputId": "5f639989-dee3-43ba-f6f6-55da60d8bbcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#minibatch\n",
        "my_neuralnet2 = Neural_Network(neurons, activation_functions)\n",
        "loss2 = my_neuralnet2.train(train_X,train_Y,minibatch=True,minibatchsize=20,lr=0.001,epochs=1000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 0.11058931137655806\n",
            "Epoch: 1 Loss: 0.11046119944126725\n",
            "Epoch: 2 Loss: 0.11033310947697096\n",
            "Epoch: 3 Loss: 0.11020504203739759\n",
            "Epoch: 4 Loss: 0.11007699767608654\n",
            "Epoch: 5 Loss: 0.1099489769463755\n",
            "Epoch: 6 Loss: 0.10982098040138734\n",
            "Epoch: 7 Loss: 0.10969300859401727\n",
            "Epoch: 8 Loss: 0.10956506207691998\n",
            "Epoch: 9 Loss: 0.10943714140249679\n",
            "Epoch: 10 Loss: 0.10930924712288266\n",
            "Epoch: 11 Loss: 0.10918137978993343\n",
            "Epoch: 12 Loss: 0.10905353995521287\n",
            "Epoch: 13 Loss: 0.10892572816997971\n",
            "Epoch: 14 Loss: 0.10879794498517512\n",
            "Epoch: 15 Loss: 0.10867019095140948\n",
            "Epoch: 16 Loss: 0.10854246661894981\n",
            "Epoch: 17 Loss: 0.10841477253770686\n",
            "Epoch: 18 Loss: 0.10828710925722214\n",
            "Epoch: 19 Loss: 0.10815947732665544\n",
            "Epoch: 20 Loss: 0.10803187729477175\n",
            "Epoch: 21 Loss: 0.10790430970992863\n",
            "Epoch: 22 Loss: 0.1077767751200634\n",
            "Epoch: 23 Loss: 0.10764927407268031\n",
            "Epoch: 24 Loss: 0.10752180711483804\n",
            "Epoch: 25 Loss: 0.10739437479313665\n",
            "Epoch: 26 Loss: 0.10726697765370513\n",
            "Epoch: 27 Loss: 0.10713961624218875\n",
            "Epoch: 28 Loss: 0.10701229110373602\n",
            "Epoch: 29 Loss: 0.10688500278298653\n",
            "Epoch: 30 Loss: 0.10675775182405799\n",
            "Epoch: 31 Loss: 0.10663053877053373\n",
            "Epoch: 32 Loss: 0.10650336416545002\n",
            "Epoch: 33 Loss: 0.10637622855128376\n",
            "Epoch: 34 Loss: 0.10624913246993963\n",
            "Epoch: 35 Loss: 0.10612207646273769\n",
            "Epoch: 36 Loss: 0.10599506107040091\n",
            "Epoch: 37 Loss: 0.10586808683304268\n",
            "Epoch: 38 Loss: 0.10574115429015435\n",
            "Epoch: 39 Loss: 0.10561426398059283\n",
            "Epoch: 40 Loss: 0.10548741644256805\n",
            "Epoch: 41 Loss: 0.10536061221363088\n",
            "Epoch: 42 Loss: 0.10523385183066054\n",
            "Epoch: 43 Loss: 0.10510713582985233\n",
            "Epoch: 44 Loss: 0.10498046474670543\n",
            "Epoch: 45 Loss: 0.10485383911601052\n",
            "Epoch: 46 Loss: 0.10472725947183777\n",
            "Epoch: 47 Loss: 0.10460072634752436\n",
            "Epoch: 48 Loss: 0.10447424027566252\n",
            "Epoch: 49 Loss: 0.10434780178808731\n",
            "Epoch: 50 Loss: 0.10422141141586463\n",
            "Epoch: 51 Loss: 0.10409506968927902\n",
            "Epoch: 52 Loss: 0.1039687771378217\n",
            "Epoch: 53 Loss: 0.10384253429017853\n",
            "Epoch: 54 Loss: 0.10371634167421821\n",
            "Epoch: 55 Loss: 0.10359019981698012\n",
            "Epoch: 56 Loss: 0.10346410924466268\n",
            "Epoch: 57 Loss: 0.10333807048261123\n",
            "Epoch: 58 Loss: 0.10321208405530648\n",
            "Epoch: 59 Loss: 0.10308615048635263\n",
            "Epoch: 60 Loss: 0.10296027029846556\n",
            "Epoch: 61 Loss: 0.10283444401346137\n",
            "Epoch: 62 Loss: 0.10270867215224444\n",
            "Epoch: 63 Loss: 0.10258295523479606\n",
            "Epoch: 64 Loss: 0.10245729378016273\n",
            "Epoch: 65 Loss: 0.10233168830644467\n",
            "Epoch: 66 Loss: 0.10220613933078435\n",
            "Epoch: 67 Loss: 0.10208064736935504\n",
            "Epoch: 68 Loss: 0.10195521293734956\n",
            "Epoch: 69 Loss: 0.10182983654896863\n",
            "Epoch: 70 Loss: 0.10170451871740978\n",
            "Epoch: 71 Loss: 0.10157925995485621\n",
            "Epoch: 72 Loss: 0.10145406077246527\n",
            "Epoch: 73 Loss: 0.10132892168035756\n",
            "Epoch: 74 Loss: 0.10120384318760563\n",
            "Epoch: 75 Loss: 0.10107882580222309\n",
            "Epoch: 76 Loss: 0.1009538700311535\n",
            "Epoch: 77 Loss: 0.10082897638025937\n",
            "Epoch: 78 Loss: 0.10070414535431137\n",
            "Epoch: 79 Loss: 0.10057937745697734\n",
            "Epoch: 80 Loss: 0.10045467319081154\n",
            "Epoch: 81 Loss: 0.10033003305724385\n",
            "Epoch: 82 Loss: 0.10020545755656916\n",
            "Epoch: 83 Loss: 0.10008094718793663\n",
            "Epoch: 84 Loss: 0.09995650244933901\n",
            "Epoch: 85 Loss: 0.09983212383760236\n",
            "Epoch: 86 Loss: 0.09970781184837518\n",
            "Epoch: 87 Loss: 0.09958356697611837\n",
            "Epoch: 88 Loss: 0.09945938971409457\n",
            "Epoch: 89 Loss: 0.09933528055435788\n",
            "Epoch: 90 Loss: 0.09921123998774366\n",
            "Epoch: 91 Loss: 0.09908726850385835\n",
            "Epoch: 92 Loss: 0.09896336659106916\n",
            "Epoch: 93 Loss: 0.09883953473649415\n",
            "Epoch: 94 Loss: 0.09871577342599204\n",
            "Epoch: 95 Loss: 0.09859208314415233\n",
            "Epoch: 96 Loss: 0.09846846437428537\n",
            "Epoch: 97 Loss: 0.09834491759841243\n",
            "Epoch: 98 Loss: 0.09822144329725604\n",
            "Epoch: 99 Loss: 0.09809804195023007\n",
            "Epoch: 100 Loss: 0.0979747140354302\n",
            "Epoch: 101 Loss: 0.09785146002962429\n",
            "Epoch: 102 Loss: 0.09772828040824269\n",
            "Epoch: 103 Loss: 0.0976051756453689\n",
            "Epoch: 104 Loss: 0.09748214621373016\n",
            "Epoch: 105 Loss: 0.09735919258468796\n",
            "Epoch: 106 Loss: 0.09723631522822876\n",
            "Epoch: 107 Loss: 0.0971135146129549\n",
            "Epoch: 108 Loss: 0.09699079120607522\n",
            "Epoch: 109 Loss: 0.09686814547339617\n",
            "Epoch: 110 Loss: 0.0967455778793126\n",
            "Epoch: 111 Loss: 0.09662308888679882\n",
            "Epoch: 112 Loss: 0.09650067895739983\n",
            "Epoch: 113 Loss: 0.09637834855122226\n",
            "Epoch: 114 Loss: 0.09625609812692584\n",
            "Epoch: 115 Loss: 0.09613392814171438\n",
            "Epoch: 116 Loss: 0.09601183905132749\n",
            "Epoch: 117 Loss: 0.09588983131003176\n",
            "Epoch: 118 Loss: 0.09576790537061233\n",
            "Epoch: 119 Loss: 0.0956460616843645\n",
            "Epoch: 120 Loss: 0.09552430070108534\n",
            "Epoch: 121 Loss: 0.09540262286906533\n",
            "Epoch: 122 Loss: 0.0952810286350802\n",
            "Epoch: 123 Loss: 0.09515951844438283\n",
            "Epoch: 124 Loss: 0.0950380927406951\n",
            "Epoch: 125 Loss: 0.09491675196619981\n",
            "Epoch: 126 Loss: 0.09479549656153291\n",
            "Epoch: 127 Loss: 0.09467432696577546\n",
            "Epoch: 128 Loss: 0.09455324361644596\n",
            "Epoch: 129 Loss: 0.09443224694949258\n",
            "Epoch: 130 Loss: 0.09431133739928542\n",
            "Epoch: 131 Loss: 0.09419051539860913\n",
            "Epoch: 132 Loss: 0.09406978137865515\n",
            "Epoch: 133 Loss: 0.09394913576901448\n",
            "Epoch: 134 Loss: 0.09382857899767026\n",
            "Epoch: 135 Loss: 0.0937081114909904\n",
            "Epoch: 136 Loss: 0.0935877336737205\n",
            "Epoch: 137 Loss: 0.09346744596897658\n",
            "Epoch: 138 Loss: 0.0933472487982381\n",
            "Epoch: 139 Loss: 0.09322714258134093\n",
            "Epoch: 140 Loss: 0.09310712773647045\n",
            "Epoch: 141 Loss: 0.09298720468015477\n",
            "Epoch: 142 Loss: 0.09286737382725777\n",
            "Epoch: 143 Loss: 0.09274763559097261\n",
            "Epoch: 144 Loss: 0.09262799038281502\n",
            "Epoch: 145 Loss: 0.09250843861261682\n",
            "Epoch: 146 Loss: 0.0923889806885193\n",
            "Epoch: 147 Loss: 0.0922696170169671\n",
            "Epoch: 148 Loss: 0.09215034800270162\n",
            "Epoch: 149 Loss: 0.09203117404875494\n",
            "Epoch: 150 Loss: 0.0919120955564437\n",
            "Epoch: 151 Loss: 0.09179311292536277\n",
            "Epoch: 152 Loss: 0.0916742265533796\n",
            "Epoch: 153 Loss: 0.09155543683662805\n",
            "Epoch: 154 Loss: 0.09143674416950245\n",
            "Epoch: 155 Loss: 0.09131814894465227\n",
            "Epoch: 156 Loss: 0.09119965155297584\n",
            "Epoch: 157 Loss: 0.09108125238361514\n",
            "Epoch: 158 Loss: 0.09096295182395014\n",
            "Epoch: 159 Loss: 0.09084475025959331\n",
            "Epoch: 160 Loss: 0.09072664807438419\n",
            "Epoch: 161 Loss: 0.09060864565038418\n",
            "Epoch: 162 Loss: 0.09049074336787138\n",
            "Epoch: 163 Loss: 0.09037294160533513\n",
            "Epoch: 164 Loss: 0.09025524073947121\n",
            "Epoch: 165 Loss: 0.09013764114517682\n",
            "Epoch: 166 Loss: 0.09002014319554547\n",
            "Epoch: 167 Loss: 0.08990274726186243\n",
            "Epoch: 168 Loss: 0.08978545371359964\n",
            "Epoch: 169 Loss: 0.08966826291841129\n",
            "Epoch: 170 Loss: 0.08955117524212915\n",
            "Epoch: 171 Loss: 0.08943419104875788\n",
            "Epoch: 172 Loss: 0.08931731070047083\n",
            "Epoch: 173 Loss: 0.08920053455760547\n",
            "Epoch: 174 Loss: 0.08908386297865915\n",
            "Epoch: 175 Loss: 0.08896729632028501\n",
            "Epoch: 176 Loss: 0.08885083493728767\n",
            "Epoch: 177 Loss: 0.08873447918261922\n",
            "Epoch: 178 Loss: 0.08861822940737535\n",
            "Epoch: 179 Loss: 0.08850208596079122\n",
            "Epoch: 180 Loss: 0.08838604919023794\n",
            "Epoch: 181 Loss: 0.08827011944121858\n",
            "Epoch: 182 Loss: 0.08815429705736454\n",
            "Epoch: 183 Loss: 0.08803858238043208\n",
            "Epoch: 184 Loss: 0.08792297575029875\n",
            "Epoch: 185 Loss: 0.08780747750495985\n",
            "Epoch: 186 Loss: 0.0876920879805253\n",
            "Epoch: 187 Loss: 0.08757680751121612\n",
            "Epoch: 188 Loss: 0.08746163642936138\n",
            "Epoch: 189 Loss: 0.08734657506539507\n",
            "Epoch: 190 Loss: 0.08723162374785307\n",
            "Epoch: 191 Loss: 0.08711678280337003\n",
            "Epoch: 192 Loss: 0.08700205255667669\n",
            "Epoch: 193 Loss: 0.08688743333059691\n",
            "Epoch: 194 Loss: 0.08677292544604512\n",
            "Epoch: 195 Loss: 0.0866585292220234\n",
            "Epoch: 196 Loss: 0.08654424497561919\n",
            "Epoch: 197 Loss: 0.08643007302200253\n",
            "Epoch: 198 Loss: 0.08631601367442387\n",
            "Epoch: 199 Loss: 0.08620206724421148\n",
            "Epoch: 200 Loss: 0.08608823404076939\n",
            "Epoch: 201 Loss: 0.08597451437157506\n",
            "Epoch: 202 Loss: 0.08586090854217739\n",
            "Epoch: 203 Loss: 0.08574741685619448\n",
            "Epoch: 204 Loss: 0.08563403961531188\n",
            "Epoch: 205 Loss: 0.08552077711928055\n",
            "Epoch: 206 Loss: 0.08540762966591509\n",
            "Epoch: 207 Loss: 0.08529459755109205\n",
            "Epoch: 208 Loss: 0.0851816810687481\n",
            "Epoch: 209 Loss: 0.08506888051087877\n",
            "Epoch: 210 Loss: 0.08495619616753651\n",
            "Epoch: 211 Loss: 0.08484362832682957\n",
            "Epoch: 212 Loss: 0.08473117727492052\n",
            "Epoch: 213 Loss: 0.08461884329602501\n",
            "Epoch: 214 Loss: 0.08450662667241042\n",
            "Epoch: 215 Loss: 0.08439452768439487\n",
            "Epoch: 216 Loss: 0.08428254661034609\n",
            "Epoch: 217 Loss: 0.08417068372668045\n",
            "Epoch: 218 Loss: 0.08405893930786192\n",
            "Epoch: 219 Loss: 0.08394731362640144\n",
            "Epoch: 220 Loss: 0.08383580695285593\n",
            "Epoch: 221 Loss: 0.08372441955582771\n",
            "Epoch: 222 Loss: 0.0836131517019638\n",
            "Epoch: 223 Loss: 0.08350200365595552\n",
            "Epoch: 224 Loss: 0.0833909756805377\n",
            "Epoch: 225 Loss: 0.08328006803648856\n",
            "Epoch: 226 Loss: 0.0831692809826292\n",
            "Epoch: 227 Loss: 0.08305861477582344\n",
            "Epoch: 228 Loss: 0.08294806967097747\n",
            "Epoch: 229 Loss: 0.08283764592103987\n",
            "Epoch: 230 Loss: 0.08272734377700144\n",
            "Epoch: 231 Loss: 0.08261716348789525\n",
            "Epoch: 232 Loss: 0.08250710530079677\n",
            "Epoch: 233 Loss: 0.08239716946082391\n",
            "Epoch: 234 Loss: 0.0822873562111373\n",
            "Epoch: 235 Loss: 0.08217766579294061\n",
            "Epoch: 236 Loss: 0.08206809844548081\n",
            "Epoch: 237 Loss: 0.0819586544060487\n",
            "Epoch: 238 Loss: 0.08184933390997932\n",
            "Epoch: 239 Loss: 0.08174013719065253\n",
            "Epoch: 240 Loss: 0.08163106447949367\n",
            "Epoch: 241 Loss: 0.0815221160059742\n",
            "Epoch: 242 Loss: 0.08141329199761252\n",
            "Epoch: 243 Loss: 0.08130459267997471\n",
            "Epoch: 244 Loss: 0.08119601827667555\n",
            "Epoch: 245 Loss: 0.0810875690093793\n",
            "Epoch: 246 Loss: 0.08097924509780097\n",
            "Epoch: 247 Loss: 0.08087104675970708\n",
            "Epoch: 248 Loss: 0.08076297421091719\n",
            "Epoch: 249 Loss: 0.0806550276653048\n",
            "Epoch: 250 Loss: 0.08054720733479881\n",
            "Epoch: 251 Loss: 0.0804395134293848\n",
            "Epoch: 252 Loss: 0.08033194615710647\n",
            "Epoch: 253 Loss: 0.08022450572406711\n",
            "Epoch: 254 Loss: 0.08011719233443106\n",
            "Epoch: 255 Loss: 0.08001000619042546\n",
            "Epoch: 256 Loss: 0.07990294749234174\n",
            "Epoch: 257 Loss: 0.07979601643853748\n",
            "Epoch: 258 Loss: 0.07968921322543811\n",
            "Epoch: 259 Loss: 0.07958253804753877\n",
            "Epoch: 260 Loss: 0.07947599109740627\n",
            "Epoch: 261 Loss: 0.07936957256568092\n",
            "Epoch: 262 Loss: 0.07926328264107871\n",
            "Epoch: 263 Loss: 0.07915712151039334\n",
            "Epoch: 264 Loss: 0.07905108935849828\n",
            "Epoch: 265 Loss: 0.07894518636834906\n",
            "Epoch: 266 Loss: 0.07883941272098557\n",
            "Epoch: 267 Loss: 0.07873376859553426\n",
            "Epoch: 268 Loss: 0.07862825416921056\n",
            "Epoch: 269 Loss: 0.07852286961732133\n",
            "Epoch: 270 Loss: 0.0784176151132674\n",
            "Epoch: 271 Loss: 0.07831249082854601\n",
            "Epoch: 272 Loss: 0.07820749693275345\n",
            "Epoch: 273 Loss: 0.07810263359358777\n",
            "Epoch: 274 Loss: 0.0779979009768514\n",
            "Epoch: 275 Loss: 0.07789329924645401\n",
            "Epoch: 276 Loss: 0.07778882856441528\n",
            "Epoch: 277 Loss: 0.07768448909086778\n",
            "Epoch: 278 Loss: 0.07758028098405989\n",
            "Epoch: 279 Loss: 0.07747620440035882\n",
            "Epoch: 280 Loss: 0.07737225949425361\n",
            "Epoch: 281 Loss: 0.07726844641835823\n",
            "Epoch: 282 Loss: 0.07716476532341471\n",
            "Epoch: 283 Loss: 0.07706121635829635\n",
            "Epoch: 284 Loss: 0.07695779967001097\n",
            "Epoch: 285 Loss: 0.07685451540370415\n",
            "Epoch: 286 Loss: 0.07675136370266261\n",
            "Epoch: 287 Loss: 0.07664834470831765\n",
            "Epoch: 288 Loss: 0.07654545856024847\n",
            "Epoch: 289 Loss: 0.07644270539618579\n",
            "Epoch: 290 Loss: 0.07634008535201535\n",
            "Epoch: 291 Loss: 0.07623759856178142\n",
            "Epoch: 292 Loss: 0.07613524515769057\n",
            "Epoch: 293 Loss: 0.07603302527011523\n",
            "Epoch: 294 Loss: 0.07593093902759751\n",
            "Epoch: 295 Loss: 0.07582898655685297\n",
            "Epoch: 296 Loss: 0.0757271679827743\n",
            "Epoch: 297 Loss: 0.07562548342843547\n",
            "Epoch: 298 Loss: 0.07552393301509537\n",
            "Epoch: 299 Loss: 0.0754225168622019\n",
            "Epoch: 300 Loss: 0.07532123508739598\n",
            "Epoch: 301 Loss: 0.07522008780651557\n",
            "Epoch: 302 Loss: 0.07511907513359971\n",
            "Epoch: 303 Loss: 0.07501819718089278\n",
            "Epoch: 304 Loss: 0.07491745405884856\n",
            "Epoch: 305 Loss: 0.0748168458761345\n",
            "Epoch: 306 Loss: 0.074716372739636\n",
            "Epoch: 307 Loss: 0.07461603475446064\n",
            "Epoch: 308 Loss: 0.07451583202394263\n",
            "Epoch: 309 Loss: 0.0744157646496471\n",
            "Epoch: 310 Loss: 0.07431583273137457\n",
            "Epoch: 311 Loss: 0.07421603636716538\n",
            "Epoch: 312 Loss: 0.07411637565330427\n",
            "Epoch: 313 Loss: 0.07401685068432483\n",
            "Epoch: 314 Loss: 0.0739174615530141\n",
            "Epoch: 315 Loss: 0.07381820835041729\n",
            "Epoch: 316 Loss: 0.07371909116584226\n",
            "Epoch: 317 Loss: 0.07362011008686437\n",
            "Epoch: 318 Loss: 0.07352126519933112\n",
            "Epoch: 319 Loss: 0.07342255658736696\n",
            "Epoch: 320 Loss: 0.07332398433337807\n",
            "Epoch: 321 Loss: 0.0732255485180572\n",
            "Epoch: 322 Loss: 0.07312724922038853\n",
            "Epoch: 323 Loss: 0.07302908651765262\n",
            "Epoch: 324 Loss: 0.07293106048543133\n",
            "Epoch: 325 Loss: 0.07283317119761273\n",
            "Epoch: 326 Loss: 0.07273541872639624\n",
            "Epoch: 327 Loss: 0.07263780314229755\n",
            "Epoch: 328 Loss: 0.07254032451415374\n",
            "Epoch: 329 Loss: 0.0724429829091284\n",
            "Epoch: 330 Loss: 0.07234577839271675\n",
            "Epoch: 331 Loss: 0.07224871102875076\n",
            "Epoch: 332 Loss: 0.07215178087940448\n",
            "Epoch: 333 Loss: 0.0720549880051991\n",
            "Epoch: 334 Loss: 0.07195833246500839\n",
            "Epoch: 335 Loss: 0.07186181431606381\n",
            "Epoch: 336 Loss: 0.07176543361395989\n",
            "Epoch: 337 Loss: 0.07166919041265968\n",
            "Epoch: 338 Loss: 0.07157308476449997\n",
            "Epoch: 339 Loss: 0.0714771167201968\n",
            "Epoch: 340 Loss: 0.0713812863288508\n",
            "Epoch: 341 Loss: 0.0712855936379527\n",
            "Epoch: 342 Loss: 0.07119003869338886\n",
            "Epoch: 343 Loss: 0.07109462153944662\n",
            "Epoch: 344 Loss: 0.07099934221882004\n",
            "Epoch: 345 Loss: 0.0709042007726153\n",
            "Epoch: 346 Loss: 0.07080919724035631\n",
            "Epoch: 347 Loss: 0.0707143316599904\n",
            "Epoch: 348 Loss: 0.07061960406789386\n",
            "Epoch: 349 Loss: 0.07052501449887766\n",
            "Epoch: 350 Loss: 0.07043056298619306\n",
            "Epoch: 351 Loss: 0.07033624956153742\n",
            "Epoch: 352 Loss: 0.07024207425505975\n",
            "Epoch: 353 Loss: 0.07014803709536668\n",
            "Epoch: 354 Loss: 0.07005413810952804\n",
            "Epoch: 355 Loss: 0.06996037732308272\n",
            "Epoch: 356 Loss: 0.0698667547600445\n",
            "Epoch: 357 Loss: 0.06977327044290782\n",
            "Epoch: 358 Loss: 0.06967992439265375\n",
            "Epoch: 359 Loss: 0.06958671662875565\n",
            "Epoch: 360 Loss: 0.0694936471691853\n",
            "Epoch: 361 Loss: 0.0694007160304186\n",
            "Epoch: 362 Loss: 0.06930792322744166\n",
            "Epoch: 363 Loss: 0.06921526877375661\n",
            "Epoch: 364 Loss: 0.06912275268138765\n",
            "Epoch: 365 Loss: 0.069030374960887\n",
            "Epoch: 366 Loss: 0.06893813562134089\n",
            "Epoch: 367 Loss: 0.06884603467037556\n",
            "Epoch: 368 Loss: 0.06875407211416329\n",
            "Epoch: 369 Loss: 0.06866224795742852\n",
            "Epoch: 370 Loss: 0.06857056220345376\n",
            "Epoch: 371 Loss: 0.06847901485408579\n",
            "Epoch: 372 Loss: 0.06838760590974173\n",
            "Epoch: 373 Loss: 0.06829633536941505\n",
            "Epoch: 374 Loss: 0.0682052032306818\n",
            "Epoch: 375 Loss: 0.06811420948970666\n",
            "Epoch: 376 Loss: 0.06802335414124916\n",
            "Epoch: 377 Loss: 0.06793263717866975\n",
            "Epoch: 378 Loss: 0.06784205859393598\n",
            "Epoch: 379 Loss: 0.06775161837762876\n",
            "Epoch: 380 Loss: 0.06766131651894848\n",
            "Epoch: 381 Loss: 0.06757115300572121\n",
            "Epoch: 382 Loss: 0.06748112782440492\n",
            "Epoch: 383 Loss: 0.06739124096009574\n",
            "Epoch: 384 Loss: 0.06730149239653416\n",
            "Epoch: 385 Loss: 0.06721188211611125\n",
            "Epoch: 386 Loss: 0.06712241009987499\n",
            "Epoch: 387 Loss: 0.06703307632753643\n",
            "Epoch: 388 Loss: 0.06694388077747605\n",
            "Epoch: 389 Loss: 0.06685482342675\n",
            "Epoch: 390 Loss: 0.06676590425109637\n",
            "Epoch: 391 Loss: 0.06667712322494153\n",
            "Epoch: 392 Loss: 0.06658848032140635\n",
            "Epoch: 393 Loss: 0.06649997551231265\n",
            "Epoch: 394 Loss: 0.06641160876818936\n",
            "Epoch: 395 Loss: 0.06632338005827892\n",
            "Epoch: 396 Loss: 0.06623528935054364\n",
            "Epoch: 397 Loss: 0.06614733661167198\n",
            "Epoch: 398 Loss: 0.06605952180708484\n",
            "Epoch: 399 Loss: 0.06597184490094209\n",
            "Epoch: 400 Loss: 0.06588430585614864\n",
            "Epoch: 401 Loss: 0.06579690463436111\n",
            "Epoch: 402 Loss: 0.06570964119599391\n",
            "Epoch: 403 Loss: 0.06562251550022578\n",
            "Epoch: 404 Loss: 0.06553552750500607\n",
            "Epoch: 405 Loss: 0.06544867716706113\n",
            "Epoch: 406 Loss: 0.06536196444190066\n",
            "Epoch: 407 Loss: 0.06527538928382413\n",
            "Epoch: 408 Loss: 0.06518895164592715\n",
            "Epoch: 409 Loss: 0.0651026514801078\n",
            "Epoch: 410 Loss: 0.06501648873707301\n",
            "Epoch: 411 Loss: 0.06493046336634503\n",
            "Epoch: 412 Loss: 0.06484457531626768\n",
            "Epoch: 413 Loss: 0.06475882453401285\n",
            "Epoch: 414 Loss: 0.06467321096558676\n",
            "Epoch: 415 Loss: 0.06458773455583654\n",
            "Epoch: 416 Loss: 0.06450239524845633\n",
            "Epoch: 417 Loss: 0.06441719298599394\n",
            "Epoch: 418 Loss: 0.06433212770985704\n",
            "Epoch: 419 Loss: 0.06424719936031964\n",
            "Epoch: 420 Loss: 0.06416240787652844\n",
            "Epoch: 421 Loss: 0.06407775319650917\n",
            "Epoch: 422 Loss: 0.06399323525717303\n",
            "Epoch: 423 Loss: 0.06390885399432303\n",
            "Epoch: 424 Loss: 0.06382460934266035\n",
            "Epoch: 425 Loss: 0.06374050123579074\n",
            "Epoch: 426 Loss: 0.06365652960623085\n",
            "Epoch: 427 Loss: 0.06357269438541462\n",
            "Epoch: 428 Loss: 0.06348899550369963\n",
            "Epoch: 429 Loss: 0.06340543289037344\n",
            "Epoch: 430 Loss: 0.06332200647365997\n",
            "Epoch: 431 Loss: 0.06323871618072581\n",
            "Epoch: 432 Loss: 0.06315556193768662\n",
            "Epoch: 433 Loss: 0.06307254366961337\n",
            "Epoch: 434 Loss: 0.06298966130053879\n",
            "Epoch: 435 Loss: 0.06290691475346362\n",
            "Epoch: 436 Loss: 0.06282430395036293\n",
            "Epoch: 437 Loss: 0.06274182881219251\n",
            "Epoch: 438 Loss: 0.06265948925889506\n",
            "Epoch: 439 Loss: 0.0625772852094066\n",
            "Epoch: 440 Loss: 0.062495216581662766\n",
            "Epoch: 441 Loss: 0.062413283292605\n",
            "Epoch: 442 Loss: 0.06233148525818691\n",
            "Epoch: 443 Loss: 0.062249822393380574\n",
            "Epoch: 444 Loss: 0.06216829461218274\n",
            "Epoch: 445 Loss: 0.06208690182762112\n",
            "Epoch: 446 Loss: 0.062005643951760635\n",
            "Epoch: 447 Loss: 0.061924520895709684\n",
            "Epoch: 448 Loss: 0.0618435325696264\n",
            "Epoch: 449 Loss: 0.06176267888272477\n",
            "Epoch: 450 Loss: 0.06168195974328098\n",
            "Epoch: 451 Loss: 0.06160137505863959\n",
            "Epoch: 452 Loss: 0.06152092473521968\n",
            "Epoch: 453 Loss: 0.061440608678521136\n",
            "Epoch: 454 Loss: 0.06136042679313076\n",
            "Epoch: 455 Loss: 0.06128037898272847\n",
            "Epoch: 456 Loss: 0.061200465150093475\n",
            "Epoch: 457 Loss: 0.061120685197110454\n",
            "Epoch: 458 Loss: 0.06104103902477562\n",
            "Epoch: 459 Loss: 0.060961526533202894\n",
            "Epoch: 460 Loss: 0.06088214762163007\n",
            "Epoch: 461 Loss: 0.06080290218842488\n",
            "Epoch: 462 Loss: 0.06072379013109109\n",
            "Epoch: 463 Loss: 0.06064481134627465\n",
            "Epoch: 464 Loss: 0.06056596572976967\n",
            "Epoch: 465 Loss: 0.060487253176524584\n",
            "Epoch: 466 Loss: 0.06040867358064816\n",
            "Epoch: 467 Loss: 0.060330226835415536\n",
            "Epoch: 468 Loss: 0.06025191283327429\n",
            "Epoch: 469 Loss: 0.0601737314658504\n",
            "Epoch: 470 Loss: 0.06009568262395435\n",
            "Epoch: 471 Loss: 0.060017766197586954\n",
            "Epoch: 472 Loss: 0.05993998207594555\n",
            "Epoch: 473 Loss: 0.05986233014742976\n",
            "Epoch: 474 Loss: 0.059784810299647655\n",
            "Epoch: 475 Loss: 0.05970742241942144\n",
            "Epoch: 476 Loss: 0.05963016639279365\n",
            "Epoch: 477 Loss: 0.05955304210503289\n",
            "Epoch: 478 Loss: 0.059476049440639805\n",
            "Epoch: 479 Loss: 0.05939918828335293\n",
            "Epoch: 480 Loss: 0.05932245851615458\n",
            "Epoch: 481 Loss: 0.05924586002127672\n",
            "Epoch: 482 Loss: 0.05916939268020683\n",
            "Epoch: 483 Loss: 0.05909305637369362\n",
            "Epoch: 484 Loss: 0.05901685098175304\n",
            "Epoch: 485 Loss: 0.05894077638367387\n",
            "Epoch: 486 Loss: 0.05886483245802365\n",
            "Epoch: 487 Loss: 0.058789019082654444\n",
            "Epoch: 488 Loss: 0.05871333613470847\n",
            "Epoch: 489 Loss: 0.05863778349062402\n",
            "Epoch: 490 Loss: 0.05856236102614099\n",
            "Epoch: 491 Loss: 0.058487068616306814\n",
            "Epoch: 492 Loss: 0.05841190613548191\n",
            "Epoch: 493 Loss: 0.058336873457345575\n",
            "Epoch: 494 Loss: 0.058261970454901515\n",
            "Epoch: 495 Loss: 0.05818719700048353\n",
            "Epoch: 496 Loss: 0.05811255296576115\n",
            "Epoch: 497 Loss: 0.05803803822174524\n",
            "Epoch: 498 Loss: 0.05796365263879362\n",
            "Epoch: 499 Loss: 0.05788939608661657\n",
            "Epoch: 500 Loss: 0.0578152684342825\n",
            "Epoch: 501 Loss: 0.057741269550223434\n",
            "Epoch: 502 Loss: 0.057667399302240525\n",
            "Epoch: 503 Loss: 0.05759365755750964\n",
            "Epoch: 504 Loss: 0.05752004418258676\n",
            "Epoch: 505 Loss: 0.057446559043413566\n",
            "Epoch: 506 Loss: 0.05737320200532279\n",
            "Epoch: 507 Loss: 0.057299972933043765\n",
            "Epoch: 508 Loss: 0.05722687169070777\n",
            "Epoch: 509 Loss: 0.05715389814185348\n",
            "Epoch: 510 Loss: 0.05708105214943238\n",
            "Epoch: 511 Loss: 0.057008333575814064\n",
            "Epoch: 512 Loss: 0.056935742282791676\n",
            "Epoch: 513 Loss: 0.0568632781315872\n",
            "Epoch: 514 Loss: 0.05679094098285678\n",
            "Epoch: 515 Loss: 0.05671873069669606\n",
            "Epoch: 516 Loss: 0.05664664713264544\n",
            "Epoch: 517 Loss: 0.05657469014969532\n",
            "Epoch: 518 Loss: 0.05650285960629143\n",
            "Epoch: 519 Loss: 0.05643115536033993\n",
            "Epoch: 520 Loss: 0.0563595772692128\n",
            "Epoch: 521 Loss: 0.05628812518975282\n",
            "Epoch: 522 Loss: 0.05621679897827897\n",
            "Epoch: 523 Loss: 0.05614559849059136\n",
            "Epoch: 524 Loss: 0.05607452358197658\n",
            "Epoch: 525 Loss: 0.056003574107212656\n",
            "Epoch: 526 Loss: 0.055932749920574246\n",
            "Epoch: 527 Loss: 0.055862050875837656\n",
            "Epoch: 528 Loss: 0.05579147682628598\n",
            "Epoch: 529 Loss: 0.05572102762471408\n",
            "Epoch: 530 Loss: 0.05565070312343357\n",
            "Epoch: 531 Loss: 0.05558050317427798\n",
            "Epoch: 532 Loss: 0.05551042762860758\n",
            "Epoch: 533 Loss: 0.05544047633731439\n",
            "Epoch: 534 Loss: 0.05537064915082723\n",
            "Epoch: 535 Loss: 0.05530094591911643\n",
            "Epoch: 536 Loss: 0.055231366491699\n",
            "Epoch: 537 Loss: 0.05516191071764332\n",
            "Epoch: 538 Loss: 0.05509257844557411\n",
            "Epoch: 539 Loss: 0.055023369523677215\n",
            "Epoch: 540 Loss: 0.054954283799704504\n",
            "Epoch: 541 Loss: 0.054885321120978645\n",
            "Epoch: 542 Loss: 0.054816481334397905\n",
            "Epoch: 543 Loss: 0.05474776428644088\n",
            "Epoch: 544 Loss: 0.05467916982317133\n",
            "Epoch: 545 Loss: 0.05461069779024284\n",
            "Epoch: 546 Loss: 0.05454234803290357\n",
            "Epoch: 547 Loss: 0.054474120396000913\n",
            "Epoch: 548 Loss: 0.05440601472398624\n",
            "Epoch: 549 Loss: 0.054338030860919435\n",
            "Epoch: 550 Loss: 0.05427016865047366\n",
            "Epoch: 551 Loss: 0.05420242793593987\n",
            "Epoch: 552 Loss: 0.054134808560231465\n",
            "Epoch: 553 Loss: 0.05406731036588881\n",
            "Epoch: 554 Loss: 0.05399993319508385\n",
            "Epoch: 555 Loss: 0.053932676889624595\n",
            "Epoch: 556 Loss: 0.053865541290959655\n",
            "Epoch: 557 Loss: 0.05379852624018271\n",
            "Epoch: 558 Loss: 0.05373163157803706\n",
            "Epoch: 559 Loss: 0.05366485714491995\n",
            "Epoch: 560 Loss: 0.053598202780887096\n",
            "Epoch: 561 Loss: 0.0535316683256571\n",
            "Epoch: 562 Loss: 0.053465253618615816\n",
            "Epoch: 563 Loss: 0.05339895849882068\n",
            "Epoch: 564 Loss: 0.053332782805005134\n",
            "Epoch: 565 Loss: 0.053266726375582954\n",
            "Epoch: 566 Loss: 0.05320078904865246\n",
            "Epoch: 567 Loss: 0.053134970662001006\n",
            "Epoch: 568 Loss: 0.05306927105310903\n",
            "Epoch: 569 Loss: 0.05300369005915444\n",
            "Epoch: 570 Loss: 0.05293822751701683\n",
            "Epoch: 571 Loss: 0.05287288326328165\n",
            "Epoch: 572 Loss: 0.05280765713424441\n",
            "Epoch: 573 Loss: 0.05274254896591493\n",
            "Epoch: 574 Loss: 0.052677558594021325\n",
            "Epoch: 575 Loss: 0.05261268585401432\n",
            "Epoch: 576 Loss: 0.05254793058107117\n",
            "Epoch: 577 Loss: 0.05248329261009995\n",
            "Epoch: 578 Loss: 0.05241877177574347\n",
            "Epoch: 579 Loss: 0.052354367912383376\n",
            "Epoch: 580 Loss: 0.05229008085414422\n",
            "Epoch: 581 Loss: 0.05222591043489738\n",
            "Epoch: 582 Loss: 0.05216185648826516\n",
            "Epoch: 583 Loss: 0.05209791884762462\n",
            "Epoch: 584 Loss: 0.052034097346111696\n",
            "Epoch: 585 Loss: 0.05197039181662495\n",
            "Epoch: 586 Loss: 0.0519068020918296\n",
            "Epoch: 587 Loss: 0.05184332800416139\n",
            "Epoch: 588 Loss: 0.05177996938583039\n",
            "Epoch: 589 Loss: 0.05171672606882494\n",
            "Epoch: 590 Loss: 0.051653597884915387\n",
            "Epoch: 591 Loss: 0.05159058466565797\n",
            "Epoch: 592 Loss: 0.05152768624239853\n",
            "Epoch: 593 Loss: 0.05146490244627636\n",
            "Epoch: 594 Loss: 0.051402233108227904\n",
            "Epoch: 595 Loss: 0.05133967805899047\n",
            "Epoch: 596 Loss: 0.05127723712910598\n",
            "Epoch: 597 Loss: 0.05121491014892463\n",
            "Epoch: 598 Loss: 0.051152696948608564\n",
            "Epoch: 599 Loss: 0.05109059735813552\n",
            "Epoch: 600 Loss: 0.051028611207302496\n",
            "Epoch: 601 Loss: 0.05096673832572925\n",
            "Epoch: 602 Loss: 0.05090497854286201\n",
            "Epoch: 603 Loss: 0.05084333168797699\n",
            "Epoch: 604 Loss: 0.05078179759018392\n",
            "Epoch: 605 Loss: 0.05072037607842961\n",
            "Epoch: 606 Loss: 0.050659066981501424\n",
            "Epoch: 607 Loss: 0.05059787012803084\n",
            "Epoch: 608 Loss: 0.05053678534649679\n",
            "Epoch: 609 Loss: 0.050475812465229274\n",
            "Epoch: 610 Loss: 0.05041495131241262\n",
            "Epoch: 611 Loss: 0.05035420171608907\n",
            "Epoch: 612 Loss: 0.05029356350416198\n",
            "Epoch: 613 Loss: 0.050233036504399385\n",
            "Epoch: 614 Loss: 0.050172620544437205\n",
            "Epoch: 615 Loss: 0.05011231545178264\n",
            "Epoch: 616 Loss: 0.05005212105381749\n",
            "Epoch: 617 Loss: 0.049992037177801436\n",
            "Epoch: 618 Loss: 0.04993206365087527\n",
            "Epoch: 619 Loss: 0.04987220030006425\n",
            "Epoch: 620 Loss: 0.04981244695228124\n",
            "Epoch: 621 Loss: 0.04975280343432998\n",
            "Epoch: 622 Loss: 0.049693269572908275\n",
            "Epoch: 623 Loss: 0.04963384519461117\n",
            "Epoch: 624 Loss: 0.049574530125934074\n",
            "Epoch: 625 Loss: 0.049515324193276\n",
            "Epoch: 626 Loss: 0.04945622722294253\n",
            "Epoch: 627 Loss: 0.04939723904114911\n",
            "Epoch: 628 Loss: 0.04933835947402393\n",
            "Epoch: 629 Loss: 0.04927958834761118\n",
            "Epoch: 630 Loss: 0.04922092548787397\n",
            "Epoch: 631 Loss: 0.049162370720697346\n",
            "Epoch: 632 Loss: 0.049103923871891435\n",
            "Epoch: 633 Loss: 0.049045584767194284\n",
            "Epoch: 634 Loss: 0.048987353232274884\n",
            "Epoch: 635 Loss: 0.04892922909273617\n",
            "Epoch: 636 Loss: 0.048871212174117905\n",
            "Epoch: 637 Loss: 0.04881330230189959\n",
            "Epoch: 638 Loss: 0.04875549930150335\n",
            "Epoch: 639 Loss: 0.04869780299829689\n",
            "Epoch: 640 Loss: 0.04864021321759629\n",
            "Epoch: 641 Loss: 0.04858272978466883\n",
            "Epoch: 642 Loss: 0.048525352524735846\n",
            "Epoch: 643 Loss: 0.04846808126297557\n",
            "Epoch: 644 Loss: 0.04841091582452583\n",
            "Epoch: 645 Loss: 0.0483538560344869\n",
            "Epoch: 646 Loss: 0.04829690171792418\n",
            "Epoch: 647 Loss: 0.04824005269987101\n",
            "Epoch: 648 Loss: 0.048183308805331296\n",
            "Epoch: 649 Loss: 0.04812666985928229\n",
            "Epoch: 650 Loss: 0.04807013568667716\n",
            "Epoch: 651 Loss: 0.04801370611244781\n",
            "Epoch: 652 Loss: 0.047957380961507375\n",
            "Epoch: 653 Loss: 0.04790116005875289\n",
            "Epoch: 654 Loss: 0.047845043229067984\n",
            "Epoch: 655 Loss: 0.04778903029732533\n",
            "Epoch: 656 Loss: 0.047733121088389285\n",
            "Epoch: 657 Loss: 0.0476773154271185\n",
            "Epoch: 658 Loss: 0.04762161313836834\n",
            "Epoch: 659 Loss: 0.047566014046993496\n",
            "Epoch: 660 Loss: 0.04751051797785046\n",
            "Epoch: 661 Loss: 0.04745512475580002\n",
            "Epoch: 662 Loss: 0.04739983420570969\n",
            "Epoch: 663 Loss: 0.04734464615245619\n",
            "Epoch: 664 Loss: 0.0472895604209279\n",
            "Epoch: 665 Loss: 0.047234576836027234\n",
            "Epoch: 666 Loss: 0.04717969522267307\n",
            "Epoch: 667 Loss: 0.047124915405803124\n",
            "Epoch: 668 Loss: 0.04707023721037633\n",
            "Epoch: 669 Loss: 0.04701566046137513\n",
            "Epoch: 670 Loss: 0.04696118498380792\n",
            "Epoch: 671 Loss: 0.04690681060271123\n",
            "Epoch: 672 Loss: 0.046852537143152136\n",
            "Epoch: 673 Loss: 0.046798364430230464\n",
            "Epoch: 674 Loss: 0.04674429228908115\n",
            "Epoch: 675 Loss: 0.04669032054487632\n",
            "Epoch: 676 Loss: 0.046636449022827764\n",
            "Epoch: 677 Loss: 0.04658267754818892\n",
            "Epoch: 678 Loss: 0.04652900594625724\n",
            "Epoch: 679 Loss: 0.046475434042376244\n",
            "Epoch: 680 Loss: 0.046421961661937836\n",
            "Epoch: 681 Loss: 0.046368588630384275\n",
            "Epoch: 682 Loss: 0.04631531477321051\n",
            "Epoch: 683 Loss: 0.04626213991596613\n",
            "Epoch: 684 Loss: 0.04620906388425755\n",
            "Epoch: 685 Loss: 0.046156086503750124\n",
            "Epoch: 686 Loss: 0.046103207600170136\n",
            "Epoch: 687 Loss: 0.04605042699930692\n",
            "Epoch: 688 Loss: 0.04599774452701489\n",
            "Epoch: 689 Loss: 0.04594516000921553\n",
            "Epoch: 690 Loss: 0.04589267327189945\n",
            "Epoch: 691 Loss: 0.04584028414112835\n",
            "Epoch: 692 Loss: 0.04578799244303702\n",
            "Epoch: 693 Loss: 0.04573579800383529\n",
            "Epoch: 694 Loss: 0.045683700649809963\n",
            "Epoch: 695 Loss: 0.045631700207326806\n",
            "Epoch: 696 Loss: 0.04557979650283239\n",
            "Epoch: 697 Loss: 0.04552798936285602\n",
            "Epoch: 698 Loss: 0.04547627861401168\n",
            "Epoch: 699 Loss: 0.045424664082999805\n",
            "Epoch: 700 Loss: 0.04537314559660924\n",
            "Epoch: 701 Loss: 0.045321722981718986\n",
            "Epoch: 702 Loss: 0.0452703960653001\n",
            "Epoch: 703 Loss: 0.0452191646744175\n",
            "Epoch: 704 Loss: 0.0451680286362317\n",
            "Epoch: 705 Loss: 0.04511698777800066\n",
            "Epoch: 706 Loss: 0.04506604192708157\n",
            "Epoch: 707 Loss: 0.04501519091093252\n",
            "Epoch: 708 Loss: 0.044964434557114324\n",
            "Epoch: 709 Loss: 0.04491377269329216\n",
            "Epoch: 710 Loss: 0.04486320514723742\n",
            "Epoch: 711 Loss: 0.044812731746829246\n",
            "Epoch: 712 Loss: 0.044762352320056306\n",
            "Epoch: 713 Loss: 0.04471206669501848\n",
            "Epoch: 714 Loss: 0.04466187469992841\n",
            "Epoch: 715 Loss: 0.04461177616311327\n",
            "Epoch: 716 Loss: 0.04456177091301632\n",
            "Epoch: 717 Loss: 0.04451185877819852\n",
            "Epoch: 718 Loss: 0.044462039587340116\n",
            "Epoch: 719 Loss: 0.04441231316924234\n",
            "Epoch: 720 Loss: 0.044362679352828796\n",
            "Epoch: 721 Loss: 0.044313137967147156\n",
            "Epoch: 722 Loss: 0.04426368884137067\n",
            "Epoch: 723 Loss: 0.044214331804799674\n",
            "Epoch: 724 Loss: 0.044165066686863146\n",
            "Epoch: 725 Loss: 0.044115893317120174\n",
            "Epoch: 726 Loss: 0.04406681152526146\n",
            "Epoch: 727 Loss: 0.0440178211411108\n",
            "Epoch: 728 Loss: 0.04396892199462657\n",
            "Epoch: 729 Loss: 0.04392011391590314\n",
            "Epoch: 730 Loss: 0.043871396735172326\n",
            "Epoch: 731 Loss: 0.04382277028280481\n",
            "Epoch: 732 Loss: 0.04377423438931161\n",
            "Epoch: 733 Loss: 0.043725788885345375\n",
            "Epoch: 734 Loss: 0.04367743360170185\n",
            "Epoch: 735 Loss: 0.043629168369321236\n",
            "Epoch: 736 Loss: 0.043580993019289534\n",
            "Epoch: 737 Loss: 0.04353290738283991\n",
            "Epoch: 738 Loss: 0.04348491129135402\n",
            "Epoch: 739 Loss: 0.043437004576363354\n",
            "Epoch: 740 Loss: 0.04338918706955051\n",
            "Epoch: 741 Loss: 0.04334145860275054\n",
            "Epoch: 742 Loss: 0.04329381900795221\n",
            "Epoch: 743 Loss: 0.04324626811729927\n",
            "Epoch: 744 Loss: 0.04319880576309172\n",
            "Epoch: 745 Loss: 0.04315143177778709\n",
            "Epoch: 746 Loss: 0.043104145994001614\n",
            "Epoch: 747 Loss: 0.043056948244511534\n",
            "Epoch: 748 Loss: 0.04300983836225426\n",
            "Epoch: 749 Loss: 0.04296281618032959\n",
            "Epoch: 750 Loss: 0.04291588153200093\n",
            "Epoch: 751 Loss: 0.04286903425069638\n",
            "Epoch: 752 Loss: 0.04282227417001003\n",
            "Epoch: 753 Loss: 0.04277560112370298\n",
            "Epoch: 754 Loss: 0.04272901494570465\n",
            "Epoch: 755 Loss: 0.042682515470113744\n",
            "Epoch: 756 Loss: 0.042636102531199455\n",
            "Epoch: 757 Loss: 0.042589775963402576\n",
            "Epoch: 758 Loss: 0.042543535601336634\n",
            "Epoch: 759 Loss: 0.042497381279788825\n",
            "Epoch: 760 Loss: 0.0424513128337213\n",
            "Epoch: 761 Loss: 0.04240533009827207\n",
            "Epoch: 762 Loss: 0.04235943290875614\n",
            "Epoch: 763 Loss: 0.042313621100666526\n",
            "Epoch: 764 Loss: 0.042267894509675275\n",
            "Epoch: 765 Loss: 0.04222225297163454\n",
            "Epoch: 766 Loss: 0.0421766963225775\n",
            "Epoch: 767 Loss: 0.042131224398719434\n",
            "Epoch: 768 Loss: 0.042085837036458705\n",
            "Epoch: 769 Loss: 0.042040534072377726\n",
            "Epoch: 770 Loss: 0.041995315343243876\n",
            "Epoch: 771 Loss: 0.04195018068601054\n",
            "Epoch: 772 Loss: 0.04190512993781802\n",
            "Epoch: 773 Loss: 0.041860162935994455\n",
            "Epoch: 774 Loss: 0.041815279518056775\n",
            "Epoch: 775 Loss: 0.041770479521711625\n",
            "Epoch: 776 Loss: 0.04172576278485622\n",
            "Epoch: 777 Loss: 0.04168112914557928\n",
            "Epoch: 778 Loss: 0.04163657844216192\n",
            "Epoch: 779 Loss: 0.041592110513078484\n",
            "Epoch: 780 Loss: 0.04154772519699746\n",
            "Epoch: 781 Loss: 0.041503422332782286\n",
            "Epoch: 782 Loss: 0.041459201759492276\n",
            "Epoch: 783 Loss: 0.04141506331638336\n",
            "Epoch: 784 Loss: 0.041371006842908946\n",
            "Epoch: 785 Loss: 0.04132703217872076\n",
            "Epoch: 786 Loss: 0.04128313916366965\n",
            "Epoch: 787 Loss: 0.041239327637806335\n",
            "Epoch: 788 Loss: 0.04119559744138224\n",
            "Epoch: 789 Loss: 0.04115194841485025\n",
            "Epoch: 790 Loss: 0.0411083803988655\n",
            "Epoch: 791 Loss: 0.041064893234286116\n",
            "Epoch: 792 Loss: 0.04102148676217397\n",
            "Epoch: 793 Loss: 0.04097816082379539\n",
            "Epoch: 794 Loss: 0.04093491526062199\n",
            "Epoch: 795 Loss: 0.04089174991433127\n",
            "Epoch: 796 Loss: 0.04084866462680737\n",
            "Epoch: 797 Loss: 0.040805659240141824\n",
            "Epoch: 798 Loss: 0.040762733596634196\n",
            "Epoch: 799 Loss: 0.04071988753879279\n",
            "Epoch: 800 Loss: 0.04067712090933534\n",
            "Epoch: 801 Loss: 0.040634433551189675\n",
            "Epoch: 802 Loss: 0.04059182530749432\n",
            "Epoch: 803 Loss: 0.04054929602159921\n",
            "Epoch: 804 Loss: 0.040506845537066315\n",
            "Epoch: 805 Loss: 0.0404644736976703\n",
            "Epoch: 806 Loss: 0.040422180347399125\n",
            "Epoch: 807 Loss: 0.0403799653304546\n",
            "Epoch: 808 Loss: 0.04033782849125311\n",
            "Epoch: 809 Loss: 0.04029576967442614\n",
            "Epoch: 810 Loss: 0.04025378872482091\n",
            "Epoch: 811 Loss: 0.04021188548750091\n",
            "Epoch: 812 Loss: 0.04017005980774648\n",
            "Epoch: 813 Loss: 0.04012831153105546\n",
            "Epoch: 814 Loss: 0.04008664050314362\n",
            "Epoch: 815 Loss: 0.0400450465699453\n",
            "Epoch: 816 Loss: 0.04000352957761393\n",
            "Epoch: 817 Loss: 0.039962089372522554\n",
            "Epoch: 818 Loss: 0.03992072580126438\n",
            "Epoch: 819 Loss: 0.039879438710653255\n",
            "Epoch: 820 Loss: 0.03983822794772423\n",
            "Epoch: 821 Loss: 0.039797093359734\n",
            "Epoch: 822 Loss: 0.03975603479416151\n",
            "Epoch: 823 Loss: 0.039715052098708306\n",
            "Epoch: 824 Loss: 0.0396741451212991\n",
            "Epoch: 825 Loss: 0.03963331371008221\n",
            "Epoch: 826 Loss: 0.03959255771343007\n",
            "Epoch: 827 Loss: 0.039551876979939636\n",
            "Epoch: 828 Loss: 0.03951127135843285\n",
            "Epoch: 829 Loss: 0.03947074069795708\n",
            "Epoch: 830 Loss: 0.03943028484778557\n",
            "Epoch: 831 Loss: 0.03938990365741788\n",
            "Epoch: 832 Loss: 0.039349596976580276\n",
            "Epoch: 833 Loss: 0.03930936465522616\n",
            "Epoch: 834 Loss: 0.03926920654353647\n",
            "Epoch: 835 Loss: 0.039229122491920065\n",
            "Epoch: 836 Loss: 0.03918911235101419\n",
            "Epoch: 837 Loss: 0.03914917597168474\n",
            "Epoch: 838 Loss: 0.039109313205026794\n",
            "Epoch: 839 Loss: 0.039069523902364824\n",
            "Epoch: 840 Loss: 0.03902980791525315\n",
            "Epoch: 841 Loss: 0.0389901650954763\n",
            "Epoch: 842 Loss: 0.03895059529504934\n",
            "Epoch: 843 Loss: 0.03891109836621824\n",
            "Epoch: 844 Loss: 0.038871674161460194\n",
            "Epoch: 845 Loss: 0.03883232253348392\n",
            "Epoch: 846 Loss: 0.038793043335230065\n",
            "Epoch: 847 Loss: 0.038753836419871464\n",
            "Epoch: 848 Loss: 0.03871470164081345\n",
            "Epoch: 849 Loss: 0.03867563885169417\n",
            "Epoch: 850 Loss: 0.03863664790638497\n",
            "Epoch: 851 Loss: 0.038597728658990506\n",
            "Epoch: 852 Loss: 0.03855888096384919\n",
            "Epoch: 853 Loss: 0.038520104675533406\n",
            "Epoch: 854 Loss: 0.03848139964884977\n",
            "Epoch: 855 Loss: 0.038442765738839466\n",
            "Epoch: 856 Loss: 0.038404202800778414\n",
            "Epoch: 857 Loss: 0.03836571069017761\n",
            "Epoch: 858 Loss: 0.03832728926278331\n",
            "Epoch: 859 Loss: 0.03828893837457732\n",
            "Epoch: 860 Loss: 0.03825065788177725\n",
            "Epoch: 861 Loss: 0.03821244764083666\n",
            "Epoch: 862 Loss: 0.03817430750844537\n",
            "Epoch: 863 Loss: 0.03813623734152963\n",
            "Epoch: 864 Loss: 0.0380982369972524\n",
            "Epoch: 865 Loss: 0.03806030633301347\n",
            "Epoch: 866 Loss: 0.03802244520644973\n",
            "Epoch: 867 Loss: 0.03798465347543534\n",
            "Epoch: 868 Loss: 0.03794693099808193\n",
            "Epoch: 869 Loss: 0.037909277632738776\n",
            "Epoch: 870 Loss: 0.03787169323799302\n",
            "Epoch: 871 Loss: 0.037834177672669764\n",
            "Epoch: 872 Loss: 0.03779673079583234\n",
            "Epoch: 873 Loss: 0.03775935246678241\n",
            "Epoch: 874 Loss: 0.03772204254506013\n",
            "Epoch: 875 Loss: 0.03768480089044432\n",
            "Epoch: 876 Loss: 0.03764762736295263\n",
            "Epoch: 877 Loss: 0.03761052182284161\n",
            "Epoch: 878 Loss: 0.03757348413060696\n",
            "Epoch: 879 Loss: 0.03753651414698358\n",
            "Epoch: 880 Loss: 0.0374996117329457\n",
            "Epoch: 881 Loss: 0.03746277674970705\n",
            "Epoch: 882 Loss: 0.037426009058720976\n",
            "Epoch: 883 Loss: 0.03738930852168044\n",
            "Epoch: 884 Loss: 0.03735267500051832\n",
            "Epoch: 885 Loss: 0.03731610835740737\n",
            "Epoch: 886 Loss: 0.03727960845476033\n",
            "Epoch: 887 Loss: 0.03724317515523012\n",
            "Epoch: 888 Loss: 0.037206808321709764\n",
            "Epoch: 889 Loss: 0.03717050781733264\n",
            "Epoch: 890 Loss: 0.03713427350547246\n",
            "Epoch: 891 Loss: 0.03709810524974337\n",
            "Epoch: 892 Loss: 0.037062002914000025\n",
            "Epoch: 893 Loss: 0.03702596636233765\n",
            "Epoch: 894 Loss: 0.036989995459092095\n",
            "Epoch: 895 Loss: 0.03695409006883991\n",
            "Epoch: 896 Loss: 0.036918250056398344\n",
            "Epoch: 897 Loss: 0.036882475286825485\n",
            "Epoch: 898 Loss: 0.03684676562542019\n",
            "Epoch: 899 Loss: 0.03681112093772225\n",
            "Epoch: 900 Loss: 0.03677554108951228\n",
            "Epoch: 901 Loss: 0.03674002594681188\n",
            "Epoch: 902 Loss: 0.036704575375883584\n",
            "Epoch: 903 Loss: 0.03666918924323089\n",
            "Epoch: 904 Loss: 0.036633867415598306\n",
            "Epoch: 905 Loss: 0.03659860975997135\n",
            "Epoch: 906 Loss: 0.03656341614357651\n",
            "Epoch: 907 Loss: 0.036528286433881355\n",
            "Epoch: 908 Loss: 0.03649322049859441\n",
            "Epoch: 909 Loss: 0.03645821820566525\n",
            "Epoch: 910 Loss: 0.036423279423284424\n",
            "Epoch: 911 Loss: 0.03638840401988351\n",
            "Epoch: 912 Loss: 0.036353591864135046\n",
            "Epoch: 913 Loss: 0.03631884282495254\n",
            "Epoch: 914 Loss: 0.03628415677149038\n",
            "Epoch: 915 Loss: 0.036249533573143936\n",
            "Epoch: 916 Loss: 0.036214973099549415\n",
            "Epoch: 917 Loss: 0.03618047522058387\n",
            "Epoch: 918 Loss: 0.036146039806365146\n",
            "Epoch: 919 Loss: 0.03611166672725184\n",
            "Epoch: 920 Loss: 0.036077355853843286\n",
            "Epoch: 921 Loss: 0.03604310705697947\n",
            "Epoch: 922 Loss: 0.03600892020774097\n",
            "Epoch: 923 Loss: 0.03597479517744892\n",
            "Epoch: 924 Loss: 0.03594073183766497\n",
            "Epoch: 925 Loss: 0.035906730060191176\n",
            "Epoch: 926 Loss: 0.03587278971706994\n",
            "Epoch: 927 Loss: 0.03583891068058396\n",
            "Epoch: 928 Loss: 0.03580509282325616\n",
            "Epoch: 929 Loss: 0.03577133601784958\n",
            "Epoch: 930 Loss: 0.03573764013736732\n",
            "Epoch: 931 Loss: 0.03570400505505244\n",
            "Epoch: 932 Loss: 0.03567043064438787\n",
            "Epoch: 933 Loss: 0.035636916779096375\n",
            "Epoch: 934 Loss: 0.03560346333314033\n",
            "Epoch: 935 Loss: 0.03557007018072179\n",
            "Epoch: 936 Loss: 0.035536737196282225\n",
            "Epoch: 937 Loss: 0.035503464254502556\n",
            "Epoch: 938 Loss: 0.03547025123030296\n",
            "Epoch: 939 Loss: 0.03543709799884276\n",
            "Epoch: 940 Loss: 0.035404004435520386\n",
            "Epoch: 941 Loss: 0.03537097041597316\n",
            "Epoch: 942 Loss: 0.035337995816077244\n",
            "Epoch: 943 Loss: 0.0353050805119475\n",
            "Epoch: 944 Loss: 0.035272224379937336\n",
            "Epoch: 945 Loss: 0.03523942729663864\n",
            "Epoch: 946 Loss: 0.035206689138881565\n",
            "Epoch: 947 Loss: 0.03517400978373446\n",
            "Epoch: 948 Loss: 0.035141389108503736\n",
            "Epoch: 949 Loss: 0.03510882699073365\n",
            "Epoch: 950 Loss: 0.03507632330820623\n",
            "Epoch: 951 Loss: 0.03504387793894113\n",
            "Epoch: 952 Loss: 0.035011490761195425\n",
            "Epoch: 953 Loss: 0.03497916165346354\n",
            "Epoch: 954 Loss: 0.034946890494477044\n",
            "Epoch: 955 Loss: 0.03491467716320445\n",
            "Epoch: 956 Loss: 0.03488252153885116\n",
            "Epoch: 957 Loss: 0.034850423500859234\n",
            "Epoch: 958 Loss: 0.034818382928907245\n",
            "Epoch: 959 Loss: 0.03478639970291009\n",
            "Epoch: 960 Loss: 0.03475447370301883\n",
            "Epoch: 961 Loss: 0.034722604809620555\n",
            "Epoch: 962 Loss: 0.03469079290333814\n",
            "Epoch: 963 Loss: 0.03465903786503013\n",
            "Epoch: 964 Loss: 0.034627339575790526\n",
            "Epoch: 965 Loss: 0.03459569791694861\n",
            "Epoch: 966 Loss: 0.03456411277006877\n",
            "Epoch: 967 Loss: 0.034532584016950266\n",
            "Epoch: 968 Loss: 0.034501111539627136\n",
            "Epoch: 969 Loss: 0.03446969522036789\n",
            "Epoch: 970 Loss: 0.03443833494167542\n",
            "Epoch: 971 Loss: 0.03440703058628671\n",
            "Epoch: 972 Loss: 0.034375782037172696\n",
            "Epoch: 973 Loss: 0.034344589177538076\n",
            "Epoch: 974 Loss: 0.034313451890821016\n",
            "Epoch: 975 Loss: 0.034282370060693096\n",
            "Epoch: 976 Loss: 0.03425134357105893\n",
            "Epoch: 977 Loss: 0.03422037230605608\n",
            "Epoch: 978 Loss: 0.03418945615005483\n",
            "Epoch: 979 Loss: 0.034158594987657886\n",
            "Epoch: 980 Loss: 0.034127788703700285\n",
            "Epoch: 981 Loss: 0.03409703718324906\n",
            "Epoch: 982 Loss: 0.03406634031160307\n",
            "Epoch: 983 Loss: 0.0340356979742928\n",
            "Epoch: 984 Loss: 0.034005110057080144\n",
            "Epoch: 985 Loss: 0.03397457644595808\n",
            "Epoch: 986 Loss: 0.03394409702715054\n",
            "Epoch: 987 Loss: 0.03391367168711218\n",
            "Epoch: 988 Loss: 0.03388330031252806\n",
            "Epoch: 989 Loss: 0.03385298279031351\n",
            "Epoch: 990 Loss: 0.03382271900761385\n",
            "Epoch: 991 Loss: 0.03379250885180414\n",
            "Epoch: 992 Loss: 0.03376235221048893\n",
            "Epoch: 993 Loss: 0.03373224897150208\n",
            "Epoch: 994 Loss: 0.03370219902290647\n",
            "Epoch: 995 Loss: 0.03367220225299374\n",
            "Epoch: 996 Loss: 0.033642258550284077\n",
            "Epoch: 997 Loss: 0.03361236780352597\n",
            "Epoch: 998 Loss: 0.03358252990169593\n",
            "Epoch: 999 Loss: 0.03355274473399825\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PxurF963gz1Y",
        "outputId": "ab9c48ec-8a95-4e14-d9e5-c9e4f4f94541",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 117
        }
      },
      "source": [
        "my_neuralnet2.predict(np.array([8,4,9]).reshape((3,1)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input : \n",
            "[[8]\n",
            " [4]\n",
            " [9]]\n",
            "Output: \n",
            "[[0.99999899]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PzOnQlFbBk3"
      },
      "source": [
        "# plt.plot(loss1,label=\"without minibatch\")\n",
        "# plt.plot(loss2,label=\"minibatch\")\n",
        "# plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kqw4fgTcF6n",
        "outputId": "5a62c633-04f6-471a-bd5a-75743f9133ff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "sns.set()\n",
        "plt.plot(loss2)\n",
        "plt.title(\"Minibatch Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "# plt.savefig(\"180100039.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMSewMBydqhl",
        "outputId": "64514c68-8b74-4937-babd-5cb53b4d3b4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        }
      },
      "source": [
        "plt.plot(loss1,label=\"SGD\")\n",
        "plt.plot(loss2,label=\"minibatch\")\n",
        "plt.title(\"Both Losses Together\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x235f61e3e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRVn-n0JJpNG"
      },
      "source": [
        "**So the major differnce we notice in the two learning algorithms are:**\n",
        "<br>\n",
        "**<i>Minibatch is much faster than SGD but it learns less for a give number of epochs as can be seen in the loss curves above</i>**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQ-j459JD1pl"
      },
      "source": [
        "# Exploding and Vanishing gradients "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzD99_zPJpNH"
      },
      "source": [
        "**Exploding**: For this we increase the no. of neurons in the hidden layers\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOybsnCqJpNH"
      },
      "source": [
        "#D_in is input dimension\n",
        "#H1 is dimension of first hidden layer \n",
        "#H2 is dimension of second hidden layer\n",
        "#D_out is output dimension.\n",
        "D_in, H1, H2,H3,H4, D_out = inputsize, 30, 50 ,100, 50, 1      \n",
        "\n",
        "#NOTE THE CHANGE IN THE NUMBER OF NEURONS ABOVE\n",
        "\n",
        "#You can add more layers if you wish to \n",
        "neurons = [D_in, H1, H2,H3,H4, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['linear','linear', 'relu','tanh', 'sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
        "my_neuralnet3= Neural_Network(neurons, activation_functions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKDeYA_OJpNJ",
        "outputId": "de8b12f2-a0f7-4101-c81a-71d0669dd18a"
      },
      "source": [
        "loss3 = my_neuralnet3.train(train_X,train_Y,minibatch=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "C:\\Users\\DAY17\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: overflow encountered in exp\n",
            "C:\\Users\\DAY17\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: RuntimeWarning: invalid value encountered in true_divide\n",
            "C:\\Users\\DAY17\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:74: RuntimeWarning: invalid value encountered in greater\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: nan\n",
            "Epoch: 1 Loss: nan\n",
            "Epoch: 2 Loss: nan\n",
            "Epoch: 3 Loss: nan\n",
            "Epoch: 4 Loss: nan\n",
            "Epoch: 5 Loss: nan\n",
            "Epoch: 6 Loss: nan\n",
            "Epoch: 7 Loss: nan\n",
            "Epoch: 8 Loss: nan\n",
            "Epoch: 9 Loss: nan\n",
            "Epoch: 10 Loss: nan\n",
            "Epoch: 11 Loss: nan\n",
            "Epoch: 12 Loss: nan\n",
            "Epoch: 13 Loss: nan\n",
            "Epoch: 14 Loss: nan\n",
            "Epoch: 15 Loss: nan\n",
            "Epoch: 16 Loss: nan\n",
            "Epoch: 17 Loss: nan\n",
            "Epoch: 18 Loss: nan\n",
            "Epoch: 19 Loss: nan\n",
            "Epoch: 20 Loss: nan\n",
            "Epoch: 21 Loss: nan\n",
            "Epoch: 22 Loss: nan\n",
            "Epoch: 23 Loss: nan\n",
            "Epoch: 24 Loss: nan\n",
            "Epoch: 25 Loss: nan\n",
            "Epoch: 26 Loss: nan\n",
            "Epoch: 27 Loss: nan\n",
            "Epoch: 28 Loss: nan\n",
            "Epoch: 29 Loss: nan\n",
            "Epoch: 30 Loss: nan\n",
            "Epoch: 31 Loss: nan\n",
            "Epoch: 32 Loss: nan\n",
            "Epoch: 33 Loss: nan\n",
            "Epoch: 34 Loss: nan\n",
            "Epoch: 35 Loss: nan\n",
            "Epoch: 36 Loss: nan\n",
            "Epoch: 37 Loss: nan\n",
            "Epoch: 38 Loss: nan\n",
            "Epoch: 39 Loss: nan\n",
            "Epoch: 40 Loss: nan\n",
            "Epoch: 41 Loss: nan\n",
            "Epoch: 42 Loss: nan\n",
            "Epoch: 43 Loss: nan\n",
            "Epoch: 44 Loss: nan\n",
            "Epoch: 45 Loss: nan\n",
            "Epoch: 46 Loss: nan\n",
            "Epoch: 47 Loss: nan\n",
            "Epoch: 48 Loss: nan\n",
            "Epoch: 49 Loss: nan\n",
            "Epoch: 50 Loss: nan\n",
            "Epoch: 51 Loss: nan\n",
            "Epoch: 52 Loss: nan\n",
            "Epoch: 53 Loss: nan\n",
            "Epoch: 54 Loss: nan\n",
            "Epoch: 55 Loss: nan\n",
            "Epoch: 56 Loss: nan\n",
            "Epoch: 57 Loss: nan\n",
            "Epoch: 58 Loss: nan\n",
            "Epoch: 59 Loss: nan\n",
            "Epoch: 60 Loss: nan\n",
            "Epoch: 61 Loss: nan\n",
            "Epoch: 62 Loss: nan\n",
            "Epoch: 63 Loss: nan\n",
            "Epoch: 64 Loss: nan\n",
            "Epoch: 65 Loss: nan\n",
            "Epoch: 66 Loss: nan\n",
            "Epoch: 67 Loss: nan\n",
            "Epoch: 68 Loss: nan\n",
            "Epoch: 69 Loss: nan\n",
            "Epoch: 70 Loss: nan\n",
            "Epoch: 71 Loss: nan\n",
            "Epoch: 72 Loss: nan\n",
            "Epoch: 73 Loss: nan\n",
            "Epoch: 74 Loss: nan\n",
            "Epoch: 75 Loss: nan\n",
            "Epoch: 76 Loss: nan\n",
            "Epoch: 77 Loss: nan\n",
            "Epoch: 78 Loss: nan\n",
            "Epoch: 79 Loss: nan\n",
            "Epoch: 80 Loss: nan\n",
            "Epoch: 81 Loss: nan\n",
            "Epoch: 82 Loss: nan\n",
            "Epoch: 83 Loss: nan\n",
            "Epoch: 84 Loss: nan\n",
            "Epoch: 85 Loss: nan\n",
            "Epoch: 86 Loss: nan\n",
            "Epoch: 87 Loss: nan\n",
            "Epoch: 88 Loss: nan\n",
            "Epoch: 89 Loss: nan\n",
            "Epoch: 90 Loss: nan\n",
            "Epoch: 91 Loss: nan\n",
            "Epoch: 92 Loss: nan\n",
            "Epoch: 93 Loss: nan\n",
            "Epoch: 94 Loss: nan\n",
            "Epoch: 95 Loss: nan\n",
            "Epoch: 96 Loss: nan\n",
            "Epoch: 97 Loss: nan\n",
            "Epoch: 98 Loss: nan\n",
            "Epoch: 99 Loss: nan\n",
            "Epoch: 100 Loss: nan\n",
            "Epoch: 101 Loss: nan\n",
            "Epoch: 102 Loss: nan\n",
            "Epoch: 103 Loss: nan\n",
            "Epoch: 104 Loss: nan\n",
            "Epoch: 105 Loss: nan\n",
            "Epoch: 106 Loss: nan\n",
            "Epoch: 107 Loss: nan\n",
            "Epoch: 108 Loss: nan\n",
            "Epoch: 109 Loss: nan\n",
            "Epoch: 110 Loss: nan\n",
            "Epoch: 111 Loss: nan\n",
            "Epoch: 112 Loss: nan\n",
            "Epoch: 113 Loss: nan\n",
            "Epoch: 114 Loss: nan\n",
            "Epoch: 115 Loss: nan\n",
            "Epoch: 116 Loss: nan\n",
            "Epoch: 117 Loss: nan\n",
            "Epoch: 118 Loss: nan\n",
            "Epoch: 119 Loss: nan\n",
            "Epoch: 120 Loss: nan\n",
            "Epoch: 121 Loss: nan\n",
            "Epoch: 122 Loss: nan\n",
            "Epoch: 123 Loss: nan\n",
            "Epoch: 124 Loss: nan\n",
            "Epoch: 125 Loss: nan\n",
            "Epoch: 126 Loss: nan\n",
            "Epoch: 127 Loss: nan\n",
            "Epoch: 128 Loss: nan\n",
            "Epoch: 129 Loss: nan\n",
            "Epoch: 130 Loss: nan\n",
            "Epoch: 131 Loss: nan\n",
            "Epoch: 132 Loss: nan\n",
            "Epoch: 133 Loss: nan\n",
            "Epoch: 134 Loss: nan\n",
            "Epoch: 135 Loss: nan\n",
            "Epoch: 136 Loss: nan\n",
            "Epoch: 137 Loss: nan\n",
            "Epoch: 138 Loss: nan\n",
            "Epoch: 139 Loss: nan\n",
            "Epoch: 140 Loss: nan\n",
            "Epoch: 141 Loss: nan\n",
            "Epoch: 142 Loss: nan\n",
            "Epoch: 143 Loss: nan\n",
            "Epoch: 144 Loss: nan\n",
            "Epoch: 145 Loss: nan\n",
            "Epoch: 146 Loss: nan\n",
            "Epoch: 147 Loss: nan\n",
            "Epoch: 148 Loss: nan\n",
            "Epoch: 149 Loss: nan\n",
            "Epoch: 150 Loss: nan\n",
            "Epoch: 151 Loss: nan\n",
            "Epoch: 152 Loss: nan\n",
            "Epoch: 153 Loss: nan\n",
            "Epoch: 154 Loss: nan\n",
            "Epoch: 155 Loss: nan\n",
            "Epoch: 156 Loss: nan\n",
            "Epoch: 157 Loss: nan\n",
            "Epoch: 158 Loss: nan\n",
            "Epoch: 159 Loss: nan\n",
            "Epoch: 160 Loss: nan\n",
            "Epoch: 161 Loss: nan\n",
            "Epoch: 162 Loss: nan\n",
            "Epoch: 163 Loss: nan\n",
            "Epoch: 164 Loss: nan\n",
            "Epoch: 165 Loss: nan\n",
            "Epoch: 166 Loss: nan\n",
            "Epoch: 167 Loss: nan\n",
            "Epoch: 168 Loss: nan\n",
            "Epoch: 169 Loss: nan\n",
            "Epoch: 170 Loss: nan\n",
            "Epoch: 171 Loss: nan\n",
            "Epoch: 172 Loss: nan\n",
            "Epoch: 173 Loss: nan\n",
            "Epoch: 174 Loss: nan\n",
            "Epoch: 175 Loss: nan\n",
            "Epoch: 176 Loss: nan\n",
            "Epoch: 177 Loss: nan\n",
            "Epoch: 178 Loss: nan\n",
            "Epoch: 179 Loss: nan\n",
            "Epoch: 180 Loss: nan\n",
            "Epoch: 181 Loss: nan\n",
            "Epoch: 182 Loss: nan\n",
            "Epoch: 183 Loss: nan\n",
            "Epoch: 184 Loss: nan\n",
            "Epoch: 185 Loss: nan\n",
            "Epoch: 186 Loss: nan\n",
            "Epoch: 187 Loss: nan\n",
            "Epoch: 188 Loss: nan\n",
            "Epoch: 189 Loss: nan\n",
            "Epoch: 190 Loss: nan\n",
            "Epoch: 191 Loss: nan\n",
            "Epoch: 192 Loss: nan\n",
            "Epoch: 193 Loss: nan\n",
            "Epoch: 194 Loss: nan\n",
            "Epoch: 195 Loss: nan\n",
            "Epoch: 196 Loss: nan\n",
            "Epoch: 197 Loss: nan\n",
            "Epoch: 198 Loss: nan\n",
            "Epoch: 199 Loss: nan\n",
            "Epoch: 200 Loss: nan\n",
            "Epoch: 201 Loss: nan\n",
            "Epoch: 202 Loss: nan\n",
            "Epoch: 203 Loss: nan\n",
            "Epoch: 204 Loss: nan\n",
            "Epoch: 205 Loss: nan\n",
            "Epoch: 206 Loss: nan\n",
            "Epoch: 207 Loss: nan\n",
            "Epoch: 208 Loss: nan\n",
            "Epoch: 209 Loss: nan\n",
            "Epoch: 210 Loss: nan\n",
            "Epoch: 211 Loss: nan\n",
            "Epoch: 212 Loss: nan\n",
            "Epoch: 213 Loss: nan\n",
            "Epoch: 214 Loss: nan\n",
            "Epoch: 215 Loss: nan\n",
            "Epoch: 216 Loss: nan\n",
            "Epoch: 217 Loss: nan\n",
            "Epoch: 218 Loss: nan\n",
            "Epoch: 219 Loss: nan\n",
            "Epoch: 220 Loss: nan\n",
            "Epoch: 221 Loss: nan\n",
            "Epoch: 222 Loss: nan\n",
            "Epoch: 223 Loss: nan\n",
            "Epoch: 224 Loss: nan\n",
            "Epoch: 225 Loss: nan\n",
            "Epoch: 226 Loss: nan\n",
            "Epoch: 227 Loss: nan\n",
            "Epoch: 228 Loss: nan\n",
            "Epoch: 229 Loss: nan\n",
            "Epoch: 230 Loss: nan\n",
            "Epoch: 231 Loss: nan\n",
            "Epoch: 232 Loss: nan\n",
            "Epoch: 233 Loss: nan\n",
            "Epoch: 234 Loss: nan\n",
            "Epoch: 235 Loss: nan\n",
            "Epoch: 236 Loss: nan\n",
            "Epoch: 237 Loss: nan\n",
            "Epoch: 238 Loss: nan\n",
            "Epoch: 239 Loss: nan\n",
            "Epoch: 240 Loss: nan\n",
            "Epoch: 241 Loss: nan\n",
            "Epoch: 242 Loss: nan\n",
            "Epoch: 243 Loss: nan\n",
            "Epoch: 244 Loss: nan\n",
            "Epoch: 245 Loss: nan\n",
            "Epoch: 246 Loss: nan\n",
            "Epoch: 247 Loss: nan\n",
            "Epoch: 248 Loss: nan\n",
            "Epoch: 249 Loss: nan\n",
            "Epoch: 250 Loss: nan\n",
            "Epoch: 251 Loss: nan\n",
            "Epoch: 252 Loss: nan\n",
            "Epoch: 253 Loss: nan\n",
            "Epoch: 254 Loss: nan\n",
            "Epoch: 255 Loss: nan\n",
            "Epoch: 256 Loss: nan\n",
            "Epoch: 257 Loss: nan\n",
            "Epoch: 258 Loss: nan\n",
            "Epoch: 259 Loss: nan\n",
            "Epoch: 260 Loss: nan\n",
            "Epoch: 261 Loss: nan\n",
            "Epoch: 262 Loss: nan\n",
            "Epoch: 263 Loss: nan\n",
            "Epoch: 264 Loss: nan\n",
            "Epoch: 265 Loss: nan\n",
            "Epoch: 266 Loss: nan\n",
            "Epoch: 267 Loss: nan\n",
            "Epoch: 268 Loss: nan\n",
            "Epoch: 269 Loss: nan\n",
            "Epoch: 270 Loss: nan\n",
            "Epoch: 271 Loss: nan\n",
            "Epoch: 272 Loss: nan\n",
            "Epoch: 273 Loss: nan\n",
            "Epoch: 274 Loss: nan\n",
            "Epoch: 275 Loss: nan\n",
            "Epoch: 276 Loss: nan\n",
            "Epoch: 277 Loss: nan\n",
            "Epoch: 278 Loss: nan\n",
            "Epoch: 279 Loss: nan\n",
            "Epoch: 280 Loss: nan\n",
            "Epoch: 281 Loss: nan\n",
            "Epoch: 282 Loss: nan\n",
            "Epoch: 283 Loss: nan\n",
            "Epoch: 284 Loss: nan\n",
            "Epoch: 285 Loss: nan\n",
            "Epoch: 286 Loss: nan\n",
            "Epoch: 287 Loss: nan\n",
            "Epoch: 288 Loss: nan\n",
            "Epoch: 289 Loss: nan\n",
            "Epoch: 290 Loss: nan\n",
            "Epoch: 291 Loss: nan\n",
            "Epoch: 292 Loss: nan\n",
            "Epoch: 293 Loss: nan\n",
            "Epoch: 294 Loss: nan\n",
            "Epoch: 295 Loss: nan\n",
            "Epoch: 296 Loss: nan\n",
            "Epoch: 297 Loss: nan\n",
            "Epoch: 298 Loss: nan\n",
            "Epoch: 299 Loss: nan\n",
            "Epoch: 300 Loss: nan\n",
            "Epoch: 301 Loss: nan\n",
            "Epoch: 302 Loss: nan\n",
            "Epoch: 303 Loss: nan\n",
            "Epoch: 304 Loss: nan\n",
            "Epoch: 305 Loss: nan\n",
            "Epoch: 306 Loss: nan\n",
            "Epoch: 307 Loss: nan\n",
            "Epoch: 308 Loss: nan\n",
            "Epoch: 309 Loss: nan\n",
            "Epoch: 310 Loss: nan\n",
            "Epoch: 311 Loss: nan\n",
            "Epoch: 312 Loss: nan\n",
            "Epoch: 313 Loss: nan\n",
            "Epoch: 314 Loss: nan\n",
            "Epoch: 315 Loss: nan\n",
            "Epoch: 316 Loss: nan\n",
            "Epoch: 317 Loss: nan\n",
            "Epoch: 318 Loss: nan\n",
            "Epoch: 319 Loss: nan\n",
            "Epoch: 320 Loss: nan\n",
            "Epoch: 321 Loss: nan\n",
            "Epoch: 322 Loss: nan\n",
            "Epoch: 323 Loss: nan\n",
            "Epoch: 324 Loss: nan\n",
            "Epoch: 325 Loss: nan\n",
            "Epoch: 326 Loss: nan\n",
            "Epoch: 327 Loss: nan\n",
            "Epoch: 328 Loss: nan\n",
            "Epoch: 329 Loss: nan\n",
            "Epoch: 330 Loss: nan\n",
            "Epoch: 331 Loss: nan\n",
            "Epoch: 332 Loss: nan\n",
            "Epoch: 333 Loss: nan\n",
            "Epoch: 334 Loss: nan\n",
            "Epoch: 335 Loss: nan\n",
            "Epoch: 336 Loss: nan\n",
            "Epoch: 337 Loss: nan\n",
            "Epoch: 338 Loss: nan\n",
            "Epoch: 339 Loss: nan\n",
            "Epoch: 340 Loss: nan\n",
            "Epoch: 341 Loss: nan\n",
            "Epoch: 342 Loss: nan\n",
            "Epoch: 343 Loss: nan\n",
            "Epoch: 344 Loss: nan\n",
            "Epoch: 345 Loss: nan\n",
            "Epoch: 346 Loss: nan\n",
            "Epoch: 347 Loss: nan\n",
            "Epoch: 348 Loss: nan\n",
            "Epoch: 349 Loss: nan\n",
            "Epoch: 350 Loss: nan\n",
            "Epoch: 351 Loss: nan\n",
            "Epoch: 352 Loss: nan\n",
            "Epoch: 353 Loss: nan\n",
            "Epoch: 354 Loss: nan\n",
            "Epoch: 355 Loss: nan\n",
            "Epoch: 356 Loss: nan\n",
            "Epoch: 357 Loss: nan\n",
            "Epoch: 358 Loss: nan\n",
            "Epoch: 359 Loss: nan\n",
            "Epoch: 360 Loss: nan\n",
            "Epoch: 361 Loss: nan\n",
            "Epoch: 362 Loss: nan\n",
            "Epoch: 363 Loss: nan\n",
            "Epoch: 364 Loss: nan\n",
            "Epoch: 365 Loss: nan\n",
            "Epoch: 366 Loss: nan\n",
            "Epoch: 367 Loss: nan\n",
            "Epoch: 368 Loss: nan\n",
            "Epoch: 369 Loss: nan\n",
            "Epoch: 370 Loss: nan\n",
            "Epoch: 371 Loss: nan\n",
            "Epoch: 372 Loss: nan\n",
            "Epoch: 373 Loss: nan\n",
            "Epoch: 374 Loss: nan\n",
            "Epoch: 375 Loss: nan\n",
            "Epoch: 376 Loss: nan\n",
            "Epoch: 377 Loss: nan\n",
            "Epoch: 378 Loss: nan\n",
            "Epoch: 379 Loss: nan\n",
            "Epoch: 380 Loss: nan\n",
            "Epoch: 381 Loss: nan\n",
            "Epoch: 382 Loss: nan\n",
            "Epoch: 383 Loss: nan\n",
            "Epoch: 384 Loss: nan\n",
            "Epoch: 385 Loss: nan\n",
            "Epoch: 386 Loss: nan\n",
            "Epoch: 387 Loss: nan\n",
            "Epoch: 388 Loss: nan\n",
            "Epoch: 389 Loss: nan\n",
            "Epoch: 390 Loss: nan\n",
            "Epoch: 391 Loss: nan\n",
            "Epoch: 392 Loss: nan\n",
            "Epoch: 393 Loss: nan\n",
            "Epoch: 394 Loss: nan\n",
            "Epoch: 395 Loss: nan\n",
            "Epoch: 396 Loss: nan\n",
            "Epoch: 397 Loss: nan\n",
            "Epoch: 398 Loss: nan\n",
            "Epoch: 399 Loss: nan\n",
            "Epoch: 400 Loss: nan\n",
            "Epoch: 401 Loss: nan\n",
            "Epoch: 402 Loss: nan\n",
            "Epoch: 403 Loss: nan\n",
            "Epoch: 404 Loss: nan\n",
            "Epoch: 405 Loss: nan\n",
            "Epoch: 406 Loss: nan\n",
            "Epoch: 407 Loss: nan\n",
            "Epoch: 408 Loss: nan\n",
            "Epoch: 409 Loss: nan\n",
            "Epoch: 410 Loss: nan\n",
            "Epoch: 411 Loss: nan\n",
            "Epoch: 412 Loss: nan\n",
            "Epoch: 413 Loss: nan\n",
            "Epoch: 414 Loss: nan\n",
            "Epoch: 415 Loss: nan\n",
            "Epoch: 416 Loss: nan\n",
            "Epoch: 417 Loss: nan\n",
            "Epoch: 418 Loss: nan\n",
            "Epoch: 419 Loss: nan\n",
            "Epoch: 420 Loss: nan\n",
            "Epoch: 421 Loss: nan\n",
            "Epoch: 422 Loss: nan\n",
            "Epoch: 423 Loss: nan\n",
            "Epoch: 424 Loss: nan\n",
            "Epoch: 425 Loss: nan\n",
            "Epoch: 426 Loss: nan\n",
            "Epoch: 427 Loss: nan\n",
            "Epoch: 428 Loss: nan\n",
            "Epoch: 429 Loss: nan\n",
            "Epoch: 430 Loss: nan\n",
            "Epoch: 431 Loss: nan\n",
            "Epoch: 432 Loss: nan\n",
            "Epoch: 433 Loss: nan\n",
            "Epoch: 434 Loss: nan\n",
            "Epoch: 435 Loss: nan\n",
            "Epoch: 436 Loss: nan\n",
            "Epoch: 437 Loss: nan\n",
            "Epoch: 438 Loss: nan\n",
            "Epoch: 439 Loss: nan\n",
            "Epoch: 440 Loss: nan\n",
            "Epoch: 441 Loss: nan\n",
            "Epoch: 442 Loss: nan\n",
            "Epoch: 443 Loss: nan\n",
            "Epoch: 444 Loss: nan\n",
            "Epoch: 445 Loss: nan\n",
            "Epoch: 446 Loss: nan\n",
            "Epoch: 447 Loss: nan\n",
            "Epoch: 448 Loss: nan\n",
            "Epoch: 449 Loss: nan\n",
            "Epoch: 450 Loss: nan\n",
            "Epoch: 451 Loss: nan\n",
            "Epoch: 452 Loss: nan\n",
            "Epoch: 453 Loss: nan\n",
            "Epoch: 454 Loss: nan\n",
            "Epoch: 455 Loss: nan\n",
            "Epoch: 456 Loss: nan\n",
            "Epoch: 457 Loss: nan\n",
            "Epoch: 458 Loss: nan\n",
            "Epoch: 459 Loss: nan\n",
            "Epoch: 460 Loss: nan\n",
            "Epoch: 461 Loss: nan\n",
            "Epoch: 462 Loss: nan\n",
            "Epoch: 463 Loss: nan\n",
            "Epoch: 464 Loss: nan\n",
            "Epoch: 465 Loss: nan\n",
            "Epoch: 466 Loss: nan\n",
            "Epoch: 467 Loss: nan\n",
            "Epoch: 468 Loss: nan\n",
            "Epoch: 469 Loss: nan\n",
            "Epoch: 470 Loss: nan\n",
            "Epoch: 471 Loss: nan\n",
            "Epoch: 472 Loss: nan\n",
            "Epoch: 473 Loss: nan\n",
            "Epoch: 474 Loss: nan\n",
            "Epoch: 475 Loss: nan\n",
            "Epoch: 476 Loss: nan\n",
            "Epoch: 477 Loss: nan\n",
            "Epoch: 478 Loss: nan\n",
            "Epoch: 479 Loss: nan\n",
            "Epoch: 480 Loss: nan\n",
            "Epoch: 481 Loss: nan\n",
            "Epoch: 482 Loss: nan\n",
            "Epoch: 483 Loss: nan\n",
            "Epoch: 484 Loss: nan\n",
            "Epoch: 485 Loss: nan\n",
            "Epoch: 486 Loss: nan\n",
            "Epoch: 487 Loss: nan\n",
            "Epoch: 488 Loss: nan\n",
            "Epoch: 489 Loss: nan\n",
            "Epoch: 490 Loss: nan\n",
            "Epoch: 491 Loss: nan\n",
            "Epoch: 492 Loss: nan\n",
            "Epoch: 493 Loss: nan\n",
            "Epoch: 494 Loss: nan\n",
            "Epoch: 495 Loss: nan\n",
            "Epoch: 496 Loss: nan\n",
            "Epoch: 497 Loss: nan\n",
            "Epoch: 498 Loss: nan\n",
            "Epoch: 499 Loss: nan\n",
            "Epoch: 500 Loss: nan\n",
            "Epoch: 501 Loss: nan\n",
            "Epoch: 502 Loss: nan\n",
            "Epoch: 503 Loss: nan\n",
            "Epoch: 504 Loss: nan\n",
            "Epoch: 505 Loss: nan\n",
            "Epoch: 506 Loss: nan\n",
            "Epoch: 507 Loss: nan\n",
            "Epoch: 508 Loss: nan\n",
            "Epoch: 509 Loss: nan\n",
            "Epoch: 510 Loss: nan\n",
            "Epoch: 511 Loss: nan\n",
            "Epoch: 512 Loss: nan\n",
            "Epoch: 513 Loss: nan\n",
            "Epoch: 514 Loss: nan\n",
            "Epoch: 515 Loss: nan\n",
            "Epoch: 516 Loss: nan\n",
            "Epoch: 517 Loss: nan\n",
            "Epoch: 518 Loss: nan\n",
            "Epoch: 519 Loss: nan\n",
            "Epoch: 520 Loss: nan\n",
            "Epoch: 521 Loss: nan\n",
            "Epoch: 522 Loss: nan\n",
            "Epoch: 523 Loss: nan\n",
            "Epoch: 524 Loss: nan\n",
            "Epoch: 525 Loss: nan\n",
            "Epoch: 526 Loss: nan\n",
            "Epoch: 527 Loss: nan\n",
            "Epoch: 528 Loss: nan\n",
            "Epoch: 529 Loss: nan\n",
            "Epoch: 530 Loss: nan\n",
            "Epoch: 531 Loss: nan\n",
            "Epoch: 532 Loss: nan\n",
            "Epoch: 533 Loss: nan\n",
            "Epoch: 534 Loss: nan\n",
            "Epoch: 535 Loss: nan\n",
            "Epoch: 536 Loss: nan\n",
            "Epoch: 537 Loss: nan\n",
            "Epoch: 538 Loss: nan\n",
            "Epoch: 539 Loss: nan\n",
            "Epoch: 540 Loss: nan\n",
            "Epoch: 541 Loss: nan\n",
            "Epoch: 542 Loss: nan\n",
            "Epoch: 543 Loss: nan\n",
            "Epoch: 544 Loss: nan\n",
            "Epoch: 545 Loss: nan\n",
            "Epoch: 546 Loss: nan\n",
            "Epoch: 547 Loss: nan\n",
            "Epoch: 548 Loss: nan\n",
            "Epoch: 549 Loss: nan\n",
            "Epoch: 550 Loss: nan\n",
            "Epoch: 551 Loss: nan\n",
            "Epoch: 552 Loss: nan\n",
            "Epoch: 553 Loss: nan\n",
            "Epoch: 554 Loss: nan\n",
            "Epoch: 555 Loss: nan\n",
            "Epoch: 556 Loss: nan\n",
            "Epoch: 557 Loss: nan\n",
            "Epoch: 558 Loss: nan\n",
            "Epoch: 559 Loss: nan\n",
            "Epoch: 560 Loss: nan\n",
            "Epoch: 561 Loss: nan\n",
            "Epoch: 562 Loss: nan\n",
            "Epoch: 563 Loss: nan\n",
            "Epoch: 564 Loss: nan\n",
            "Epoch: 565 Loss: nan\n",
            "Epoch: 566 Loss: nan\n",
            "Epoch: 567 Loss: nan\n",
            "Epoch: 568 Loss: nan\n",
            "Epoch: 569 Loss: nan\n",
            "Epoch: 570 Loss: nan\n",
            "Epoch: 571 Loss: nan\n",
            "Epoch: 572 Loss: nan\n",
            "Epoch: 573 Loss: nan\n",
            "Epoch: 574 Loss: nan\n",
            "Epoch: 575 Loss: nan\n",
            "Epoch: 576 Loss: nan\n",
            "Epoch: 577 Loss: nan\n",
            "Epoch: 578 Loss: nan\n",
            "Epoch: 579 Loss: nan\n",
            "Epoch: 580 Loss: nan\n",
            "Epoch: 581 Loss: nan\n",
            "Epoch: 582 Loss: nan\n",
            "Epoch: 583 Loss: nan\n",
            "Epoch: 584 Loss: nan\n",
            "Epoch: 585 Loss: nan\n",
            "Epoch: 586 Loss: nan\n",
            "Epoch: 587 Loss: nan\n",
            "Epoch: 588 Loss: nan\n",
            "Epoch: 589 Loss: nan\n",
            "Epoch: 590 Loss: nan\n",
            "Epoch: 591 Loss: nan\n",
            "Epoch: 592 Loss: nan\n",
            "Epoch: 593 Loss: nan\n",
            "Epoch: 594 Loss: nan\n",
            "Epoch: 595 Loss: nan\n",
            "Epoch: 596 Loss: nan\n",
            "Epoch: 597 Loss: nan\n",
            "Epoch: 598 Loss: nan\n",
            "Epoch: 599 Loss: nan\n",
            "Epoch: 600 Loss: nan\n",
            "Epoch: 601 Loss: nan\n",
            "Epoch: 602 Loss: nan\n",
            "Epoch: 603 Loss: nan\n",
            "Epoch: 604 Loss: nan\n",
            "Epoch: 605 Loss: nan\n",
            "Epoch: 606 Loss: nan\n",
            "Epoch: 607 Loss: nan\n",
            "Epoch: 608 Loss: nan\n",
            "Epoch: 609 Loss: nan\n",
            "Epoch: 610 Loss: nan\n",
            "Epoch: 611 Loss: nan\n",
            "Epoch: 612 Loss: nan\n",
            "Epoch: 613 Loss: nan\n",
            "Epoch: 614 Loss: nan\n",
            "Epoch: 615 Loss: nan\n",
            "Epoch: 616 Loss: nan\n",
            "Epoch: 617 Loss: nan\n",
            "Epoch: 618 Loss: nan\n",
            "Epoch: 619 Loss: nan\n",
            "Epoch: 620 Loss: nan\n",
            "Epoch: 621 Loss: nan\n",
            "Epoch: 622 Loss: nan\n",
            "Epoch: 623 Loss: nan\n",
            "Epoch: 624 Loss: nan\n",
            "Epoch: 625 Loss: nan\n",
            "Epoch: 626 Loss: nan\n",
            "Epoch: 627 Loss: nan\n",
            "Epoch: 628 Loss: nan\n",
            "Epoch: 629 Loss: nan\n",
            "Epoch: 630 Loss: nan\n",
            "Epoch: 631 Loss: nan\n",
            "Epoch: 632 Loss: nan\n",
            "Epoch: 633 Loss: nan\n",
            "Epoch: 634 Loss: nan\n",
            "Epoch: 635 Loss: nan\n",
            "Epoch: 636 Loss: nan\n",
            "Epoch: 637 Loss: nan\n",
            "Epoch: 638 Loss: nan\n",
            "Epoch: 639 Loss: nan\n",
            "Epoch: 640 Loss: nan\n",
            "Epoch: 641 Loss: nan\n",
            "Epoch: 642 Loss: nan\n",
            "Epoch: 643 Loss: nan\n",
            "Epoch: 644 Loss: nan\n",
            "Epoch: 645 Loss: nan\n",
            "Epoch: 646 Loss: nan\n",
            "Epoch: 647 Loss: nan\n",
            "Epoch: 648 Loss: nan\n",
            "Epoch: 649 Loss: nan\n",
            "Epoch: 650 Loss: nan\n",
            "Epoch: 651 Loss: nan\n",
            "Epoch: 652 Loss: nan\n",
            "Epoch: 653 Loss: nan\n",
            "Epoch: 654 Loss: nan\n",
            "Epoch: 655 Loss: nan\n",
            "Epoch: 656 Loss: nan\n",
            "Epoch: 657 Loss: nan\n",
            "Epoch: 658 Loss: nan\n",
            "Epoch: 659 Loss: nan\n",
            "Epoch: 660 Loss: nan\n",
            "Epoch: 661 Loss: nan\n",
            "Epoch: 662 Loss: nan\n",
            "Epoch: 663 Loss: nan\n",
            "Epoch: 664 Loss: nan\n",
            "Epoch: 665 Loss: nan\n",
            "Epoch: 666 Loss: nan\n",
            "Epoch: 667 Loss: nan\n",
            "Epoch: 668 Loss: nan\n",
            "Epoch: 669 Loss: nan\n",
            "Epoch: 670 Loss: nan\n",
            "Epoch: 671 Loss: nan\n",
            "Epoch: 672 Loss: nan\n",
            "Epoch: 673 Loss: nan\n",
            "Epoch: 674 Loss: nan\n",
            "Epoch: 675 Loss: nan\n",
            "Epoch: 676 Loss: nan\n",
            "Epoch: 677 Loss: nan\n",
            "Epoch: 678 Loss: nan\n",
            "Epoch: 679 Loss: nan\n",
            "Epoch: 680 Loss: nan\n",
            "Epoch: 681 Loss: nan\n",
            "Epoch: 682 Loss: nan\n",
            "Epoch: 683 Loss: nan\n",
            "Epoch: 684 Loss: nan\n",
            "Epoch: 685 Loss: nan\n",
            "Epoch: 686 Loss: nan\n",
            "Epoch: 687 Loss: nan\n",
            "Epoch: 688 Loss: nan\n",
            "Epoch: 689 Loss: nan\n",
            "Epoch: 690 Loss: nan\n",
            "Epoch: 691 Loss: nan\n",
            "Epoch: 692 Loss: nan\n",
            "Epoch: 693 Loss: nan\n",
            "Epoch: 694 Loss: nan\n",
            "Epoch: 695 Loss: nan\n",
            "Epoch: 696 Loss: nan\n",
            "Epoch: 697 Loss: nan\n",
            "Epoch: 698 Loss: nan\n",
            "Epoch: 699 Loss: nan\n",
            "Epoch: 700 Loss: nan\n",
            "Epoch: 701 Loss: nan\n",
            "Epoch: 702 Loss: nan\n",
            "Epoch: 703 Loss: nan\n",
            "Epoch: 704 Loss: nan\n",
            "Epoch: 705 Loss: nan\n",
            "Epoch: 706 Loss: nan\n",
            "Epoch: 707 Loss: nan\n",
            "Epoch: 708 Loss: nan\n",
            "Epoch: 709 Loss: nan\n",
            "Epoch: 710 Loss: nan\n",
            "Epoch: 711 Loss: nan\n",
            "Epoch: 712 Loss: nan\n",
            "Epoch: 713 Loss: nan\n",
            "Epoch: 714 Loss: nan\n",
            "Epoch: 715 Loss: nan\n",
            "Epoch: 716 Loss: nan\n",
            "Epoch: 717 Loss: nan\n",
            "Epoch: 718 Loss: nan\n",
            "Epoch: 719 Loss: nan\n",
            "Epoch: 720 Loss: nan\n",
            "Epoch: 721 Loss: nan\n",
            "Epoch: 722 Loss: nan\n",
            "Epoch: 723 Loss: nan\n",
            "Epoch: 724 Loss: nan\n",
            "Epoch: 725 Loss: nan\n",
            "Epoch: 726 Loss: nan\n",
            "Epoch: 727 Loss: nan\n",
            "Epoch: 728 Loss: nan\n",
            "Epoch: 729 Loss: nan\n",
            "Epoch: 730 Loss: nan\n",
            "Epoch: 731 Loss: nan\n",
            "Epoch: 732 Loss: nan\n",
            "Epoch: 733 Loss: nan\n",
            "Epoch: 734 Loss: nan\n",
            "Epoch: 735 Loss: nan\n",
            "Epoch: 736 Loss: nan\n",
            "Epoch: 737 Loss: nan\n",
            "Epoch: 738 Loss: nan\n",
            "Epoch: 739 Loss: nan\n",
            "Epoch: 740 Loss: nan\n",
            "Epoch: 741 Loss: nan\n",
            "Epoch: 742 Loss: nan\n",
            "Epoch: 743 Loss: nan\n",
            "Epoch: 744 Loss: nan\n",
            "Epoch: 745 Loss: nan\n",
            "Epoch: 746 Loss: nan\n",
            "Epoch: 747 Loss: nan\n",
            "Epoch: 748 Loss: nan\n",
            "Epoch: 749 Loss: nan\n",
            "Epoch: 750 Loss: nan\n",
            "Epoch: 751 Loss: nan\n",
            "Epoch: 752 Loss: nan\n",
            "Epoch: 753 Loss: nan\n",
            "Epoch: 754 Loss: nan\n",
            "Epoch: 755 Loss: nan\n",
            "Epoch: 756 Loss: nan\n",
            "Epoch: 757 Loss: nan\n",
            "Epoch: 758 Loss: nan\n",
            "Epoch: 759 Loss: nan\n",
            "Epoch: 760 Loss: nan\n",
            "Epoch: 761 Loss: nan\n",
            "Epoch: 762 Loss: nan\n",
            "Epoch: 763 Loss: nan\n",
            "Epoch: 764 Loss: nan\n",
            "Epoch: 765 Loss: nan\n",
            "Epoch: 766 Loss: nan\n",
            "Epoch: 767 Loss: nan\n",
            "Epoch: 768 Loss: nan\n",
            "Epoch: 769 Loss: nan\n",
            "Epoch: 770 Loss: nan\n",
            "Epoch: 771 Loss: nan\n",
            "Epoch: 772 Loss: nan\n",
            "Epoch: 773 Loss: nan\n",
            "Epoch: 774 Loss: nan\n",
            "Epoch: 775 Loss: nan\n",
            "Epoch: 776 Loss: nan\n",
            "Epoch: 777 Loss: nan\n",
            "Epoch: 778 Loss: nan\n",
            "Epoch: 779 Loss: nan\n",
            "Epoch: 780 Loss: nan\n",
            "Epoch: 781 Loss: nan\n",
            "Epoch: 782 Loss: nan\n",
            "Epoch: 783 Loss: nan\n",
            "Epoch: 784 Loss: nan\n",
            "Epoch: 785 Loss: nan\n",
            "Epoch: 786 Loss: nan\n",
            "Epoch: 787 Loss: nan\n",
            "Epoch: 788 Loss: nan\n",
            "Epoch: 789 Loss: nan\n",
            "Epoch: 790 Loss: nan\n",
            "Epoch: 791 Loss: nan\n",
            "Epoch: 792 Loss: nan\n",
            "Epoch: 793 Loss: nan\n",
            "Epoch: 794 Loss: nan\n",
            "Epoch: 795 Loss: nan\n",
            "Epoch: 796 Loss: nan\n",
            "Epoch: 797 Loss: nan\n",
            "Epoch: 798 Loss: nan\n",
            "Epoch: 799 Loss: nan\n",
            "Epoch: 800 Loss: nan\n",
            "Epoch: 801 Loss: nan\n",
            "Epoch: 802 Loss: nan\n",
            "Epoch: 803 Loss: nan\n",
            "Epoch: 804 Loss: nan\n",
            "Epoch: 805 Loss: nan\n",
            "Epoch: 806 Loss: nan\n",
            "Epoch: 807 Loss: nan\n",
            "Epoch: 808 Loss: nan\n",
            "Epoch: 809 Loss: nan\n",
            "Epoch: 810 Loss: nan\n",
            "Epoch: 811 Loss: nan\n",
            "Epoch: 812 Loss: nan\n",
            "Epoch: 813 Loss: nan\n",
            "Epoch: 814 Loss: nan\n",
            "Epoch: 815 Loss: nan\n",
            "Epoch: 816 Loss: nan\n",
            "Epoch: 817 Loss: nan\n",
            "Epoch: 818 Loss: nan\n",
            "Epoch: 819 Loss: nan\n",
            "Epoch: 820 Loss: nan\n",
            "Epoch: 821 Loss: nan\n",
            "Epoch: 822 Loss: nan\n",
            "Epoch: 823 Loss: nan\n",
            "Epoch: 824 Loss: nan\n",
            "Epoch: 825 Loss: nan\n",
            "Epoch: 826 Loss: nan\n",
            "Epoch: 827 Loss: nan\n",
            "Epoch: 828 Loss: nan\n",
            "Epoch: 829 Loss: nan\n",
            "Epoch: 830 Loss: nan\n",
            "Epoch: 831 Loss: nan\n",
            "Epoch: 832 Loss: nan\n",
            "Epoch: 833 Loss: nan\n",
            "Epoch: 834 Loss: nan\n",
            "Epoch: 835 Loss: nan\n",
            "Epoch: 836 Loss: nan\n",
            "Epoch: 837 Loss: nan\n",
            "Epoch: 838 Loss: nan\n",
            "Epoch: 839 Loss: nan\n",
            "Epoch: 840 Loss: nan\n",
            "Epoch: 841 Loss: nan\n",
            "Epoch: 842 Loss: nan\n",
            "Epoch: 843 Loss: nan\n",
            "Epoch: 844 Loss: nan\n",
            "Epoch: 845 Loss: nan\n",
            "Epoch: 846 Loss: nan\n",
            "Epoch: 847 Loss: nan\n",
            "Epoch: 848 Loss: nan\n",
            "Epoch: 849 Loss: nan\n",
            "Epoch: 850 Loss: nan\n",
            "Epoch: 851 Loss: nan\n",
            "Epoch: 852 Loss: nan\n",
            "Epoch: 853 Loss: nan\n",
            "Epoch: 854 Loss: nan\n",
            "Epoch: 855 Loss: nan\n",
            "Epoch: 856 Loss: nan\n",
            "Epoch: 857 Loss: nan\n",
            "Epoch: 858 Loss: nan\n",
            "Epoch: 859 Loss: nan\n",
            "Epoch: 860 Loss: nan\n",
            "Epoch: 861 Loss: nan\n",
            "Epoch: 862 Loss: nan\n",
            "Epoch: 863 Loss: nan\n",
            "Epoch: 864 Loss: nan\n",
            "Epoch: 865 Loss: nan\n",
            "Epoch: 866 Loss: nan\n",
            "Epoch: 867 Loss: nan\n",
            "Epoch: 868 Loss: nan\n",
            "Epoch: 869 Loss: nan\n",
            "Epoch: 870 Loss: nan\n",
            "Epoch: 871 Loss: nan\n",
            "Epoch: 872 Loss: nan\n",
            "Epoch: 873 Loss: nan\n",
            "Epoch: 874 Loss: nan\n",
            "Epoch: 875 Loss: nan\n",
            "Epoch: 876 Loss: nan\n",
            "Epoch: 877 Loss: nan\n",
            "Epoch: 878 Loss: nan\n",
            "Epoch: 879 Loss: nan\n",
            "Epoch: 880 Loss: nan\n",
            "Epoch: 881 Loss: nan\n",
            "Epoch: 882 Loss: nan\n",
            "Epoch: 883 Loss: nan\n",
            "Epoch: 884 Loss: nan\n",
            "Epoch: 885 Loss: nan\n",
            "Epoch: 886 Loss: nan\n",
            "Epoch: 887 Loss: nan\n",
            "Epoch: 888 Loss: nan\n",
            "Epoch: 889 Loss: nan\n",
            "Epoch: 890 Loss: nan\n",
            "Epoch: 891 Loss: nan\n",
            "Epoch: 892 Loss: nan\n",
            "Epoch: 893 Loss: nan\n",
            "Epoch: 894 Loss: nan\n",
            "Epoch: 895 Loss: nan\n",
            "Epoch: 896 Loss: nan\n",
            "Epoch: 897 Loss: nan\n",
            "Epoch: 898 Loss: nan\n",
            "Epoch: 899 Loss: nan\n",
            "Epoch: 900 Loss: nan\n",
            "Epoch: 901 Loss: nan\n",
            "Epoch: 902 Loss: nan\n",
            "Epoch: 903 Loss: nan\n",
            "Epoch: 904 Loss: nan\n",
            "Epoch: 905 Loss: nan\n",
            "Epoch: 906 Loss: nan\n",
            "Epoch: 907 Loss: nan\n",
            "Epoch: 908 Loss: nan\n",
            "Epoch: 909 Loss: nan\n",
            "Epoch: 910 Loss: nan\n",
            "Epoch: 911 Loss: nan\n",
            "Epoch: 912 Loss: nan\n",
            "Epoch: 913 Loss: nan\n",
            "Epoch: 914 Loss: nan\n",
            "Epoch: 915 Loss: nan\n",
            "Epoch: 916 Loss: nan\n",
            "Epoch: 917 Loss: nan\n",
            "Epoch: 918 Loss: nan\n",
            "Epoch: 919 Loss: nan\n",
            "Epoch: 920 Loss: nan\n",
            "Epoch: 921 Loss: nan\n",
            "Epoch: 922 Loss: nan\n",
            "Epoch: 923 Loss: nan\n",
            "Epoch: 924 Loss: nan\n",
            "Epoch: 925 Loss: nan\n",
            "Epoch: 926 Loss: nan\n",
            "Epoch: 927 Loss: nan\n",
            "Epoch: 928 Loss: nan\n",
            "Epoch: 929 Loss: nan\n",
            "Epoch: 930 Loss: nan\n",
            "Epoch: 931 Loss: nan\n",
            "Epoch: 932 Loss: nan\n",
            "Epoch: 933 Loss: nan\n",
            "Epoch: 934 Loss: nan\n",
            "Epoch: 935 Loss: nan\n",
            "Epoch: 936 Loss: nan\n",
            "Epoch: 937 Loss: nan\n",
            "Epoch: 938 Loss: nan\n",
            "Epoch: 939 Loss: nan\n",
            "Epoch: 940 Loss: nan\n",
            "Epoch: 941 Loss: nan\n",
            "Epoch: 942 Loss: nan\n",
            "Epoch: 943 Loss: nan\n",
            "Epoch: 944 Loss: nan\n",
            "Epoch: 945 Loss: nan\n",
            "Epoch: 946 Loss: nan\n",
            "Epoch: 947 Loss: nan\n",
            "Epoch: 948 Loss: nan\n",
            "Epoch: 949 Loss: nan\n",
            "Epoch: 950 Loss: nan\n",
            "Epoch: 951 Loss: nan\n",
            "Epoch: 952 Loss: nan\n",
            "Epoch: 953 Loss: nan\n",
            "Epoch: 954 Loss: nan\n",
            "Epoch: 955 Loss: nan\n",
            "Epoch: 956 Loss: nan\n",
            "Epoch: 957 Loss: nan\n",
            "Epoch: 958 Loss: nan\n",
            "Epoch: 959 Loss: nan\n",
            "Epoch: 960 Loss: nan\n",
            "Epoch: 961 Loss: nan\n",
            "Epoch: 962 Loss: nan\n",
            "Epoch: 963 Loss: nan\n",
            "Epoch: 964 Loss: nan\n",
            "Epoch: 965 Loss: nan\n",
            "Epoch: 966 Loss: nan\n",
            "Epoch: 967 Loss: nan\n",
            "Epoch: 968 Loss: nan\n",
            "Epoch: 969 Loss: nan\n",
            "Epoch: 970 Loss: nan\n",
            "Epoch: 971 Loss: nan\n",
            "Epoch: 972 Loss: nan\n",
            "Epoch: 973 Loss: nan\n",
            "Epoch: 974 Loss: nan\n",
            "Epoch: 975 Loss: nan\n",
            "Epoch: 976 Loss: nan\n",
            "Epoch: 977 Loss: nan\n",
            "Epoch: 978 Loss: nan\n",
            "Epoch: 979 Loss: nan\n",
            "Epoch: 980 Loss: nan\n",
            "Epoch: 981 Loss: nan\n",
            "Epoch: 982 Loss: nan\n",
            "Epoch: 983 Loss: nan\n",
            "Epoch: 984 Loss: nan\n",
            "Epoch: 985 Loss: nan\n",
            "Epoch: 986 Loss: nan\n",
            "Epoch: 987 Loss: nan\n",
            "Epoch: 988 Loss: nan\n",
            "Epoch: 989 Loss: nan\n",
            "Epoch: 990 Loss: nan\n",
            "Epoch: 991 Loss: nan\n",
            "Epoch: 992 Loss: nan\n",
            "Epoch: 993 Loss: nan\n",
            "Epoch: 994 Loss: nan\n",
            "Epoch: 995 Loss: nan\n",
            "Epoch: 996 Loss: nan\n",
            "Epoch: 997 Loss: nan\n",
            "Epoch: 998 Loss: nan\n",
            "Epoch: 999 Loss: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foK6U9PJJpNM"
      },
      "source": [
        "**Note** : The observation will be we will get the nan values as the graients explodes\n",
        "<br>\n",
        "<br>\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llJ-9WlHJpNM"
      },
      "source": [
        "**Vanishing** : for this we make all the activations as sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAOpyzJgJpNN"
      },
      "source": [
        "#D_in is input dimension\n",
        "#H1 is dimension of first hidden layer \n",
        "#H2 is dimension of second hidden layer\n",
        "#D_out is output dimension.\n",
        "D_in, H1, H2,H3,H4,H5,H6, D_out = inputsize, 3, 5 ,10, 5,5,5,1                              #You can add more layers if you wish to \n",
        "\n",
        "neurons = [D_in, H1, H2,H3,H4,H5,H6, D_out] # list of number of neurons in the layers sequentially.\n",
        "activation_functions = ['sigmoid','sigmoid', 'sigmoid','sigmoid', 'sigmoid','sigmoid','sigmoid'] #activations in each layer (Note: the input layer does not have any activation)\n",
        "my_neuralnet4 = Neural_Network(neurons, activation_functions )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJrtB_YkJpNP",
        "outputId": "9051e800-7995-428a-8630-84efd29c8c22"
      },
      "source": [
        "loss4 = my_neuralnet4.train(train_X,train_Y,minibatch=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0 Loss: 0.19497427287834132\n",
            "Epoch: 1 Loss: 0.19019423359505733\n",
            "Epoch: 2 Loss: 0.18496598693085303\n",
            "Epoch: 3 Loss: 0.17932771246277804\n",
            "Epoch: 4 Loss: 0.17336227304875898\n",
            "Epoch: 5 Loss: 0.16720368220194815\n",
            "Epoch: 6 Loss: 0.16103348095878517\n",
            "Epoch: 7 Loss: 0.15506286930313212\n",
            "Epoch: 8 Loss: 0.14950174872529481\n",
            "Epoch: 9 Loss: 0.14452318981898413\n",
            "Epoch: 10 Loss: 0.14023603782785232\n",
            "Epoch: 11 Loss: 0.13667522726663808\n",
            "Epoch: 12 Loss: 0.13381065795538485\n",
            "Epoch: 13 Loss: 0.13156762881757714\n",
            "Epoch: 14 Loss: 0.12984953410149364\n",
            "Epoch: 15 Loss: 0.12855622768646843\n",
            "Epoch: 16 Loss: 0.12759566304890077\n",
            "Epoch: 17 Loss: 0.1268894444010713\n",
            "Epoch: 18 Loss: 0.12637414751908999\n",
            "Epoch: 19 Loss: 0.12600026048494473\n",
            "Epoch: 20 Loss: 0.1257300924636059\n",
            "Epoch: 21 Loss: 0.1255354593822715\n",
            "Epoch: 22 Loss: 0.1253955526494702\n",
            "Epoch: 23 Loss: 0.1252951481563989\n",
            "Epoch: 24 Loss: 0.12522317979562517\n",
            "Epoch: 25 Loss: 0.12517164125390717\n",
            "Epoch: 26 Loss: 0.12513475933534746\n",
            "Epoch: 27 Loss: 0.1251083811199507\n",
            "Epoch: 28 Loss: 0.12508952442114296\n",
            "Epoch: 29 Loss: 0.1250760504566504\n",
            "Epoch: 30 Loss: 0.12506642674509347\n",
            "Epoch: 31 Loss: 0.12505955599065965\n",
            "Epoch: 32 Loss: 0.12505465291463133\n",
            "Epoch: 33 Loss: 0.1250511557652514\n",
            "Epoch: 34 Loss: 0.12504866282783067\n",
            "Epoch: 35 Loss: 0.12504688691636942\n",
            "Epoch: 36 Loss: 0.1250456227771198\n",
            "Epoch: 37 Loss: 0.1250447237528244\n",
            "Epoch: 38 Loss: 0.12504408508321202\n",
            "Epoch: 39 Loss: 0.12504363195812845\n",
            "Epoch: 40 Loss: 0.12504331097277394\n",
            "Epoch: 41 Loss: 0.12504308401746514\n",
            "Epoch: 42 Loss: 0.1250429239090742\n",
            "Epoch: 43 Loss: 0.1250428112682218\n",
            "Epoch: 44 Loss: 0.12504273228735951\n",
            "Epoch: 45 Loss: 0.12504267713586906\n",
            "Epoch: 46 Loss: 0.12504263882058955\n",
            "Epoch: 47 Loss: 0.1250426123719027\n",
            "Epoch: 48 Loss: 0.12504259426250852\n",
            "Epoch: 49 Loss: 0.12504258199248933\n",
            "Epoch: 50 Loss: 0.125042573793189\n",
            "Epoch: 51 Loss: 0.12504256841597097\n",
            "Epoch: 52 Loss: 0.12504256498159783\n",
            "Epoch: 53 Loss: 0.12504256287289683\n",
            "Epoch: 54 Loss: 0.12504256165832353\n",
            "Epoch: 55 Loss: 0.1250425610375723\n",
            "Epoch: 56 Loss: 0.12504256080291268\n",
            "Epoch: 57 Loss: 0.12504256081173545\n",
            "Epoch: 58 Loss: 0.12504256096708627\n",
            "Epoch: 59 Loss: 0.12504256120388482\n",
            "Epoch: 60 Loss: 0.12504256147918952\n",
            "Epoch: 61 Loss: 0.1250425617653358\n",
            "Epoch: 62 Loss: 0.1250425620451152\n",
            "Epoch: 63 Loss: 0.12504256230839939\n",
            "Epoch: 64 Loss: 0.12504256254978757\n",
            "Epoch: 65 Loss: 0.12504256276697553\n",
            "Epoch: 66 Loss: 0.12504256295963326\n",
            "Epoch: 67 Loss: 0.12504256312863926\n",
            "Epoch: 68 Loss: 0.12504256327556446\n",
            "Epoch: 69 Loss: 0.12504256340232936\n",
            "Epoch: 70 Loss: 0.1250425635109817\n",
            "Epoch: 71 Loss: 0.12504256360355598\n",
            "Epoch: 72 Loss: 0.12504256368198935\n",
            "Epoch: 73 Loss: 0.12504256374807501\n",
            "Epoch: 74 Loss: 0.12504256380344014\n",
            "Epoch: 75 Loss: 0.12504256384954016\n",
            "Epoch: 76 Loss: 0.12504256388766247\n",
            "Epoch: 77 Loss: 0.1250425639189364\n",
            "Epoch: 78 Loss: 0.1250425639443459\n",
            "Epoch: 79 Loss: 0.125042563964744\n",
            "Epoch: 80 Loss: 0.1250425639808667\n",
            "Epoch: 81 Loss: 0.12504256399334715\n",
            "Epoch: 82 Loss: 0.12504256400272837\n",
            "Epoch: 83 Loss: 0.125042564009475\n",
            "Epoch: 84 Loss: 0.12504256401398384\n",
            "Epoch: 85 Loss: 0.12504256401659325\n",
            "Epoch: 86 Loss: 0.12504256401759134\n",
            "Epoch: 87 Loss: 0.12504256401722322\n",
            "Epoch: 88 Loss: 0.12504256401569713\n",
            "Epoch: 89 Loss: 0.12504256401319\n",
            "Epoch: 90 Loss: 0.12504256400985184\n",
            "Epoch: 91 Loss: 0.12504256400581\n",
            "Epoch: 92 Loss: 0.1250425640011724\n",
            "Epoch: 93 Loss: 0.12504256399603045\n",
            "Epoch: 94 Loss: 0.12504256399046168\n",
            "Epoch: 95 Loss: 0.1250425639845317\n",
            "Epoch: 96 Loss: 0.12504256397829608\n",
            "Epoch: 97 Loss: 0.12504256397180186\n",
            "Epoch: 98 Loss: 0.1250425639650888\n",
            "Epoch: 99 Loss: 0.12504256395819072\n",
            "Epoch: 100 Loss: 0.125042563951136\n",
            "Epoch: 101 Loss: 0.12504256394394886\n",
            "Epoch: 102 Loss: 0.1250425639366497\n",
            "Epoch: 103 Loss: 0.12504256392925572\n",
            "Epoch: 104 Loss: 0.12504256392178165\n",
            "Epoch: 105 Loss: 0.12504256391423973\n",
            "Epoch: 106 Loss: 0.12504256390664048\n",
            "Epoch: 107 Loss: 0.12504256389899274\n",
            "Epoch: 108 Loss: 0.125042563891304\n",
            "Epoch: 109 Loss: 0.12504256388358054\n",
            "Epoch: 110 Loss: 0.12504256387582774\n",
            "Epoch: 111 Loss: 0.1250425638680501\n",
            "Epoch: 112 Loss: 0.1250425638602515\n",
            "Epoch: 113 Loss: 0.12504256385243515\n",
            "Epoch: 114 Loss: 0.1250425638446038\n",
            "Epoch: 115 Loss: 0.1250425638367597\n",
            "Epoch: 116 Loss: 0.1250425638289049\n",
            "Epoch: 117 Loss: 0.12504256382104104\n",
            "Epoch: 118 Loss: 0.12504256381316944\n",
            "Epoch: 119 Loss: 0.12504256380529136\n",
            "Epoch: 120 Loss: 0.1250425637974078\n",
            "Epoch: 121 Loss: 0.12504256378951956\n",
            "Epoch: 122 Loss: 0.1250425637816274\n",
            "Epoch: 123 Loss: 0.12504256377373194\n",
            "Epoch: 124 Loss: 0.12504256376583361\n",
            "Epoch: 125 Loss: 0.1250425637579329\n",
            "Epoch: 126 Loss: 0.1250425637500302\n",
            "Epoch: 127 Loss: 0.1250425637421258\n",
            "Epoch: 128 Loss: 0.12504256373421993\n",
            "Epoch: 129 Loss: 0.12504256372631284\n",
            "Epoch: 130 Loss: 0.12504256371840472\n",
            "Epoch: 131 Loss: 0.12504256371049574\n",
            "Epoch: 132 Loss: 0.12504256370258598\n",
            "Epoch: 133 Loss: 0.12504256369467565\n",
            "Epoch: 134 Loss: 0.12504256368676472\n",
            "Epoch: 135 Loss: 0.12504256367885336\n",
            "Epoch: 136 Loss: 0.12504256367094166\n",
            "Epoch: 137 Loss: 0.12504256366302957\n",
            "Epoch: 138 Loss: 0.12504256365511723\n",
            "Epoch: 139 Loss: 0.12504256364720467\n",
            "Epoch: 140 Loss: 0.1250425636392919\n",
            "Epoch: 141 Loss: 0.12504256363137894\n",
            "Epoch: 142 Loss: 0.12504256362346583\n",
            "Epoch: 143 Loss: 0.12504256361555263\n",
            "Epoch: 144 Loss: 0.1250425636076393\n",
            "Epoch: 145 Loss: 0.12504256359972588\n",
            "Epoch: 146 Loss: 0.1250425635918124\n",
            "Epoch: 147 Loss: 0.1250425635838988\n",
            "Epoch: 148 Loss: 0.1250425635759852\n",
            "Epoch: 149 Loss: 0.12504256356807153\n",
            "Epoch: 150 Loss: 0.12504256356015783\n",
            "Epoch: 151 Loss: 0.12504256355224405\n",
            "Epoch: 152 Loss: 0.12504256354433027\n",
            "Epoch: 153 Loss: 0.12504256353641646\n",
            "Epoch: 154 Loss: 0.12504256352850263\n",
            "Epoch: 155 Loss: 0.12504256352058873\n",
            "Epoch: 156 Loss: 0.12504256351267487\n",
            "Epoch: 157 Loss: 0.12504256350476095\n",
            "Epoch: 158 Loss: 0.125042563496847\n",
            "Epoch: 159 Loss: 0.12504256348893306\n",
            "Epoch: 160 Loss: 0.1250425634810191\n",
            "Epoch: 161 Loss: 0.12504256347310513\n",
            "Epoch: 162 Loss: 0.12504256346519116\n",
            "Epoch: 163 Loss: 0.12504256345727713\n",
            "Epoch: 164 Loss: 0.12504256344936313\n",
            "Epoch: 165 Loss: 0.12504256344144912\n",
            "Epoch: 166 Loss: 0.1250425634335351\n",
            "Epoch: 167 Loss: 0.12504256342562106\n",
            "Epoch: 168 Loss: 0.125042563417707\n",
            "Epoch: 169 Loss: 0.12504256340979295\n",
            "Epoch: 170 Loss: 0.12504256340187886\n",
            "Epoch: 171 Loss: 0.1250425633939648\n",
            "Epoch: 172 Loss: 0.1250425633860507\n",
            "Epoch: 173 Loss: 0.12504256337813657\n",
            "Epoch: 174 Loss: 0.1250425633702225\n",
            "Epoch: 175 Loss: 0.12504256336230835\n",
            "Epoch: 176 Loss: 0.12504256335439426\n",
            "Epoch: 177 Loss: 0.12504256334648012\n",
            "Epoch: 178 Loss: 0.12504256333856595\n",
            "Epoch: 179 Loss: 0.1250425633306518\n",
            "Epoch: 180 Loss: 0.12504256332273767\n",
            "Epoch: 181 Loss: 0.1250425633148235\n",
            "Epoch: 182 Loss: 0.1250425633069093\n",
            "Epoch: 183 Loss: 0.12504256329899513\n",
            "Epoch: 184 Loss: 0.1250425632910809\n",
            "Epoch: 185 Loss: 0.1250425632831667\n",
            "Epoch: 186 Loss: 0.12504256327525248\n",
            "Epoch: 187 Loss: 0.12504256326733826\n",
            "Epoch: 188 Loss: 0.12504256325942406\n",
            "Epoch: 189 Loss: 0.1250425632515098\n",
            "Epoch: 190 Loss: 0.12504256324359558\n",
            "Epoch: 191 Loss: 0.1250425632356813\n",
            "Epoch: 192 Loss: 0.12504256322776708\n",
            "Epoch: 193 Loss: 0.12504256321985277\n",
            "Epoch: 194 Loss: 0.12504256321193852\n",
            "Epoch: 195 Loss: 0.1250425632040242\n",
            "Epoch: 196 Loss: 0.12504256319610993\n",
            "Epoch: 197 Loss: 0.12504256318819562\n",
            "Epoch: 198 Loss: 0.1250425631802813\n",
            "Epoch: 199 Loss: 0.125042563172367\n",
            "Epoch: 200 Loss: 0.12504256316445267\n",
            "Epoch: 201 Loss: 0.12504256315653833\n",
            "Epoch: 202 Loss: 0.12504256314862397\n",
            "Epoch: 203 Loss: 0.12504256314070963\n",
            "Epoch: 204 Loss: 0.12504256313279527\n",
            "Epoch: 205 Loss: 0.1250425631248809\n",
            "Epoch: 206 Loss: 0.12504256311696654\n",
            "Epoch: 207 Loss: 0.12504256310905215\n",
            "Epoch: 208 Loss: 0.12504256310113773\n",
            "Epoch: 209 Loss: 0.12504256309322334\n",
            "Epoch: 210 Loss: 0.12504256308530892\n",
            "Epoch: 211 Loss: 0.1250425630773945\n",
            "Epoch: 212 Loss: 0.12504256306948008\n",
            "Epoch: 213 Loss: 0.12504256306156564\n",
            "Epoch: 214 Loss: 0.12504256305365122\n",
            "Epoch: 215 Loss: 0.12504256304573674\n",
            "Epoch: 216 Loss: 0.1250425630378223\n",
            "Epoch: 217 Loss: 0.12504256302990782\n",
            "Epoch: 218 Loss: 0.12504256302199335\n",
            "Epoch: 219 Loss: 0.12504256301407884\n",
            "Epoch: 220 Loss: 0.12504256300616437\n",
            "Epoch: 221 Loss: 0.12504256299824984\n",
            "Epoch: 222 Loss: 0.12504256299033534\n",
            "Epoch: 223 Loss: 0.12504256298242084\n",
            "Epoch: 224 Loss: 0.1250425629745063\n",
            "Epoch: 225 Loss: 0.12504256296659175\n",
            "Epoch: 226 Loss: 0.12504256295867722\n",
            "Epoch: 227 Loss: 0.12504256295076266\n",
            "Epoch: 228 Loss: 0.1250425629428481\n",
            "Epoch: 229 Loss: 0.12504256293493352\n",
            "Epoch: 230 Loss: 0.12504256292701896\n",
            "Epoch: 231 Loss: 0.12504256291910437\n",
            "Epoch: 232 Loss: 0.1250425629111898\n",
            "Epoch: 233 Loss: 0.12504256290327517\n",
            "Epoch: 234 Loss: 0.12504256289536056\n",
            "Epoch: 235 Loss: 0.12504256288744595\n",
            "Epoch: 236 Loss: 0.1250425628795313\n",
            "Epoch: 237 Loss: 0.12504256287161666\n",
            "Epoch: 238 Loss: 0.12504256286370202\n",
            "Epoch: 239 Loss: 0.12504256285578738\n",
            "Epoch: 240 Loss: 0.1250425628478727\n",
            "Epoch: 241 Loss: 0.12504256283995804\n",
            "Epoch: 242 Loss: 0.12504256283204337\n",
            "Epoch: 243 Loss: 0.1250425628241287\n",
            "Epoch: 244 Loss: 0.125042562816214\n",
            "Epoch: 245 Loss: 0.12504256280829928\n",
            "Epoch: 246 Loss: 0.1250425628003846\n",
            "Epoch: 247 Loss: 0.12504256279246986\n",
            "Epoch: 248 Loss: 0.12504256278455514\n",
            "Epoch: 249 Loss: 0.12504256277664041\n",
            "Epoch: 250 Loss: 0.12504256276872566\n",
            "Epoch: 251 Loss: 0.12504256276081088\n",
            "Epoch: 252 Loss: 0.12504256275289616\n",
            "Epoch: 253 Loss: 0.12504256274498138\n",
            "Epoch: 254 Loss: 0.1250425627370666\n",
            "Epoch: 255 Loss: 0.12504256272915182\n",
            "Epoch: 256 Loss: 0.125042562721237\n",
            "Epoch: 257 Loss: 0.1250425627133222\n",
            "Epoch: 258 Loss: 0.1250425627054074\n",
            "Epoch: 259 Loss: 0.1250425626974926\n",
            "Epoch: 260 Loss: 0.12504256268957775\n",
            "Epoch: 261 Loss: 0.12504256268166292\n",
            "Epoch: 262 Loss: 0.12504256267374808\n",
            "Epoch: 263 Loss: 0.12504256266583322\n",
            "Epoch: 264 Loss: 0.12504256265791838\n",
            "Epoch: 265 Loss: 0.1250425626500035\n",
            "Epoch: 266 Loss: 0.12504256264208863\n",
            "Epoch: 267 Loss: 0.12504256263417374\n",
            "Epoch: 268 Loss: 0.12504256262625885\n",
            "Epoch: 269 Loss: 0.12504256261834393\n",
            "Epoch: 270 Loss: 0.12504256261042904\n",
            "Epoch: 271 Loss: 0.12504256260251412\n",
            "Epoch: 272 Loss: 0.1250425625945992\n",
            "Epoch: 273 Loss: 0.12504256258668425\n",
            "Epoch: 274 Loss: 0.12504256257876933\n",
            "Epoch: 275 Loss: 0.12504256257085436\n",
            "Epoch: 276 Loss: 0.12504256256293939\n",
            "Epoch: 277 Loss: 0.12504256255502444\n",
            "Epoch: 278 Loss: 0.12504256254710946\n",
            "Epoch: 279 Loss: 0.12504256253919446\n",
            "Epoch: 280 Loss: 0.12504256253127946\n",
            "Epoch: 281 Loss: 0.1250425625233645\n",
            "Epoch: 282 Loss: 0.12504256251544948\n",
            "Epoch: 283 Loss: 0.12504256250753445\n",
            "Epoch: 284 Loss: 0.1250425624996194\n",
            "Epoch: 285 Loss: 0.12504256249170437\n",
            "Epoch: 286 Loss: 0.12504256248378934\n",
            "Epoch: 287 Loss: 0.12504256247587428\n",
            "Epoch: 288 Loss: 0.12504256246795922\n",
            "Epoch: 289 Loss: 0.12504256246004417\n",
            "Epoch: 290 Loss: 0.12504256245212908\n",
            "Epoch: 291 Loss: 0.12504256244421402\n",
            "Epoch: 292 Loss: 0.1250425624362989\n",
            "Epoch: 293 Loss: 0.1250425624283838\n",
            "Epoch: 294 Loss: 0.1250425624204687\n",
            "Epoch: 295 Loss: 0.12504256241255357\n",
            "Epoch: 296 Loss: 0.12504256240463846\n",
            "Epoch: 297 Loss: 0.12504256239672332\n",
            "Epoch: 298 Loss: 0.1250425623888082\n",
            "Epoch: 299 Loss: 0.12504256238089304\n",
            "Epoch: 300 Loss: 0.12504256237297787\n",
            "Epoch: 301 Loss: 0.1250425623650627\n",
            "Epoch: 302 Loss: 0.12504256235714753\n",
            "Epoch: 303 Loss: 0.12504256234923233\n",
            "Epoch: 304 Loss: 0.12504256234131714\n",
            "Epoch: 305 Loss: 0.12504256233340194\n",
            "Epoch: 306 Loss: 0.12504256232548674\n",
            "Epoch: 307 Loss: 0.12504256231757152\n",
            "Epoch: 308 Loss: 0.1250425623096563\n",
            "Epoch: 309 Loss: 0.12504256230174104\n",
            "Epoch: 310 Loss: 0.1250425622938258\n",
            "Epoch: 311 Loss: 0.12504256228591057\n",
            "Epoch: 312 Loss: 0.1250425622779953\n",
            "Epoch: 313 Loss: 0.12504256227008004\n",
            "Epoch: 314 Loss: 0.12504256226216476\n",
            "Epoch: 315 Loss: 0.12504256225424948\n",
            "Epoch: 316 Loss: 0.1250425622463342\n",
            "Epoch: 317 Loss: 0.12504256223841886\n",
            "Epoch: 318 Loss: 0.12504256223050356\n",
            "Epoch: 319 Loss: 0.12504256222258825\n",
            "Epoch: 320 Loss: 0.1250425622146729\n",
            "Epoch: 321 Loss: 0.1250425622067576\n",
            "Epoch: 322 Loss: 0.12504256219884224\n",
            "Epoch: 323 Loss: 0.12504256219092688\n",
            "Epoch: 324 Loss: 0.12504256218301152\n",
            "Epoch: 325 Loss: 0.12504256217509616\n",
            "Epoch: 326 Loss: 0.12504256216718077\n",
            "Epoch: 327 Loss: 0.1250425621592654\n",
            "Epoch: 328 Loss: 0.12504256215135\n",
            "Epoch: 329 Loss: 0.12504256214343462\n",
            "Epoch: 330 Loss: 0.1250425621355192\n",
            "Epoch: 331 Loss: 0.12504256212760378\n",
            "Epoch: 332 Loss: 0.12504256211968834\n",
            "Epoch: 333 Loss: 0.12504256211177292\n",
            "Epoch: 334 Loss: 0.12504256210385747\n",
            "Epoch: 335 Loss: 0.12504256209594203\n",
            "Epoch: 336 Loss: 0.12504256208802655\n",
            "Epoch: 337 Loss: 0.1250425620801111\n",
            "Epoch: 338 Loss: 0.12504256207219563\n",
            "Epoch: 339 Loss: 0.12504256206428016\n",
            "Epoch: 340 Loss: 0.12504256205636463\n",
            "Epoch: 341 Loss: 0.12504256204844916\n",
            "Epoch: 342 Loss: 0.12504256204053363\n",
            "Epoch: 343 Loss: 0.12504256203261813\n",
            "Epoch: 344 Loss: 0.1250425620247026\n",
            "Epoch: 345 Loss: 0.12504256201678707\n",
            "Epoch: 346 Loss: 0.12504256200887154\n",
            "Epoch: 347 Loss: 0.12504256200095598\n",
            "Epoch: 348 Loss: 0.1250425619930404\n",
            "Epoch: 349 Loss: 0.1250425619851248\n",
            "Epoch: 350 Loss: 0.12504256197720925\n",
            "Epoch: 351 Loss: 0.12504256196929367\n",
            "Epoch: 352 Loss: 0.12504256196137808\n",
            "Epoch: 353 Loss: 0.1250425619534625\n",
            "Epoch: 354 Loss: 0.1250425619455469\n",
            "Epoch: 355 Loss: 0.12504256193763127\n",
            "Epoch: 356 Loss: 0.1250425619297156\n",
            "Epoch: 357 Loss: 0.1250425619218\n",
            "Epoch: 358 Loss: 0.12504256191388438\n",
            "Epoch: 359 Loss: 0.1250425619059687\n",
            "Epoch: 360 Loss: 0.12504256189805305\n",
            "Epoch: 361 Loss: 0.1250425618901374\n",
            "Epoch: 362 Loss: 0.1250425618822217\n",
            "Epoch: 363 Loss: 0.125042561874306\n",
            "Epoch: 364 Loss: 0.12504256186639032\n",
            "Epoch: 365 Loss: 0.12504256185847465\n",
            "Epoch: 366 Loss: 0.12504256185055893\n",
            "Epoch: 367 Loss: 0.1250425618426432\n",
            "Epoch: 368 Loss: 0.12504256183472748\n",
            "Epoch: 369 Loss: 0.12504256182681173\n",
            "Epoch: 370 Loss: 0.125042561818896\n",
            "Epoch: 371 Loss: 0.12504256181098025\n",
            "Epoch: 372 Loss: 0.12504256180306447\n",
            "Epoch: 373 Loss: 0.12504256179514872\n",
            "Epoch: 374 Loss: 0.12504256178723294\n",
            "Epoch: 375 Loss: 0.12504256177931716\n",
            "Epoch: 376 Loss: 0.12504256177140136\n",
            "Epoch: 377 Loss: 0.12504256176348558\n",
            "Epoch: 378 Loss: 0.12504256175556974\n",
            "Epoch: 379 Loss: 0.12504256174765394\n",
            "Epoch: 380 Loss: 0.12504256173973813\n",
            "Epoch: 381 Loss: 0.12504256173182227\n",
            "Epoch: 382 Loss: 0.12504256172390643\n",
            "Epoch: 383 Loss: 0.1250425617159906\n",
            "Epoch: 384 Loss: 0.12504256170807473\n",
            "Epoch: 385 Loss: 0.12504256170015887\n",
            "Epoch: 386 Loss: 0.125042561692243\n",
            "Epoch: 387 Loss: 0.12504256168432712\n",
            "Epoch: 388 Loss: 0.1250425616764112\n",
            "Epoch: 389 Loss: 0.12504256166849528\n",
            "Epoch: 390 Loss: 0.12504256166057942\n",
            "Epoch: 391 Loss: 0.1250425616526635\n",
            "Epoch: 392 Loss: 0.12504256164474756\n",
            "Epoch: 393 Loss: 0.12504256163683164\n",
            "Epoch: 394 Loss: 0.12504256162891567\n",
            "Epoch: 395 Loss: 0.12504256162099972\n",
            "Epoch: 396 Loss: 0.12504256161308377\n",
            "Epoch: 397 Loss: 0.1250425616051678\n",
            "Epoch: 398 Loss: 0.12504256159725183\n",
            "Epoch: 399 Loss: 0.12504256158933585\n",
            "Epoch: 400 Loss: 0.12504256158141985\n",
            "Epoch: 401 Loss: 0.12504256157350385\n",
            "Epoch: 402 Loss: 0.12504256156558785\n",
            "Epoch: 403 Loss: 0.12504256155767182\n",
            "Epoch: 404 Loss: 0.1250425615497558\n",
            "Epoch: 405 Loss: 0.12504256154183976\n",
            "Epoch: 406 Loss: 0.1250425615339237\n",
            "Epoch: 407 Loss: 0.12504256152600768\n",
            "Epoch: 408 Loss: 0.1250425615180916\n",
            "Epoch: 409 Loss: 0.12504256151017554\n",
            "Epoch: 410 Loss: 0.12504256150225948\n",
            "Epoch: 411 Loss: 0.1250425614943434\n",
            "Epoch: 412 Loss: 0.1250425614864273\n",
            "Epoch: 413 Loss: 0.1250425614785112\n",
            "Epoch: 414 Loss: 0.1250425614705951\n",
            "Epoch: 415 Loss: 0.12504256146267897\n",
            "Epoch: 416 Loss: 0.12504256145476284\n",
            "Epoch: 417 Loss: 0.12504256144684672\n",
            "Epoch: 418 Loss: 0.12504256143893056\n",
            "Epoch: 419 Loss: 0.12504256143101442\n",
            "Epoch: 420 Loss: 0.12504256142309825\n",
            "Epoch: 421 Loss: 0.12504256141518208\n",
            "Epoch: 422 Loss: 0.1250425614072659\n",
            "Epoch: 423 Loss: 0.12504256139934974\n",
            "Epoch: 424 Loss: 0.12504256139143355\n",
            "Epoch: 425 Loss: 0.12504256138351733\n",
            "Epoch: 426 Loss: 0.12504256137560113\n",
            "Epoch: 427 Loss: 0.1250425613676849\n",
            "Epoch: 428 Loss: 0.1250425613597687\n",
            "Epoch: 429 Loss: 0.12504256135185243\n",
            "Epoch: 430 Loss: 0.1250425613439362\n",
            "Epoch: 431 Loss: 0.12504256133601996\n",
            "Epoch: 432 Loss: 0.1250425613281037\n",
            "Epoch: 433 Loss: 0.12504256132018746\n",
            "Epoch: 434 Loss: 0.12504256131227115\n",
            "Epoch: 435 Loss: 0.12504256130435487\n",
            "Epoch: 436 Loss: 0.12504256129643856\n",
            "Epoch: 437 Loss: 0.12504256128852229\n",
            "Epoch: 438 Loss: 0.12504256128060598\n",
            "Epoch: 439 Loss: 0.12504256127268965\n",
            "Epoch: 440 Loss: 0.12504256126477334\n",
            "Epoch: 441 Loss: 0.12504256125685698\n",
            "Epoch: 442 Loss: 0.12504256124894064\n",
            "Epoch: 443 Loss: 0.1250425612410243\n",
            "Epoch: 444 Loss: 0.12504256123310792\n",
            "Epoch: 445 Loss: 0.12504256122519158\n",
            "Epoch: 446 Loss: 0.1250425612172752\n",
            "Epoch: 447 Loss: 0.1250425612093588\n",
            "Epoch: 448 Loss: 0.12504256120144241\n",
            "Epoch: 449 Loss: 0.12504256119352603\n",
            "Epoch: 450 Loss: 0.12504256118560964\n",
            "Epoch: 451 Loss: 0.1250425611776932\n",
            "Epoch: 452 Loss: 0.12504256116977677\n",
            "Epoch: 453 Loss: 0.12504256116186033\n",
            "Epoch: 454 Loss: 0.12504256115394388\n",
            "Epoch: 455 Loss: 0.12504256114602744\n",
            "Epoch: 456 Loss: 0.125042561138111\n",
            "Epoch: 457 Loss: 0.12504256113019452\n",
            "Epoch: 458 Loss: 0.12504256112227805\n",
            "Epoch: 459 Loss: 0.12504256111436157\n",
            "Epoch: 460 Loss: 0.12504256110644507\n",
            "Epoch: 461 Loss: 0.12504256109852854\n",
            "Epoch: 462 Loss: 0.12504256109061204\n",
            "Epoch: 463 Loss: 0.12504256108269554\n",
            "Epoch: 464 Loss: 0.125042561074779\n",
            "Epoch: 465 Loss: 0.12504256106686248\n",
            "Epoch: 466 Loss: 0.12504256105894593\n",
            "Epoch: 467 Loss: 0.1250425610510294\n",
            "Epoch: 468 Loss: 0.12504256104311284\n",
            "Epoch: 469 Loss: 0.12504256103519626\n",
            "Epoch: 470 Loss: 0.12504256102727968\n",
            "Epoch: 471 Loss: 0.12504256101936312\n",
            "Epoch: 472 Loss: 0.1250425610114465\n",
            "Epoch: 473 Loss: 0.1250425610035299\n",
            "Epoch: 474 Loss: 0.12504256099561328\n",
            "Epoch: 475 Loss: 0.12504256098769667\n",
            "Epoch: 476 Loss: 0.12504256097978006\n",
            "Epoch: 477 Loss: 0.12504256097186342\n",
            "Epoch: 478 Loss: 0.12504256096394678\n",
            "Epoch: 479 Loss: 0.12504256095603014\n",
            "Epoch: 480 Loss: 0.12504256094811347\n",
            "Epoch: 481 Loss: 0.1250425609401968\n",
            "Epoch: 482 Loss: 0.1250425609322801\n",
            "Epoch: 483 Loss: 0.12504256092436344\n",
            "Epoch: 484 Loss: 0.12504256091644675\n",
            "Epoch: 485 Loss: 0.12504256090853005\n",
            "Epoch: 486 Loss: 0.12504256090061333\n",
            "Epoch: 487 Loss: 0.1250425608926966\n",
            "Epoch: 488 Loss: 0.12504256088477989\n",
            "Epoch: 489 Loss: 0.12504256087686316\n",
            "Epoch: 490 Loss: 0.1250425608689464\n",
            "Epoch: 491 Loss: 0.12504256086102966\n",
            "Epoch: 492 Loss: 0.1250425608531129\n",
            "Epoch: 493 Loss: 0.12504256084519616\n",
            "Epoch: 494 Loss: 0.12504256083727935\n",
            "Epoch: 495 Loss: 0.12504256082936258\n",
            "Epoch: 496 Loss: 0.1250425608214458\n",
            "Epoch: 497 Loss: 0.125042560813529\n",
            "Epoch: 498 Loss: 0.12504256080561216\n",
            "Epoch: 499 Loss: 0.12504256079769535\n",
            "Epoch: 500 Loss: 0.12504256078977852\n",
            "Epoch: 501 Loss: 0.1250425607818617\n",
            "Epoch: 502 Loss: 0.12504256077394488\n",
            "Epoch: 503 Loss: 0.12504256076602802\n",
            "Epoch: 504 Loss: 0.12504256075811113\n",
            "Epoch: 505 Loss: 0.12504256075019426\n",
            "Epoch: 506 Loss: 0.1250425607422774\n",
            "Epoch: 507 Loss: 0.12504256073436051\n",
            "Epoch: 508 Loss: 0.12504256072644362\n",
            "Epoch: 509 Loss: 0.12504256071852674\n",
            "Epoch: 510 Loss: 0.12504256071060982\n",
            "Epoch: 511 Loss: 0.1250425607026929\n",
            "Epoch: 512 Loss: 0.12504256069477598\n",
            "Epoch: 513 Loss: 0.12504256068685904\n",
            "Epoch: 514 Loss: 0.1250425606789421\n",
            "Epoch: 515 Loss: 0.12504256067102515\n",
            "Epoch: 516 Loss: 0.1250425606631082\n",
            "Epoch: 517 Loss: 0.1250425606551912\n",
            "Epoch: 518 Loss: 0.12504256064727423\n",
            "Epoch: 519 Loss: 0.1250425606393573\n",
            "Epoch: 520 Loss: 0.12504256063144026\n",
            "Epoch: 521 Loss: 0.12504256062352326\n",
            "Epoch: 522 Loss: 0.12504256061560626\n",
            "Epoch: 523 Loss: 0.12504256060768923\n",
            "Epoch: 524 Loss: 0.1250425605997722\n",
            "Epoch: 525 Loss: 0.12504256059185517\n",
            "Epoch: 526 Loss: 0.12504256058393812\n",
            "Epoch: 527 Loss: 0.12504256057602106\n",
            "Epoch: 528 Loss: 0.12504256056810403\n",
            "Epoch: 529 Loss: 0.12504256056018695\n",
            "Epoch: 530 Loss: 0.12504256055226987\n",
            "Epoch: 531 Loss: 0.12504256054435278\n",
            "Epoch: 532 Loss: 0.1250425605364357\n",
            "Epoch: 533 Loss: 0.12504256052851861\n",
            "Epoch: 534 Loss: 0.12504256052060148\n",
            "Epoch: 535 Loss: 0.12504256051268436\n",
            "Epoch: 536 Loss: 0.12504256050476723\n",
            "Epoch: 537 Loss: 0.1250425604968501\n",
            "Epoch: 538 Loss: 0.12504256048893297\n",
            "Epoch: 539 Loss: 0.1250425604810158\n",
            "Epoch: 540 Loss: 0.12504256047309867\n",
            "Epoch: 541 Loss: 0.1250425604651815\n",
            "Epoch: 542 Loss: 0.1250425604572643\n",
            "Epoch: 543 Loss: 0.1250425604493471\n",
            "Epoch: 544 Loss: 0.12504256044142992\n",
            "Epoch: 545 Loss: 0.12504256043351272\n",
            "Epoch: 546 Loss: 0.1250425604255955\n",
            "Epoch: 547 Loss: 0.1250425604176783\n",
            "Epoch: 548 Loss: 0.12504256040976108\n",
            "Epoch: 549 Loss: 0.12504256040184383\n",
            "Epoch: 550 Loss: 0.1250425603939266\n",
            "Epoch: 551 Loss: 0.12504256038600933\n",
            "Epoch: 552 Loss: 0.12504256037809208\n",
            "Epoch: 553 Loss: 0.1250425603701748\n",
            "Epoch: 554 Loss: 0.12504256036225755\n",
            "Epoch: 555 Loss: 0.12504256035434025\n",
            "Epoch: 556 Loss: 0.12504256034642297\n",
            "Epoch: 557 Loss: 0.12504256033850566\n",
            "Epoch: 558 Loss: 0.12504256033058836\n",
            "Epoch: 559 Loss: 0.12504256032267103\n",
            "Epoch: 560 Loss: 0.1250425603147537\n",
            "Epoch: 561 Loss: 0.1250425603068364\n",
            "Epoch: 562 Loss: 0.12504256029891903\n",
            "Epoch: 563 Loss: 0.12504256029100166\n",
            "Epoch: 564 Loss: 0.1250425602830843\n",
            "Epoch: 565 Loss: 0.12504256027516694\n",
            "Epoch: 566 Loss: 0.12504256026724958\n",
            "Epoch: 567 Loss: 0.1250425602593322\n",
            "Epoch: 568 Loss: 0.1250425602514148\n",
            "Epoch: 569 Loss: 0.1250425602434974\n",
            "Epoch: 570 Loss: 0.12504256023558\n",
            "Epoch: 571 Loss: 0.12504256022766258\n",
            "Epoch: 572 Loss: 0.12504256021974514\n",
            "Epoch: 573 Loss: 0.12504256021182772\n",
            "Epoch: 574 Loss: 0.12504256020391027\n",
            "Epoch: 575 Loss: 0.1250425601959928\n",
            "Epoch: 576 Loss: 0.12504256018807533\n",
            "Epoch: 577 Loss: 0.12504256018015786\n",
            "Epoch: 578 Loss: 0.1250425601722404\n",
            "Epoch: 579 Loss: 0.1250425601643229\n",
            "Epoch: 580 Loss: 0.1250425601564054\n",
            "Epoch: 581 Loss: 0.1250425601484879\n",
            "Epoch: 582 Loss: 0.1250425601405704\n",
            "Epoch: 583 Loss: 0.12504256013265289\n",
            "Epoch: 584 Loss: 0.12504256012473536\n",
            "Epoch: 585 Loss: 0.1250425601168178\n",
            "Epoch: 586 Loss: 0.1250425601089003\n",
            "Epoch: 587 Loss: 0.12504256010098272\n",
            "Epoch: 588 Loss: 0.12504256009306516\n",
            "Epoch: 589 Loss: 0.1250425600851476\n",
            "Epoch: 590 Loss: 0.12504256007723002\n",
            "Epoch: 591 Loss: 0.12504256006931244\n",
            "Epoch: 592 Loss: 0.12504256006139486\n",
            "Epoch: 593 Loss: 0.12504256005347725\n",
            "Epoch: 594 Loss: 0.12504256004555964\n",
            "Epoch: 595 Loss: 0.125042560037642\n",
            "Epoch: 596 Loss: 0.1250425600297244\n",
            "Epoch: 597 Loss: 0.12504256002180675\n",
            "Epoch: 598 Loss: 0.1250425600138891\n",
            "Epoch: 599 Loss: 0.12504256000597147\n",
            "Epoch: 600 Loss: 0.1250425599980538\n",
            "Epoch: 601 Loss: 0.1250425599901361\n",
            "Epoch: 602 Loss: 0.12504255998221844\n",
            "Epoch: 603 Loss: 0.12504255997430075\n",
            "Epoch: 604 Loss: 0.12504255996638305\n",
            "Epoch: 605 Loss: 0.12504255995846536\n",
            "Epoch: 606 Loss: 0.12504255995054767\n",
            "Epoch: 607 Loss: 0.12504255994262994\n",
            "Epoch: 608 Loss: 0.1250425599347122\n",
            "Epoch: 609 Loss: 0.12504255992679447\n",
            "Epoch: 610 Loss: 0.12504255991887672\n",
            "Epoch: 611 Loss: 0.12504255991095897\n",
            "Epoch: 612 Loss: 0.1250425599030412\n",
            "Epoch: 613 Loss: 0.12504255989512345\n",
            "Epoch: 614 Loss: 0.12504255988720567\n",
            "Epoch: 615 Loss: 0.1250425598792879\n",
            "Epoch: 616 Loss: 0.12504255987137008\n",
            "Epoch: 617 Loss: 0.12504255986345228\n",
            "Epoch: 618 Loss: 0.12504255985553447\n",
            "Epoch: 619 Loss: 0.12504255984761667\n",
            "Epoch: 620 Loss: 0.12504255983969884\n",
            "Epoch: 621 Loss: 0.12504255983178097\n",
            "Epoch: 622 Loss: 0.12504255982386314\n",
            "Epoch: 623 Loss: 0.1250425598159453\n",
            "Epoch: 624 Loss: 0.12504255980802742\n",
            "Epoch: 625 Loss: 0.1250425598001096\n",
            "Epoch: 626 Loss: 0.12504255979219167\n",
            "Epoch: 627 Loss: 0.12504255978427378\n",
            "Epoch: 628 Loss: 0.1250425597763559\n",
            "Epoch: 629 Loss: 0.125042559768438\n",
            "Epoch: 630 Loss: 0.1250425597605201\n",
            "Epoch: 631 Loss: 0.12504255975260217\n",
            "Epoch: 632 Loss: 0.12504255974468423\n",
            "Epoch: 633 Loss: 0.12504255973676628\n",
            "Epoch: 634 Loss: 0.12504255972884834\n",
            "Epoch: 635 Loss: 0.12504255972093042\n",
            "Epoch: 636 Loss: 0.12504255971301245\n",
            "Epoch: 637 Loss: 0.12504255970509448\n",
            "Epoch: 638 Loss: 0.1250425596971765\n",
            "Epoch: 639 Loss: 0.12504255968925854\n",
            "Epoch: 640 Loss: 0.1250425596813405\n",
            "Epoch: 641 Loss: 0.1250425596734225\n",
            "Epoch: 642 Loss: 0.12504255966550448\n",
            "Epoch: 643 Loss: 0.12504255965758648\n",
            "Epoch: 644 Loss: 0.12504255964966846\n",
            "Epoch: 645 Loss: 0.12504255964175043\n",
            "Epoch: 646 Loss: 0.12504255963383235\n",
            "Epoch: 647 Loss: 0.12504255962591432\n",
            "Epoch: 648 Loss: 0.12504255961799626\n",
            "Epoch: 649 Loss: 0.12504255961007818\n",
            "Epoch: 650 Loss: 0.1250425596021601\n",
            "Epoch: 651 Loss: 0.12504255959424201\n",
            "Epoch: 652 Loss: 0.1250425595863239\n",
            "Epoch: 653 Loss: 0.12504255957840582\n",
            "Epoch: 654 Loss: 0.1250425595704877\n",
            "Epoch: 655 Loss: 0.1250425595625696\n",
            "Epoch: 656 Loss: 0.12504255955465146\n",
            "Epoch: 657 Loss: 0.12504255954673332\n",
            "Epoch: 658 Loss: 0.12504255953881516\n",
            "Epoch: 659 Loss: 0.12504255953089702\n",
            "Epoch: 660 Loss: 0.12504255952297885\n",
            "Epoch: 661 Loss: 0.1250425595150607\n",
            "Epoch: 662 Loss: 0.1250425595071425\n",
            "Epoch: 663 Loss: 0.12504255949922433\n",
            "Epoch: 664 Loss: 0.12504255949130613\n",
            "Epoch: 665 Loss: 0.1250425594833879\n",
            "Epoch: 666 Loss: 0.12504255947546972\n",
            "Epoch: 667 Loss: 0.1250425594675515\n",
            "Epoch: 668 Loss: 0.12504255945963325\n",
            "Epoch: 669 Loss: 0.12504255945171502\n",
            "Epoch: 670 Loss: 0.12504255944379677\n",
            "Epoch: 671 Loss: 0.12504255943587853\n",
            "Epoch: 672 Loss: 0.12504255942796028\n",
            "Epoch: 673 Loss: 0.12504255942004197\n",
            "Epoch: 674 Loss: 0.12504255941212372\n",
            "Epoch: 675 Loss: 0.12504255940420544\n",
            "Epoch: 676 Loss: 0.12504255939628714\n",
            "Epoch: 677 Loss: 0.12504255938836883\n",
            "Epoch: 678 Loss: 0.1250425593804505\n",
            "Epoch: 679 Loss: 0.1250425593725322\n",
            "Epoch: 680 Loss: 0.1250425593646139\n",
            "Epoch: 681 Loss: 0.12504255935669553\n",
            "Epoch: 682 Loss: 0.12504255934877717\n",
            "Epoch: 683 Loss: 0.1250425593408588\n",
            "Epoch: 684 Loss: 0.12504255933294048\n",
            "Epoch: 685 Loss: 0.1250425593250221\n",
            "Epoch: 686 Loss: 0.1250425593171037\n",
            "Epoch: 687 Loss: 0.1250425593091853\n",
            "Epoch: 688 Loss: 0.12504255930126693\n",
            "Epoch: 689 Loss: 0.1250425592933485\n",
            "Epoch: 690 Loss: 0.1250425592854301\n",
            "Epoch: 691 Loss: 0.12504255927751168\n",
            "Epoch: 692 Loss: 0.12504255926959326\n",
            "Epoch: 693 Loss: 0.12504255926167482\n",
            "Epoch: 694 Loss: 0.12504255925375637\n",
            "Epoch: 695 Loss: 0.1250425592458379\n",
            "Epoch: 696 Loss: 0.12504255923791946\n",
            "Epoch: 697 Loss: 0.125042559230001\n",
            "Epoch: 698 Loss: 0.12504255922208252\n",
            "Epoch: 699 Loss: 0.12504255921416402\n",
            "Epoch: 700 Loss: 0.12504255920624552\n",
            "Epoch: 701 Loss: 0.12504255919832702\n",
            "Epoch: 702 Loss: 0.1250425591904085\n",
            "Epoch: 703 Loss: 0.12504255918249\n",
            "Epoch: 704 Loss: 0.12504255917457144\n",
            "Epoch: 705 Loss: 0.1250425591666529\n",
            "Epoch: 706 Loss: 0.12504255915873438\n",
            "Epoch: 707 Loss: 0.1250425591508158\n",
            "Epoch: 708 Loss: 0.12504255914289725\n",
            "Epoch: 709 Loss: 0.12504255913497866\n",
            "Epoch: 710 Loss: 0.12504255912706008\n",
            "Epoch: 711 Loss: 0.12504255911914153\n",
            "Epoch: 712 Loss: 0.1250425591112229\n",
            "Epoch: 713 Loss: 0.1250425591033043\n",
            "Epoch: 714 Loss: 0.1250425590953857\n",
            "Epoch: 715 Loss: 0.12504255908746706\n",
            "Epoch: 716 Loss: 0.12504255907954845\n",
            "Epoch: 717 Loss: 0.1250425590716298\n",
            "Epoch: 718 Loss: 0.12504255906371117\n",
            "Epoch: 719 Loss: 0.1250425590557925\n",
            "Epoch: 720 Loss: 0.12504255904787384\n",
            "Epoch: 721 Loss: 0.12504255903995518\n",
            "Epoch: 722 Loss: 0.12504255903203648\n",
            "Epoch: 723 Loss: 0.1250425590241178\n",
            "Epoch: 724 Loss: 0.1250425590161991\n",
            "Epoch: 725 Loss: 0.1250425590082804\n",
            "Epoch: 726 Loss: 0.12504255900036165\n",
            "Epoch: 727 Loss: 0.12504255899244296\n",
            "Epoch: 728 Loss: 0.1250425589845242\n",
            "Epoch: 729 Loss: 0.1250425589766055\n",
            "Epoch: 730 Loss: 0.12504255896868674\n",
            "Epoch: 731 Loss: 0.12504255896076796\n",
            "Epoch: 732 Loss: 0.12504255895284921\n",
            "Epoch: 733 Loss: 0.12504255894493044\n",
            "Epoch: 734 Loss: 0.1250425589370117\n",
            "Epoch: 735 Loss: 0.12504255892909288\n",
            "Epoch: 736 Loss: 0.12504255892117408\n",
            "Epoch: 737 Loss: 0.12504255891325528\n",
            "Epoch: 738 Loss: 0.12504255890533644\n",
            "Epoch: 739 Loss: 0.12504255889741764\n",
            "Epoch: 740 Loss: 0.1250425588894988\n",
            "Epoch: 741 Loss: 0.12504255888157995\n",
            "Epoch: 742 Loss: 0.1250425588736611\n",
            "Epoch: 743 Loss: 0.12504255886574225\n",
            "Epoch: 744 Loss: 0.1250425588578234\n",
            "Epoch: 745 Loss: 0.1250425588499045\n",
            "Epoch: 746 Loss: 0.12504255884198565\n",
            "Epoch: 747 Loss: 0.12504255883406676\n",
            "Epoch: 748 Loss: 0.12504255882614784\n",
            "Epoch: 749 Loss: 0.12504255881822893\n",
            "Epoch: 750 Loss: 0.12504255881031004\n",
            "Epoch: 751 Loss: 0.12504255880239112\n",
            "Epoch: 752 Loss: 0.12504255879447218\n",
            "Epoch: 753 Loss: 0.12504255878655324\n",
            "Epoch: 754 Loss: 0.1250425587786343\n",
            "Epoch: 755 Loss: 0.12504255877071535\n",
            "Epoch: 756 Loss: 0.12504255876279638\n",
            "Epoch: 757 Loss: 0.12504255875487738\n",
            "Epoch: 758 Loss: 0.1250425587469584\n",
            "Epoch: 759 Loss: 0.1250425587390394\n",
            "Epoch: 760 Loss: 0.1250425587311204\n",
            "Epoch: 761 Loss: 0.1250425587232014\n",
            "Epoch: 762 Loss: 0.1250425587152824\n",
            "Epoch: 763 Loss: 0.12504255870736336\n",
            "Epoch: 764 Loss: 0.12504255869944433\n",
            "Epoch: 765 Loss: 0.1250425586915253\n",
            "Epoch: 766 Loss: 0.12504255868360625\n",
            "Epoch: 767 Loss: 0.1250425586756872\n",
            "Epoch: 768 Loss: 0.12504255866776814\n",
            "Epoch: 769 Loss: 0.12504255865984906\n",
            "Epoch: 770 Loss: 0.12504255865192995\n",
            "Epoch: 771 Loss: 0.12504255864401087\n",
            "Epoch: 772 Loss: 0.1250425586360918\n",
            "Epoch: 773 Loss: 0.12504255862817268\n",
            "Epoch: 774 Loss: 0.12504255862025357\n",
            "Epoch: 775 Loss: 0.12504255861233443\n",
            "Epoch: 776 Loss: 0.1250425586044153\n",
            "Epoch: 777 Loss: 0.12504255859649616\n",
            "Epoch: 778 Loss: 0.12504255858857702\n",
            "Epoch: 779 Loss: 0.12504255858065785\n",
            "Epoch: 780 Loss: 0.1250425585727387\n",
            "Epoch: 781 Loss: 0.12504255856481952\n",
            "Epoch: 782 Loss: 0.12504255855690033\n",
            "Epoch: 783 Loss: 0.12504255854898114\n",
            "Epoch: 784 Loss: 0.12504255854106194\n",
            "Epoch: 785 Loss: 0.12504255853314275\n",
            "Epoch: 786 Loss: 0.12504255852522353\n",
            "Epoch: 787 Loss: 0.12504255851730428\n",
            "Epoch: 788 Loss: 0.12504255850938506\n",
            "Epoch: 789 Loss: 0.1250425585014658\n",
            "Epoch: 790 Loss: 0.12504255849354656\n",
            "Epoch: 791 Loss: 0.12504255848562731\n",
            "Epoch: 792 Loss: 0.12504255847770804\n",
            "Epoch: 793 Loss: 0.12504255846978876\n",
            "Epoch: 794 Loss: 0.12504255846186949\n",
            "Epoch: 795 Loss: 0.12504255845395018\n",
            "Epoch: 796 Loss: 0.12504255844603088\n",
            "Epoch: 797 Loss: 0.12504255843811157\n",
            "Epoch: 798 Loss: 0.12504255843019227\n",
            "Epoch: 799 Loss: 0.12504255842227294\n",
            "Epoch: 800 Loss: 0.1250425584143536\n",
            "Epoch: 801 Loss: 0.12504255840643427\n",
            "Epoch: 802 Loss: 0.1250425583985149\n",
            "Epoch: 803 Loss: 0.12504255839059555\n",
            "Epoch: 804 Loss: 0.1250425583826762\n",
            "Epoch: 805 Loss: 0.12504255837475683\n",
            "Epoch: 806 Loss: 0.12504255836683745\n",
            "Epoch: 807 Loss: 0.12504255835891806\n",
            "Epoch: 808 Loss: 0.12504255835099864\n",
            "Epoch: 809 Loss: 0.12504255834307923\n",
            "Epoch: 810 Loss: 0.12504255833515981\n",
            "Epoch: 811 Loss: 0.12504255832724037\n",
            "Epoch: 812 Loss: 0.12504255831932096\n",
            "Epoch: 813 Loss: 0.1250425583114015\n",
            "Epoch: 814 Loss: 0.12504255830348207\n",
            "Epoch: 815 Loss: 0.1250425582955626\n",
            "Epoch: 816 Loss: 0.12504255828764316\n",
            "Epoch: 817 Loss: 0.12504255827972366\n",
            "Epoch: 818 Loss: 0.1250425582718042\n",
            "Epoch: 819 Loss: 0.1250425582638847\n",
            "Epoch: 820 Loss: 0.1250425582559652\n",
            "Epoch: 821 Loss: 0.1250425582480457\n",
            "Epoch: 822 Loss: 0.12504255824012617\n",
            "Epoch: 823 Loss: 0.12504255823220664\n",
            "Epoch: 824 Loss: 0.1250425582242871\n",
            "Epoch: 825 Loss: 0.12504255821636756\n",
            "Epoch: 826 Loss: 0.125042558208448\n",
            "Epoch: 827 Loss: 0.12504255820052843\n",
            "Epoch: 828 Loss: 0.12504255819260887\n",
            "Epoch: 829 Loss: 0.1250425581846893\n",
            "Epoch: 830 Loss: 0.1250425581767697\n",
            "Epoch: 831 Loss: 0.12504255816885013\n",
            "Epoch: 832 Loss: 0.12504255816093052\n",
            "Epoch: 833 Loss: 0.1250425581530109\n",
            "Epoch: 834 Loss: 0.1250425581450913\n",
            "Epoch: 835 Loss: 0.12504255813717166\n",
            "Epoch: 836 Loss: 0.12504255812925202\n",
            "Epoch: 837 Loss: 0.1250425581213324\n",
            "Epoch: 838 Loss: 0.12504255811341272\n",
            "Epoch: 839 Loss: 0.12504255810549308\n",
            "Epoch: 840 Loss: 0.12504255809757342\n",
            "Epoch: 841 Loss: 0.1250425580896537\n",
            "Epoch: 842 Loss: 0.12504255808173403\n",
            "Epoch: 843 Loss: 0.12504255807381434\n",
            "Epoch: 844 Loss: 0.12504255806589465\n",
            "Epoch: 845 Loss: 0.12504255805797493\n",
            "Epoch: 846 Loss: 0.1250425580500552\n",
            "Epoch: 847 Loss: 0.1250425580421355\n",
            "Epoch: 848 Loss: 0.12504255803421577\n",
            "Epoch: 849 Loss: 0.12504255802629602\n",
            "Epoch: 850 Loss: 0.12504255801837627\n",
            "Epoch: 851 Loss: 0.1250425580104565\n",
            "Epoch: 852 Loss: 0.12504255800253672\n",
            "Epoch: 853 Loss: 0.12504255799461694\n",
            "Epoch: 854 Loss: 0.12504255798669717\n",
            "Epoch: 855 Loss: 0.1250425579787774\n",
            "Epoch: 856 Loss: 0.12504255797085756\n",
            "Epoch: 857 Loss: 0.12504255796293776\n",
            "Epoch: 858 Loss: 0.12504255795501792\n",
            "Epoch: 859 Loss: 0.12504255794709812\n",
            "Epoch: 860 Loss: 0.12504255793917826\n",
            "Epoch: 861 Loss: 0.1250425579312584\n",
            "Epoch: 862 Loss: 0.12504255792333857\n",
            "Epoch: 863 Loss: 0.1250425579154187\n",
            "Epoch: 864 Loss: 0.12504255790749885\n",
            "Epoch: 865 Loss: 0.125042557899579\n",
            "Epoch: 866 Loss: 0.12504255789165908\n",
            "Epoch: 867 Loss: 0.1250425578837392\n",
            "Epoch: 868 Loss: 0.12504255787581928\n",
            "Epoch: 869 Loss: 0.12504255786789936\n",
            "Epoch: 870 Loss: 0.12504255785997945\n",
            "Epoch: 871 Loss: 0.1250425578520595\n",
            "Epoch: 872 Loss: 0.1250425578441396\n",
            "Epoch: 873 Loss: 0.12504255783621962\n",
            "Epoch: 874 Loss: 0.12504255782829968\n",
            "Epoch: 875 Loss: 0.12504255782037973\n",
            "Epoch: 876 Loss: 0.12504255781245976\n",
            "Epoch: 877 Loss: 0.12504255780453977\n",
            "Epoch: 878 Loss: 0.1250425577966198\n",
            "Epoch: 879 Loss: 0.1250425577886998\n",
            "Epoch: 880 Loss: 0.1250425577807798\n",
            "Epoch: 881 Loss: 0.12504255777285977\n",
            "Epoch: 882 Loss: 0.12504255776493975\n",
            "Epoch: 883 Loss: 0.12504255775701972\n",
            "Epoch: 884 Loss: 0.1250425577490997\n",
            "Epoch: 885 Loss: 0.12504255774117964\n",
            "Epoch: 886 Loss: 0.1250425577332596\n",
            "Epoch: 887 Loss: 0.12504255772533954\n",
            "Epoch: 888 Loss: 0.12504255771741948\n",
            "Epoch: 889 Loss: 0.12504255770949937\n",
            "Epoch: 890 Loss: 0.1250425577015793\n",
            "Epoch: 891 Loss: 0.12504255769365918\n",
            "Epoch: 892 Loss: 0.1250425576857391\n",
            "Epoch: 893 Loss: 0.125042557677819\n",
            "Epoch: 894 Loss: 0.12504255766989886\n",
            "Epoch: 895 Loss: 0.12504255766197872\n",
            "Epoch: 896 Loss: 0.12504255765405858\n",
            "Epoch: 897 Loss: 0.12504255764613845\n",
            "Epoch: 898 Loss: 0.12504255763821828\n",
            "Epoch: 899 Loss: 0.12504255763029812\n",
            "Epoch: 900 Loss: 0.12504255762237795\n",
            "Epoch: 901 Loss: 0.12504255761445776\n",
            "Epoch: 902 Loss: 0.12504255760653757\n",
            "Epoch: 903 Loss: 0.12504255759861738\n",
            "Epoch: 904 Loss: 0.12504255759069718\n",
            "Epoch: 905 Loss: 0.12504255758277696\n",
            "Epoch: 906 Loss: 0.12504255757485674\n",
            "Epoch: 907 Loss: 0.12504255756693652\n",
            "Epoch: 908 Loss: 0.12504255755901628\n",
            "Epoch: 909 Loss: 0.12504255755109603\n",
            "Epoch: 910 Loss: 0.12504255754317578\n",
            "Epoch: 911 Loss: 0.1250425575352555\n",
            "Epoch: 912 Loss: 0.12504255752733523\n",
            "Epoch: 913 Loss: 0.12504255751941495\n",
            "Epoch: 914 Loss: 0.12504255751149465\n",
            "Epoch: 915 Loss: 0.12504255750357438\n",
            "Epoch: 916 Loss: 0.12504255749565407\n",
            "Epoch: 917 Loss: 0.12504255748773374\n",
            "Epoch: 918 Loss: 0.12504255747981344\n",
            "Epoch: 919 Loss: 0.12504255747189308\n",
            "Epoch: 920 Loss: 0.12504255746397275\n",
            "Epoch: 921 Loss: 0.12504255745605242\n",
            "Epoch: 922 Loss: 0.12504255744813206\n",
            "Epoch: 923 Loss: 0.1250425574402117\n",
            "Epoch: 924 Loss: 0.1250425574322913\n",
            "Epoch: 925 Loss: 0.12504255742437093\n",
            "Epoch: 926 Loss: 0.12504255741645057\n",
            "Epoch: 927 Loss: 0.12504255740853015\n",
            "Epoch: 928 Loss: 0.12504255740060974\n",
            "Epoch: 929 Loss: 0.12504255739268932\n",
            "Epoch: 930 Loss: 0.1250425573847689\n",
            "Epoch: 931 Loss: 0.12504255737684847\n",
            "Epoch: 932 Loss: 0.12504255736892805\n",
            "Epoch: 933 Loss: 0.12504255736100758\n",
            "Epoch: 934 Loss: 0.1250425573530871\n",
            "Epoch: 935 Loss: 0.12504255734516664\n",
            "Epoch: 936 Loss: 0.1250425573372462\n",
            "Epoch: 937 Loss: 0.1250425573293257\n",
            "Epoch: 938 Loss: 0.1250425573214052\n",
            "Epoch: 939 Loss: 0.1250425573134847\n",
            "Epoch: 940 Loss: 0.12504255730556424\n",
            "Epoch: 941 Loss: 0.12504255729764369\n",
            "Epoch: 942 Loss: 0.1250425572897232\n",
            "Epoch: 943 Loss: 0.12504255728180264\n",
            "Epoch: 944 Loss: 0.12504255727388208\n",
            "Epoch: 945 Loss: 0.12504255726596156\n",
            "Epoch: 946 Loss: 0.125042557258041\n",
            "Epoch: 947 Loss: 0.12504255725012042\n",
            "Epoch: 948 Loss: 0.12504255724219984\n",
            "Epoch: 949 Loss: 0.1250425572342793\n",
            "Epoch: 950 Loss: 0.12504255722635868\n",
            "Epoch: 951 Loss: 0.12504255721843807\n",
            "Epoch: 952 Loss: 0.1250425572105175\n",
            "Epoch: 953 Loss: 0.12504255720259685\n",
            "Epoch: 954 Loss: 0.12504255719467622\n",
            "Epoch: 955 Loss: 0.1250425571867556\n",
            "Epoch: 956 Loss: 0.12504255717883495\n",
            "Epoch: 957 Loss: 0.1250425571709143\n",
            "Epoch: 958 Loss: 0.12504255716299367\n",
            "Epoch: 959 Loss: 0.12504255715507298\n",
            "Epoch: 960 Loss: 0.12504255714715232\n",
            "Epoch: 961 Loss: 0.12504255713923162\n",
            "Epoch: 962 Loss: 0.12504255713131093\n",
            "Epoch: 963 Loss: 0.1250425571233902\n",
            "Epoch: 964 Loss: 0.12504255711546952\n",
            "Epoch: 965 Loss: 0.12504255710754883\n",
            "Epoch: 966 Loss: 0.12504255709962808\n",
            "Epoch: 967 Loss: 0.12504255709170736\n",
            "Epoch: 968 Loss: 0.12504255708378662\n",
            "Epoch: 969 Loss: 0.12504255707586587\n",
            "Epoch: 970 Loss: 0.12504255706794512\n",
            "Epoch: 971 Loss: 0.12504255706002435\n",
            "Epoch: 972 Loss: 0.12504255705210354\n",
            "Epoch: 973 Loss: 0.12504255704418277\n",
            "Epoch: 974 Loss: 0.12504255703626196\n",
            "Epoch: 975 Loss: 0.1250425570283412\n",
            "Epoch: 976 Loss: 0.12504255702042036\n",
            "Epoch: 977 Loss: 0.12504255701249958\n",
            "Epoch: 978 Loss: 0.12504255700457872\n",
            "Epoch: 979 Loss: 0.1250425569966579\n",
            "Epoch: 980 Loss: 0.12504255698873706\n",
            "Epoch: 981 Loss: 0.12504255698081618\n",
            "Epoch: 982 Loss: 0.12504255697289535\n",
            "Epoch: 983 Loss: 0.1250425569649745\n",
            "Epoch: 984 Loss: 0.1250425569570536\n",
            "Epoch: 985 Loss: 0.12504255694913272\n",
            "Epoch: 986 Loss: 0.1250425569412118\n",
            "Epoch: 987 Loss: 0.12504255693329092\n",
            "Epoch: 988 Loss: 0.12504255692537\n",
            "Epoch: 989 Loss: 0.12504255691744912\n",
            "Epoch: 990 Loss: 0.12504255690952815\n",
            "Epoch: 991 Loss: 0.1250425569016072\n",
            "Epoch: 992 Loss: 0.1250425568936863\n",
            "Epoch: 993 Loss: 0.12504255688576535\n",
            "Epoch: 994 Loss: 0.12504255687784435\n",
            "Epoch: 995 Loss: 0.1250425568699234\n",
            "Epoch: 996 Loss: 0.1250425568620024\n",
            "Epoch: 997 Loss: 0.12504255685408144\n",
            "Epoch: 998 Loss: 0.12504255684616045\n",
            "Epoch: 999 Loss: 0.12504255683823945\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZZ23FbcJpNR",
        "outputId": "34895223-9bc0-44c7-d051-3168aff0e0b6"
      },
      "source": [
        "sns.set()\n",
        "plt.plot(loss4)\n",
        "plt.title(\"Vanishing Gradients\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.savefig(\"180100039_vanishing_gradients.png\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnulSg4bJpNT"
      },
      "source": [
        "**Note :** The algorithm stops learning as value of gradients are negligible hence the loss function becomes constant "
      ]
    }
  ]
}